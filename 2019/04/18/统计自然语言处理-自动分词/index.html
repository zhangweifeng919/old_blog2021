<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">






  
  
  <link rel="stylesheet" media="all" href="/lib/Han/dist/han.min.css?v=3.3">




<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="nlp,统计自然语言处理,">










<meta name="description" content="由于词是最小的能够独立运用的语言单位，而很多孤立语和黏着语（如汉语、日语、越南语、藏语等）的文本不像西方屈折语的文本，词与词之间没有任何空格之类的显式标志指示词的边界，因此，自动分词问题就成了计算机处理孤立语和黏着语文本时面临的首要基础性工作，是诸多应用系统不可或缺的一个重要环节。">
<meta name="keywords" content="nlp,统计自然语言处理">
<meta property="og:type" content="article">
<meta property="og:title" content="统计自然语言处理-自动分词">
<meta property="og:url" content="https://zhangweifeng.top/2019/04/18/统计自然语言处理-自动分词/index.html">
<meta property="og:site_name" content="漂泊在学海">
<meta property="og:description" content="由于词是最小的能够独立运用的语言单位，而很多孤立语和黏着语（如汉语、日语、越南语、藏语等）的文本不像西方屈折语的文本，词与词之间没有任何空格之类的显式标志指示词的边界，因此，自动分词问题就成了计算机处理孤立语和黏着语文本时面临的首要基础性工作，是诸多应用系统不可或缺的一个重要环节。">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://zhangweifeng.top/images/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E5%88%87%E5%88%86%E6%9C%89%E5%90%91%E6%97%A0%E7%8E%AF%E5%9B%BE.png">
<meta property="og:image" content="https://zhangweifeng.top/images/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/3-%E6%9C%80%E7%9F%AD%E8%B7%AF%E5%BE%84%E5%88%86%E8%AF%8D%E8%BF%87%E7%A8%8B.png">
<meta property="og:image" content="https://zhangweifeng.top/images/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E5%9F%BA%E4%BA%8E%E8%AF%8D%E7%9A%84%E7%94%9F%E6%88%90%E5%BC%8F%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BA%8C%E5%85%83%E6%96%87%E6%B3%95%E5%88%87%E5%88%86%E8%AF%8D%E5%9B%BE.png">
<meta property="og:image" content="https://zhangweifeng.top/images/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B7-2.png">
<meta property="og:image" content="https://zhangweifeng.top/images/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E5%B9%B3%E5%9D%87%E6%84%9F%E7%9F%A5%E6%9C%BA%E8%AE%AD%E7%BB%83%E7%AE%97%E6%B3%95.png">
<meta property="og:image" content="https://zhangweifeng.top/images/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E5%9F%BA%E4%BA%8E%E8%AF%8D%E6%84%9F%E7%9F%A5%E6%9C%BA%E8%A7%A3%E7%A0%81%E7%AE%97%E6%B3%95.png">
<meta property="og:image" content="https://zhangweifeng.top/images/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E5%9F%BA%E4%BA%8E%E8%AF%8D%E7%9A%84%E5%88%A4%E5%88%AB%E5%BC%8F%E6%A8%A1%E5%9E%8B%E4%BD%BF%E7%94%A8%E7%9A%84%E7%89%B9%E5%BE%81.png">
<meta property="og:image" content="https://zhangweifeng.top/images/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E5%80%99%E9%80%89%EF%BC%88a%EF%BC%89%E5%92%8C%EF%BC%88b%EF%BC%89%E5%AF%B9%E5%BA%94%E7%9A%84%E7%89%B9%E5%BE%81.png">
<meta property="og:image" content="https://zhangweifeng.top/images/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E8%AF%8D%E8%BE%B9%E7%95%8C%E5%92%8C%E8%AF%8D%E5%86%85%E9%83%A8%E5%AD%97%E7%9A%84%E5%88%86%E5%B8%83%E5%9B%BE.png">
<meta property="og:image" content="https://zhangweifeng.top/images/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/7.2.7%E5%90%84%E6%8C%87%E6%A0%87%E8%AE%A1%E7%AE%97%E5%85%AC%E5%BC%8F.png">
<meta property="og:image" content="https://zhangweifeng.top/images/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E4%B8%83%E4%B8%AA%E5%88%86%E8%AF%8D%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%941.png">
<meta property="og:image" content="https://zhangweifeng.top/images/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E4%B8%83%E4%B8%AA%E5%88%86%E8%AF%8D%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%942.png">
<meta property="og:updated_time" content="2019-08-20T15:51:00.158Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="统计自然语言处理-自动分词">
<meta name="twitter:description" content="由于词是最小的能够独立运用的语言单位，而很多孤立语和黏着语（如汉语、日语、越南语、藏语等）的文本不像西方屈折语的文本，词与词之间没有任何空格之类的显式标志指示词的边界，因此，自动分词问题就成了计算机处理孤立语和黏着语文本时面临的首要基础性工作，是诸多应用系统不可或缺的一个重要环节。">
<meta name="twitter:image" content="https://zhangweifeng.top/images/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E5%88%87%E5%88%86%E6%9C%89%E5%90%91%E6%97%A0%E7%8E%AF%E5%9B%BE.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://zhangweifeng.top/2019/04/18/统计自然语言处理-自动分词/">





  <title>统计自然语言处理-自动分词 | 漂泊在学海</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">漂泊在学海</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <h1 class="site-subtitle" itemprop="description">学海无涯，回头是岸</h1>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://zhangweifeng.top/2019/04/18/统计自然语言处理-自动分词/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Arvin_Zhang">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="漂泊在学海">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">统计自然语言处理-自动分词</h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-18T10:14:35+08:00">
                2019-04-18
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2019-08-20T23:51:00+08:00">
                2019-08-20
              </time>
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/读书笔记/" itemprop="url" rel="index">
                    <span itemprop="name">读书笔记</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/04/18/统计自然语言处理-自动分词/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/04/18/统计自然语言处理-自动分词/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2019/04/18/统计自然语言处理-自动分词/" class="leancloud_visitors" data-flag-title="统计自然语言处理-自动分词">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

           

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  
                </span>
              

              

              
            </div>
          

          
              <div class="post-description">
                  由于词是最小的能够独立运用的语言单位，而很多孤立语和黏着语（如汉语、日语、越南语、藏语等）的文本不像西方屈折语的文本，词与词之间没有任何空格之类的显式标志指示词的边界，因此，自动分词问题就成了计算机处理孤立语和黏着语文本时面临的首要基础性工作，是诸多应用系统不可或缺的一个重要环节。
              </div>
          

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        <h1 id="汉语自动分词中的基本问题"><a href="#汉语自动分词中的基本问题" class="headerlink" title="汉语自动分词中的基本问题"></a>汉语自动分词中的基本问题</h1><p>简单地讲，汉语自动分词就是让计算机系统在汉语文本中的词与词之间自动加上空格或其他边界标记。这样一个看似简单的问题，却使几代学人扼腕感叹。其实归纳起来，汉语自动分词的主要困难来自如下三个方面：分词规范、歧义切分和未登录词的识别。</p>
<h2 id="汉语分词规范问题"><a href="#汉语分词规范问题" class="headerlink" title="汉语分词规范问题"></a>汉语分词规范问题</h2><p>正如刘开瑛（2000）指出的，“词”这个概念一直是汉语语言学界纠缠不清而又挥之不去的问题。“词是什么”（词的抽象定义）及“什么是词”（词的具体界定），这两个基本问题有点飘忽不定，迄今拿不出一个公认的、具有权威性的词表来。主要困难出自两个方面：一方面是单字词与词素之间的划界，另一方面是词与短语（词组）的划界。此外，对于汉语“词”的认识，普通说话人的语感与语言学家的标准也有较大的差异。有关专家的调查表明，在母语为汉语的被试者之间，对汉语文本中出现的词语的认同率只有大约70％，从计算的严格意义上说，自动分词是一个没有明确定义的问题［黄昌宁等，2003］。<br>1992年国家标准局颁布了作为国家标准的《信息处理用现代汉语分词规范》［刘源等，1994；刘开瑛，2000］。在这个规范中，大部分规定都是通过举例和定性描述来体现的。例如，规范4.2规定：“二字或三字词，以及结合紧密、使用稳定的二字或三字词组，一律为分词单位。”那么，何谓“紧密”，何谓“稳定”，人们在实际操作中都很难界定。在规范4.3、4.4、5.1.1.1、5.1.1.2、5.2.4、5.2.5、5.2.6等很多规定中都对分词单位有“结合紧密、使用稳定”的要求。这种规定的操作尺度很难把握，极易受主观因素的影响。因而使得《规范》并没有从根本上统一国人对汉语词的认识，哪怕只是在信息处理界。在这种情况下，建立公平公开的自动分词评测标准的努力也一样步履维艰［黄昌宁等，2003］。</p>
<h2 id="歧义切分问题"><a href="#歧义切分问题" class="headerlink" title="歧义切分问题"></a>歧义切分问题</h2><p>歧义字段在汉语文本中普遍存在，因此，切分歧义是汉语自动分词研究中一个不可避免的“拦路虎”。我国很多学者都对切分歧义问题进行了深入研究。梁南元（1987a）最早对歧义字段进行了比较系统的考察。他定义了以下两种基本的切分歧义类型。<br>定义7-1（交集型切分歧义）　汉字串AJB称作交集型切分歧义，如果满足AJ、JB同时为词（A、J、B分别为汉字串）。此时汉字串J称作交集串。</p>
<h2 id="未登录词问题"><a href="#未登录词问题" class="headerlink" title="未登录词问题"></a>未登录词问题</h2><p>未登录词又称为生词（unknown word），可以有两种解释：一是指已有的词表中没有收录的词；二是指已有的训练语料中未曾出现过的词。在第二种含义下，未登录词又称为集外词（out of vocabulary, OOV），即训练集以外的词。由于目前的汉语自动分词系统多采用基于大规模训练语料的统计方法，或者如果已有大规模训练语料（尤其是已做了人工分词标注的训练语料）便很容易获得词汇表，因此，通常情况下将OOV与未登录词看作一回事。<br>未登录词的情况比较复杂，可以粗略划分为如下几种类型：①新出现的普通词汇，如博客、超女、恶搞、房奴、给力、奥特等，尤其在网络用语中这种词汇层出不穷。②专有名词（proper names）。专有名词在早期主要是指人名、地名和组织机构名这三类实体名称。1996年第六届信息理解会议（The Sixth Message Understanding Conference, MUC-6）将这一术语进行了扩展，首次提出了命名实体（named entity）的概念，它除了包含上述三类实体名称以外，还包括时间和数字表达（日期、时刻、时段、数量值、百分比、序数、货币数量等），并且地名被进一步细化为城市名、州（省）名和国家名称等［Grishman and Sundheim, 1996; Nadeau and Sekine, 2007］。③专业名词和研究领域名称。特定领域的专业名词和新出现的研究领域名称也是造成生词的原因之一，如三聚氰胺、苏丹红、禽流感、堰塞湖等；④其他专用名词，如新出现的产品名，电影、书籍等文艺作品的名称，等等。根据黄昌宁等人（2003）的统计，在真实文本的切分中，未登录词总数的大约九成是专有名词（人名、地名、组织机构名），其余的为新词（包括专业术语）。当然，这个统计比例与语料所属的领域密切相关。<br>对于大规模真实文本来说，未登录词对于分词精度的影响远远超过了歧义切分。我们曾随机抽取了新浪等几个网站新闻领域的418个句子，共计含有19777个汉字，11739个词，利用自主开发的基于统计方法的Urheen汉语分词系统 〔1〕（详见7.2.5节）对这批句子进行了词语切分，结果产生了120个分词错误。</p>
<h1 id="汉语分词方法"><a href="#汉语分词方法" class="headerlink" title="汉语分词方法"></a>汉语分词方法</h1><p>自汉语自动分词问题被提出以来，经过众多专家的不懈努力，人们提出了很多分词方法。刘源等（1994）曾简要介绍了16种不同的分词方法，包括正向最大匹配法（forward maximum matching method, FMM）、逆向最大匹配法（backward maximum matching method, BMM）、双向扫描法、逐词遍历法等，这些方法基本上都是在20世纪80年代或者更早的时候提出来的。由于这些分词方法大多数都是基于词表进行的，因此，一般统称为基于词表的分词方法。随着统计方法的迅速发展，人们又提出了若干基于统计模型（包括基于HMM和n元语法）的分词方法，以及规则方法与统计方法相结合的分词技术，使汉语分词问题得到了更加深入的研究。<br>由于很多专著和论文已经对汉语自动分词方法作了详细介绍［刘源等，1994；梁南元，1987b；揭春雨，1989；朱巧明等，2005］，尤其是基于词表的分词方法和传统的基于HMM和n元语法的分词方法，因此，我们不再详述这些方法。本节主要介绍几种性能较好的基于统计模型的分词方法，并对这些分词技术进行简要的比较。</p>
<h2 id="N-最短路径方法"><a href="#N-最短路径方法" class="headerlink" title="N-最短路径方法"></a>N-最短路径方法</h2><p>考虑到汉语自动分词中存在切分歧义消除和未登录词识别两个主要问题，因此，有专家将分词过程分成两个阶段：首先采用切分算法对句子词语进行初步切分，得到一个相对最好的粗分结果，然后，再进行歧义排除和未登录词识别。当然，粗切分结果的准确性与包容性（即必须涵盖正确结果）直接影响后续的歧义排除和未登录词识别模块的效果，并最终影响整个分词系统的正确率和召回率。为此，张华平等（2002）提出了旨在提高召回率并兼顾准确率的词语粗分模型——基于N-最短路径方法的汉语词语粗分模型。这种方法的基本思想是：根据词典，找出字串中所有可能的词，构造词语切分有向无环图。每个词对应图中的一条有向边，并赋给相应的边长（权值）。然后针对该切分图，在起点到终点的所有路径中，求出长度值按严格升序排列（任何两个不同位置上的值一定不等，下同）依次为第1、第2、…、第i、…、第N（N≥1）的路径集合作为相应的粗分结果集。如果两条或两条以上路径长度相等，那么，它们的长度并列第i，都要列入粗分结果集，而且不影响其他路径的排列序号，最后的粗分结果集合大小大于或等于N。</p>
<p>假设待分字串S＝$c_1c_2…c_n$，其中，$c_i$（i＝1,2，…，n）为单个的汉字，n为字串的长度，n≥1。建立一个结点数为n＋1的切分有向无环图G，各结点编号依次为$V_0，V_1，V_2，…，V_n$。<br>通过以下两步建立G所有可能的词边：</p>
<p>（1）相邻结点$V_{k-1}，V_k$（1≤k≤n）之间建立有向边$V_{k-1}，V_k$，边的长度值为$L_k$，边对应的词默认为$c_k$（k＝1,2，…，n）。</p>
<p>（2）如果w＝$c_ic_{i＋1}…c_j$（0＜i＜j≤n）是词表中的词，则结点$V_{i-1}，V_j$之间建立有向边$V_{i-1}，V_j$，边的长度值为$L_w$，边对应的词为w。<br>这样，待分字串S中包含的所有词与切分有向无环图G中的边一一对应，如图7-1所示。<br><img src="/images/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E5%88%87%E5%88%86%E6%9C%89%E5%90%91%E6%97%A0%E7%8E%AF%E5%9B%BE.png" alt="切分有向无环图"><br>考虑到切分有向无环图G中每条边边长（或权重）的影响，张华平等人（2002）又将该方法分为非统计粗分模型和统计粗分模型两种。所谓的非统计粗分模型即假定切分有向无环图G中所有词的权重都是对等的，即每个词对应的边长均设为1。<br>假设NSP为结点$V_0$到$V_n$的前N个最短路径的集合，RS是最终的N-最短路径粗分结果集。那么，N-最短路径方法将词语粗分问题转化为如何求解有向无环图G的集合NSP。<br>求解有向无环图G的集合NSP可以采取贪心技术，张华平等（2002）使用的算法是基于求解单源最短路径问题的Dijkstra贪心算法的一种简单扩展。改进之处在于：每个结点处记录N个最短路径值，并记录相应路径上当前结点的前驱。如果同一长度对应多条路径，必须同时记录这些路径上当前结点的前驱，最后通过回溯即可求出NSP。<br>图7-2以句子“他说的确实在理”为例，给出了3-最短路径的求解过程。<br><img src="/images/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/3-%E6%9C%80%E7%9F%AD%E8%B7%AF%E5%BE%84%E5%88%86%E8%AF%8D%E8%BF%87%E7%A8%8B.png" alt="3-最短路径分词过程"><br>图7-2中，虚线是回溯出的是第一条最短路径，对应的粗分结果为：“他／说／的／确实／在理／”，Table（2），Table（3）…Table（7）分别为结点2、3、…、7对应的信息记录表，Table（0）、Table（1）的信息记录表没有给出。每个结点的信息记录表里的编号为路径不同长度的编号，按由小到大的顺序排列，编号最大不超过N。如Table（5）表示从结点0出发到达结点5有两条长度为4的路径（分别为0-1-2-4-5和0-1-2-3-5）和一条长度为5的路径（0-1-2-3-4-5）。前驱（i, j）表示沿着当前路径到达当前结点的最后一条边的出发结点为i，即当前结点的前一个结点为i，相应的边为结点i的信息记录表中编号为j的路径。如果j＝0，表示没有其他候选的路径。如结点7对应的信息记录表Table（7）中编号为1的路径前驱（5,1）表示前一条边为结点5的信息表中第1条路径。类似地，Table（5）中的前驱（3,1）表示前驱为结点3的信息记录表中的第1条路径。Table（3）中的（2,0）表示前驱边的出发点为结点2，没有其他候选路径。信息记录表为系统回溯找出所有可选路径提供了依据。<br>Dijkstra算法的时间复杂度为O（$n^2$），它求的是图中所有点到单源点的最短路径，而应用于切分有向图时，有两个本质区别：首先有向边的源点编号均小于终点编号，即所有边的方向一致；其次，算法最终求解的是有向图首尾结点之间的N-最短路径。因此，在该算法中，运行时间与n（字串长度）、N（最短路径数）以及某个字作为词末端字的平均次数k（等于总词数除以所有词末端字的总数，对应的是切分图中结点入度的平均值）成正比。所以，整个算法的时间复杂度是O（n×N×k）。</p>
<p>考虑到在非统计模型构建粗切分有向无环图的过程中，给每个词对应边的长度赋值为1。随着字串长度n和最短路径数N的增大，长度相同的路径数急剧增加，同时粗切分结果数量必然上升。例如，当N＝2时，句子“江泽民在北京人民大会堂会见参加全国法院工作会议和全国法院系统打击经济犯罪先进集体表彰大会代表时要求大家要充分认识打击经济犯罪工作的艰巨性和长期性”的粗切分结果居然有138种之多。这样，大量的切分结果对后期处理以及整个分词系统性能的提高非常不利。因此，张华平等人（2002）又给出了一种基于统计信息的粗分模型。</p>
<p>假定一个词串W经过信道传送，由于噪声干扰而丢失了词界的切分标志，到输出端便成了汉字串C。N-最短路径方法词语粗分模型可以相应地改进为：求N个候选切分W，使概率P（W|C）为前N个最大值：<br>$$<br>P(W | C)=\frac{P(W) P(C | W)}{P(C)}<br>$$<br>其中，P（C）是汉字串的概率，它是一个常数，不必考虑。从词串恢复到汉字串的概率P（C|W）＝1（只有唯一的一种方式）。<br>因此，粗分的目标就是确定P（W）最大的N种切分结果。为了简化计算，张华平等人采用一元统计模型。假设$W＝w_1w_2…w_m$是字串$S＝c_1c_2…c_n$的一种切分结果。$w_i$是一个词，P（$w_i$）表示词$w_i$出现的概率，在大规模语料训练的基础上通过最大似然估计方法求得。切分W的概率为<br>$$P(W)=\prod_{i=1}^{m} P\left(w_{i}\right)$$<br>为了处理方便，令$\mathrm{P}^{*}(\mathrm{W})=-\ln \mathrm{P}(\mathrm{W})=\sum_{i=1}^{m}\left[-\ln P\left(w_{i}\right)\right]$，这样，-lnP（$w_i$）就可以看作是词wi在切分有向无环图中对应的边长（做适当的数据平滑处理）。于是，求式（7-2）的最大值问题转化为求$P^\ast$（W）的最小值问题。<br>针对修改了边长后的切分有向无环图$G^\ast$，直接使用非统计粗分模型的求解算法，就可以获得问题的最终解。<br>张华平等（2002）通过使用185192个句子进行分词测试，在N＝10的情况下，非统计粗分模型和统计粗分模型切分句子的召回率分别为99.73％和99.94％，均高于最大匹配方法和最短路径方法获得的召回率。N＝10时统计粗分模型的召回率比全切分方法［马晏，1996］的召回率低0.06％，但粗分结果切分句子平均数仅为全切分方法的1/64。</p>
<h2 id="基于词的n元语法模型的分词方法"><a href="#基于词的n元语法模型的分词方法" class="headerlink" title="基于词的n元语法模型的分词方法"></a>基于词的n元语法模型的分词方法</h2><p>基于词的n元文法模型是一个典型的生成式模型，早期很多统计分词方法均以它为基本模型，然后配合其他未登录词识别模块进行扩展。其基本思想是：首先根据词典（可以是从训练语料中抽取出来的词典，也可以是外部词典）对句子进行简单匹配，找出所有可能的词典词，然后，将它们和所有单个字作为结点，构造的n元的切分词图，图中的结点表示可能的词候选，边表示路径，边上的n元概率表示代价，最后利用相关搜索算法（如Viterbi算法）从图中找到代价最小的路径作为最后的分词结果。以输入句子“研究生物学”为例，图7-3给出了基于二元文法的切分词图。<br><img src="/images/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E5%9F%BA%E4%BA%8E%E8%AF%8D%E7%9A%84%E7%94%9F%E6%88%90%E5%BC%8F%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BA%8C%E5%85%83%E6%96%87%E6%B3%95%E5%88%87%E5%88%86%E8%AF%8D%E5%9B%BE.png" alt="基于词的生成式模型的二元文法切分词图"><br>由于未登录词的识别是汉语分词过程中的关键问题之一，因此，很多专家认为未登录词的识别与歧义切分应该是一体化处理的过程，而不是相互分离的。Richard Sproat等人（1996）曾提出了基于加权的有限状态转换机（weighted finite-state transducer）模型与未登录词识别一体化切分的实现方法。受这种方法的启发，J.Gao等人（2003）提出了基于改进的信源信道模型的分词方法。现在简要介绍一下这种基于统计语言模型的分词方法。<br>为了给自动分词任务一个明确的定义，J.Gao等人（2003）对文本中的词给出了一个可操作的定义，把汉语词定义成下列4类：<br>（1）待切分文本中能与分词词表中任意一个词相匹配的字段为一个词。<br>（2）文本中任意一个经词法派生出来的词或短语为一个词，如重叠形式（高高兴兴，说说话、天天）、前缀派生（非党员、副部长）、后缀派生（全面性、朋友们）、中缀派生（看得出、看不出）、动词加时态助词（克服了、蚕食着）、动词加趋向动词（走出、走出来）、动词的分离形式（长度不超过3个字，如：洗了澡、洗过澡），等等。<br>（3）文本中被明确定义的任意一个实体名词（如：日期、时间、货币、百分数、温度、长度、面积、体积、重量、地址、电话号码、传真号码、电子邮件地址等）是一个词。<br>（4）文本中任意一个专有名词（人名、地名、机构名）是一个词。<br>在这个定义中没有考虑文本中的新词问题。另外需要注意的是，这个定义中很多约定与《信息处理用限定汉语分词规范（GB13715）》中的规定不一致，如，按照GB 13715国家分词规范，“AAB、ABAB”重叠形式的动词词组应予切分，例如：研究研究；按照GB 13715国家分词规范，除了“人们、哥儿们、爷儿们”等个别分词单位以外，仅表示前一个名词性分词单位复数的“们”应该单独切分，例如，朋友们，等等。但这些定义上的差异并不影响对分词算法的理解，在这里我们更关心的是基于统计语言模型的分词方法本身的问题。<br>假设随机变量S为一个汉字序列，W是S上所有可能切分出来的词序列，分词过程应该是求解使条件概率P（W|S）最大的切分出来的词序列$W^\ast$，即<br>$$<br>W^{\ast}=\underset{w}{\operatorname{argmax}} P(W | S) \quad (7-3)<br>$$<br>根据贝叶斯公式，式（7-3）改写为<br>$$<br>W^{\ast}=\underset{W}{\operatorname{argmax}} \frac{P(W) P(S | W)}{P(S)} \quad (7-4)<br>$$<br>由于分母为归一化因子，因此<br>$$<br>W^{\ast}=\underset{W}{\operatorname{argmax}} P(W) P(S | W)<br>$$<br>为了把4类词纳入同一个统计语言模型框架，黄昌宁等（2003）分别把专有名词的人名（PN）、地名（LN）、机构名（ON）各作为一类，实体名词中的日期（dat）、时间（tim）、百分数（per）、货币（mon）等作为一类处理，简称为实体名，对词法派生词（MW）和词表词（LW）则每个词单独作为一类。这样，按表7-2可以把一个可能的词序列W转换成一个可能的词类序列C＝c1c2…cN，那么，式（7-5）可被改写成式（7-6）：<br>$$<br>C^{\ast}=\underset{C}{\operatorname{argmax}} P(C) P(S | C) \quad (7-6)<br>$$<br>其中，P（C）就是大家熟悉的语言模型，我们不妨将P（S|C）称为生成模型。<br><img src="/images/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B7-2.png" alt="生成模型(S|C)"><br>根据第5章中对语言模型的介绍，如果P（C）采用三元语法，可以表示为<br>$$<br>P(C)=P\left(c_{1}\right) P\left(c_{2} | c_{1}\right) \prod_{i=3}^{\mathrm{N}} P\left(c_{i} | c_{i-2} c_{i-1}\right) \quad (7-7)<br>$$<br>三元模型的参数可以通过最大似然估计在一个带有词类别标记的训练语料上计算，并采用回退平滑算法解决数据稀疏问题。<br>生成模型在满足独立性假设的条件下，可以近似为<br>$$<br>P(S | C) \approx \prod_{i=1}^{\mathrm{N}} P\left(s_{i} | c_{i}\right)<br>\quad (7-8)<br>$$<br>式（7-8）认为，任意一个词类$c_i$生成汉字串$s_i$的概率只与$c_i$自身有关，而与其上下文无关。例如，如果“教授”是词表词，则P（$s_i$＝教授|$c_i$＝LW）＝1（见表7-2）。<br>在文献［黄昌宁等，2003］介绍的实验系统中，词表含有98668个词条，词法派生词表收入59285条派生词。训练语料由88MB新闻文本构成。模型的训练由以下三步组成：①在上述两个词表的基础上，用正向最大匹配法（FMM）切分训练语料，专有名词通过一个专门模块标注，实体名词通过相应的规则和有限状态自动机标注，由此产生一个带词类别标记的初始语料；②用带词类别标记的初始语料，采用最大似然估计方法估计统计语言模型的概率参数；③采用得到的语言模型对训练语料重新进行切分和标注（见式（7-6）～式（7-8）），得到一个刷新的训练语料。重复第②、③步，直到系统性能不再有明显的提高为止。<br>另外，对于交集型歧义字段（OAS），该方法的处理措施是：首先通过最大匹配方法（包括正向最大匹配和反向最大匹配）检测出这些字段，然后，用一个特定的类〈GAP〉取代全体OAS，依次来训练语言模型P（C）。类〈GAP〉的生成模型的参数通过消歧规则或机器学习方法来估计［黄昌宁等，2003；Li et al.,2003b］。<br>对于组合型歧义字段（CAS），该方法通过对训练语料的统计，选出最高频、且其切分分布比较均衡的70条CAS，用机器学习方法为每一条CAS训练一个二值分类器，再用这些分类器在训练语料中消解这些CAS的歧义。<br>微软亚洲研究院通过选自1997年《人民日报》的测试语料（包含经济、文化、政治、科技、法律、体育等10种题材和描写文、叙述文、说明文、应用文、口语等5种体裁），对上述基于统计语言模型的分词系统做了全面测试，结果证明该系统自动分词的正确率和召回率均优于正向最大分词方法，经过对实体名词、人名、地名和组织机构名识别处理后，该系统自动分词的正确率和召回率分别达到了96.3％和97.4％［黄昌宁等，2003］。</p>
<h2 id="由字构词的汉语分词方法"><a href="#由字构词的汉语分词方法" class="headerlink" title="由字构词的汉语分词方法"></a>由字构词的汉语分词方法</h2><p>由字构词（character-based tagging 〔2〕）的汉语分词方法由N.Xue（薛念文）等人提出，其论文发表在2002年的第一届国际计算语言学学会（ACL）汉语特别兴趣小组SIGHAN 〔3〕组织的研讨会上［Xue and Converse,2002］。2003年N.Xue等人在最大熵模型上实现的由字构词的汉语自动分词系统［Xue and Shen,2003］参加了第二届SIGHAN研讨会组织的首次汉语分词评测（Chinese Word Segmentation Bakeoff，以下简称Bakeoff或SIGHAN Bakeoff）［Sproat and Emerson, 2003］，在台湾中研院（Academia Sinica, AS）提供语料的封闭测试（closed test）项目上名列第二，其未登录词的召回率位居榜首。另外，该系统在香港城市大学（CITYU）提供语料的封闭测试中获得了第三名，其未登录词的召回率仍然是该项比赛中最高的。既然未登录词对于分词精度的影响比分词歧义对分词精度的影响大10倍，人们自然青睐这种能够获得最高未登录词召回率的分词方法。2005年和2006年的两次Bakeoff评测证实了这种预测。在2005年的Bakeoff评测中［Emerson,2005］，J.Low和H.Tseng等人实现的基于该方法的分词系统几乎分别囊括了开放测试和封闭测试的全部冠军［Low et al.,2005;Tseng et al.,2005］，只不过前者采用最大熵模型，而后者选用条件随机场模型。在2006年的Bakeoff评测中［Levow,2006］，微软亚洲研究院用条件随机场模型实现的由字构词的分词系统［Zhao et al.,2006a］参加了6项评测任务，获得了4个第一和两个第三的好成绩。<br>其实由字构词的汉语分词方法的思想并不复杂，它是将分词过程看作字的分类问题。在以往的分词方法中，无论是基于规则的方法还是基于统计的方法，一般都依赖于一个事先编制的词表，自动分词过程就是通过查词表作出词语切分的决策。与此相反，由字构词的分词方法认为每个字在构造一个特定的词语时都占据着一个确定的构词位置（即词位）。假如规定每个字只有4个词位：词首（B）、词中（M）、词尾（E）和单独成词（S），那么，下面句子（1）的分词结果就可以直接表示成如（2）所示的字标注形式［黄昌宁等，2006］。<br>（1）上海／计划／到／本／世纪／末／实现／人均／国内／生产／总值／五千美元／。/</p>
<p>（2）上／B海／E计／B划／E到／S本／S世／B纪／E末／S实／B现／E人／B均／E国／B内／E生／B产／E总／B值／E五／B千／M美／M元／E。/S<br>这里所说的“字”不仅限于汉字，也可以指标点符号、外文字母、注音符号和阿拉伯数字等任何可能出现在汉语文本中的文字符号，所有这些字符都是由字构词的基本单元。<br>分词结果表示成字标注形式之后，分词问题就变成了序列标注问题。对于一个含有n个字的汉语句子$c_1^n$＝c1c2…cn，可以用下面的公式来描述分词原理：<br>$$<br>P\left(t_{1}^{n} | c_{1}^{n}\right)=\prod_{k=1}^{n} P\left(t_{k} | t_{1}^{k-1}, c_{1}^{n}\right) \approx \prod_{k=1}^{n} P\left(t_{k} | t_{k-1}, c_{k-2}^{k-2}\right) \quad (7-9)<br>$$<br>其中，$t_k$表示第k个字的词位，即$t_k$∈{B, M, E, S}。<br>（a）$c_k$（k＝-2，-1,0,1,2）<br>（b）$c_kc_{k＋1}$（k＝-2，-1,0,1）<br>（c）$c_{－1}c_1$<br>（d）$T（c_{－2}）T（c_{－1}）T（c_0）T（c_1）T（c_2）$<br>前面三类特征模板（a）～（c）是窗口内的字及其组合特征，T（$c_i$）是指字$c_i$的字符类别，例如，阿拉伯数字，中文数字，标点符号，英文字母等等。假设当前字是“北京奥运会”中的“奥”字，那么模板（a）将生成以下特征：$c_{-2}$＝北，$c_{-1}$＝京，$c_0$＝奥，$c_1$＝运，$c_2$＝会；模板（b）：$c_{-2}c_{-1}$＝北京，$c_{-1}c_0$＝京奥，$c_0c_1$＝奥运，$c_1c_2$＝运会；模板（c）：$c_{-1}c_1$＝京运。模板（d）与定义的字符类别信息有关，主要是为了处理数字、标点符号和英文字符等有明显特征的词。有了这些特征以后，我们就可以利用常用的判别式模型，如最大熵、条件随机场、支持向量机和感知机（感知器）等进行参数训练，然后利用解码算法找到最优的切分结果。<br>由字构词的分词技术的重要优势在于，它能够平衡地看待词表词和未登录词的识别问题，文本中的词表词和未登录词都是用统一的字标注过程来实现的，分词过程成为字重组的简单过程。在学习构架上，既可以不必专门强调词表词信息，也不用专门设计特定的未登录词识别模块，因此，大大简化了分词系统的设计［黄昌宁等，2006］。</p>
<h2 id="基于词感知机算法的汉语分词方法"><a href="#基于词感知机算法的汉语分词方法" class="headerlink" title="基于词感知机算法的汉语分词方法"></a>基于词感知机算法的汉语分词方法</h2><p>在2007年的ACL国际大会上Y. Zhang和S. Clark提出了一种基于词的判别式模型，该模型采用平均感知机（averaged perceptron）［Collins, 2002a］作为学习算法，直接使用词相关的特征，而不是基于字的判别式模型中经常使用的字相关特征［Zhang and Clark, 2007, 2011］。<br>以下简要介绍平均感知机算法。假设x∈X是输入句子，y∈Y是切分结果，其中X是训练语料集合，Y是X中句子标注结果集合。我们用GEN（x）表示输入句子x的切分候选集，用Φ（x, y）∈Rd表示训练实例（x, y）对应的特征向量，α表示参数向量，其中Rd是模型的特征空间。那么，给定一个输入句子x，其最优切分结果满足如下条件：<br>$$<br>F(x)=\arg \max_{y \in \operatorname{GEN}(x)}\{\Phi(x, y) \cdot \alpha\} \quad (7-10)<br>$$<br>平均感知机用来训练参数向量α。首先将α中所有参数初始化为0，然后在训练解码过程中不断更新。对每一个训练样本，用当前的模型参数进行解码得到切分结果，如果切分结果与标注结果不一致，则更新模型参数。每一次都保留参数的加和，直到进行多轮迭代以后，取参数的平均值以避免模型过拟合。<br>平均感知机训练算法如下：<br><img src="/images/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E5%B9%B3%E5%9D%87%E6%84%9F%E7%9F%A5%E6%9C%BA%E8%AE%AD%E7%BB%83%E7%AE%97%E6%B3%95.png" alt="平均感知机训练算法"><br>基于感知机算法的汉语自动分词方法的基本思路是，对于任意给定的一个输入句子，解码器每次读一个字，生成所有的候选词。生成候选词的方式有两种：①作为上一个候选词的末尾，与上一个候选词组合成一个新的候选词；②作为下一个候选词的开始。这种方式可以保证在解码过程中穷尽所有的分词候选。在解码的过程中，解码器维持两个列表：源列表和目标列表。开始时，两个列表都为空。解码器每读入一个字，就与源列表中的每个候选组合生成两个新的候选（合并为一个新的词或者作为下一个词的开始），并将新的候选词放入目标列表。当源列表中的候选都处理完成之后，将目标列表中的所有候选复制到源列表中，并清空目标列表。然后，读入下一个字，如此循环往复直到句子结束。最后，从源列表中可以获取最终的切分结果。<br>解码算法描述如下：<br><img src="/images/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E5%9F%BA%E4%BA%8E%E8%AF%8D%E6%84%9F%E7%9F%A5%E6%9C%BA%E8%A7%A3%E7%A0%81%E7%AE%97%E6%B3%95.png" alt="基于词感知机解码算法"><br>这个算法有点类似于全切分方法，理论上会生成所有的$2_{l－1}$个切分结果（l为句长）。为了提升切分速度，可以对目标列表tgt中候选词的数目进行限制，每次只保留B个得分最高的候选（如B＝16）。那么，如何对tgt列表中的切分候选进行打分和排序呢？Y. Zhang和S. Clark使用了平均感知机作为学习算法，使用的特征如表7-3所示。<br><img src="/images/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E5%9F%BA%E4%BA%8E%E8%AF%8D%E7%9A%84%E5%88%A4%E5%88%AB%E5%BC%8F%E6%A8%A1%E5%9E%8B%E4%BD%BF%E7%94%A8%E7%9A%84%E7%89%B9%E5%BE%81.png" alt="基于词的判别式模型使用的特征"><br>注：w表示词，c表示汉字，当前字和词的下标为0，函数len（·）用于计算词的长度，函数start（·）和end（·）分别用于取词的开始字和末端字。在［Zhang and Clark, 2007］中没有把c0区分成“separated”和“appended”两种情况，而是对所有的特征模板不分情况统一使用。<br>例如，假定源列表src中有一个候选为“如今／有些／露宿”，当解码器读入一个新的字“者”时，就会生成两个候选：（a）“如今／有些／露宿者”；（b）“如今／有些／露宿／者”。候选（a）将激活表7-3中编号为1～6和8～14的特征，候选（b）将激活编号为7的特征。这里$c_0$为“者”。表7-4详细描述了两个候选（a）和（b）对应的特征。<br>表7-4　候选（a）和（b）对应的特征<br><img src="/images/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E5%80%99%E9%80%89%EF%BC%88a%EF%BC%89%E5%92%8C%EF%BC%88b%EF%BC%89%E5%AF%B9%E5%BA%94%E7%9A%84%E7%89%B9%E5%BE%81.png" alt="候选（a）和（b）对应的特征"><br>根据这些特征，就可以利用平均感知机分类器对切分候选进行打分和排序。根据Zhang and Clark （2007）的实验，该方法使用2003年SIGHAN Bakeoff评测的AS语料和CITYU语料做训练集和测试集，F1值 〔4〕分别达到了96.5％和94.6％；使用2005年第二次SIGHAN Bakeoff评测的CITYU语料和微软研究院的语料（记作MSR）做训练集和测试集，F-测度值分别达到了95.1％和97.2％，都是当时的最高水平。在［Zhang and Clark, 2011］中，将$c_0$分情况采用不同的特征模板后，分词性能略有提高。</p>
<h2 id="基于字的生成式模型和区分式模型相结合的汉语分词方法"><a href="#基于字的生成式模型和区分式模型相结合的汉语分词方法" class="headerlink" title="基于字的生成式模型和区分式模型相结合的汉语分词方法"></a>基于字的生成式模型和区分式模型相结合的汉语分词方法</h2><p>根据前面的介绍，在汉语分词中基于词的n元语法模型（生成式模型）和基于字的序列标注模型（区分式模型）是两大主流方法。其中，基于词的生成式模型对于集内词（词典词）的处理可以获得较好的性能表现，而对集外词（未登录词）的分词效果欠佳；基于字的区分式模型则恰好相反，它一般对集外词的处理有较好的鲁棒性，对集内词的处理却难以获得很好的性能，比基于词的生成式模型差很多［Zhang et al., 2006a, 2006b; Wang et al., 2012］。其主要原因归结为两种方法采用的不同处理单元（分别是字和词）和模型（分别为生成式模型和区分式模型），在基于字的区分式模型中，将基本单位从词换成字以后，所有可能的“字-标记”对（character-tag-pairs）候选集要远远小于所有可能的词候选集合。［Wang et al., 2012］通过实验分析发现，两个处于词边界的字之间的依赖关系和两个处于词内部的字之间的依赖关系是不一样的。图7-4给出了logP（$c_i|c_{i－1}$）在两种情况下的分布图。<br><img src="/images/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E8%AF%8D%E8%BE%B9%E7%95%8C%E5%92%8C%E8%AF%8D%E5%86%85%E9%83%A8%E5%AD%97%E7%9A%84%E5%88%86%E5%B8%83%E5%9B%BE.png" alt="词边界和词内部字的分布图"><br>基于词的生成式模型实际上隐含地考虑了这种处于不同位置字之间的依赖关系，而在基于字的判别式模型中却无法考虑这种依赖关系。但是，区分式模型能够充分利用上下文特征信息等，有较大的灵活性。因此，基于字的区分式模型具有较强的鲁棒性。基于这种考虑，［Wang et al., 2009, 2010a, 2012］提出了利用基于字的n元语法模型以提高其分词的鲁棒性，并将基于字的生成式模型与区分式模型相结合的汉语分词方法，获得了很好的分词效果。<br>在［Wang et al., 2009, 2012］提出的基于字的生成式模型中，将词替换成相应的“字-标记”对，即<br>$$<br>P\left(w_{1}^{m} | c_{1}^{n}\right) \equiv P\left(\lbrack c,t \rbrack_{1}^{\pi} | c_{1}^{n}\right)=\frac{P\left(c_{1}^{n} |\lbrack c,t \rbrack_{1}^{n}\right) \times P\left(\lbrack c, t \rbrack_{1}^{n}\right)}{P\left(c_{1}^{n}\right)}<br>\quad (7-11)<br>$$</p>
<p>其中，$c_1^n$＝c1c2…cn为含有n个字的汉语句子，$w_1^m$＝$w_1…w_m$表示m个词，为n个“字-标记”对，$t_k$∈{B, M, E, S}。根据贝叶斯公式并参考式三元语言模型的计算公式：<br>$$<br>P\left(w_{1}^{m}\right)=\prod_{i=1}^{m} P\left(w_{i} | w_{1}^{i-1}\right) \approx \prod_{i=1}^{m} P\left(w_{i} | w_{i-2}^{i-1}\right)<br>\quad (7-12)<br>$$<br>式（7-11）可以进一步简化为：<br>$$<br>P\left(\lbrack c,t\rbrack_{1}^{n}\right) \approx \prod_{i=1}^{n} P\left(\lbrack c, t\rbrack_{i} |\rbrack c,t\lbrack_{i-k}^{i-1}\right)<br>\quad (7-12)<br>$$<br>这样，基于字的生成式模型仍然以字作为基本单位，但考虑了字与字之间的依赖关系，与基于字的判别式模型相比，处理词典词的能力有了大幅改观。但是，该模型仍然有缺陷，它并没有考虑未来信息（当前字后面的上下文）。例如，“露宿者”中的“者”字，是一个明显的后缀，当使用基于字的判别式模型切分“宿”时，能够利用后续信息判断“宿”应该标记为“M”，而基于字的生成式模型却由于只考虑了“宿”字左边的上下文，错误地将“宿”标为“E”。因此，基于字的生成式模型处理未登录词的能力仍然弱于基于字的判别式模型。为了利用基于字的判别式模型和基于字的生成式模型对词典词和未登录词进行互补性处理，［Wang et al., 2010a, 2012］利用线性插值法将这两个模型进行了整合，提出了一个集成式分词模型：<br>$$<br>\text { Score }\left(t_{k}\right)=\alpha \times \log \left(P\left(\lbrack,t\rbrack_{k} |\lbrack,t\rbrack_{k-2}^{k-1}\right)\right)+(1-\alpha) \times \log \left(P\left(t_{k} | t_{k-1}, c_{k-2}^{k-2}\right)\right)<br>\quad (7-14)<br>$$<br>其中α（0≤α≤1.0）为加权因子。</p>
<p>该模型同时融合了基于字的生成模型和基于字的判别式模型的优点，因此，分词性能比两个基本模型有了大幅度的提升。在2005年SIGHAN Bakeoff评测的AS、CITYU、MSR和北京大学语料（记作PKU）四种语料上“封闭测试”（这里指仅使用SIGHAN Bakeoff允许使用的语料）和“开放测试”（指允许使用任何数据和语言学知识）的综合分词性能（F1值）分别达到了95.7％和96.2％。值得指出的是，该方法对于数字表达、标点和外文字符的处理能力在不同的测试集上表现出非常稳定的良好性能［Wang et al., 2012］。基于这种方法实现的汉语自动分词工具Urheen已公开发布在下面的网站上：<a href="http://www.openpr.org.cn/index.php/NLP-Toolkit-for-Natural-Language-Processing/。" target="_blank" rel="noopener">http://www.openpr.org.cn/index.php/NLP-Toolkit-for-Natural-Language-Processing/。</a></p>
<h2 id="其他分词方法"><a href="#其他分词方法" class="headerlink" title="其他分词方法"></a>其他分词方法</h2><p>长期以来，对汉语自动分词问题感兴趣的学者们从来没有停止过对新方法的探索，不断尝试把机器学习和处理自然语言其他问题的新方法引入分词问题研究。Wu（2003a）曾考虑到一般汉语自动分词方法对歧义消解的局限性，尝试了将分词与句法分析技术融为一体的方法，用整个句子的句法结构来消除不正确的切分。这种方法对消解组合型歧义十分有效，如分辨“才能”或“将来”在某个特定的句子里究竟是一个词还是两个词。但遗憾的是，组合型歧义在切分歧义中毕竟占少数，而在频繁出现的交集型歧义的消解方面，使用句子分析器并没有明显优势，所以在他的实验中句法分析器并没有对分词算法起到显著的辅助效果。这里的根本原因是句法分析本身就有很多歧义，对于某些句子，句法分析器反而会产生误导，例如，名字“王爱民”可能被分析成一个句子，姓氏“王”被分析成主语，“爱”被分析成动词谓语，而“民”被分析成宾语。另外，句法分析器的语法规则很难涵盖所有的语言现象，有些句子“不合语法”，所以根本就无法分析。但无论如何，我们应该看到将句法分析方法与自动分词算法相结合的实现策略，很好地体现了词法与句法相互作用的一体化处理思路。<br>Gao et al. （2005）提出了一种汉语分词的语用方法（pragmatic approach）。该方法与多数已有方法的主要区别有三点：第一，在传统的分词方法中一般采用理论语言学家根据各种语言学标准给出的汉语词的定义，而在本方法中汉语的词是根据它们在实际使用和处理中的需要从语用上定义的切分单位；第二，该方法中提出了一种语用数学框架，在这个框架中切分已知词和检测不同类型的生词能够以一体化的方式同步进行；第三，在这种方法中假设不存在独立于具体应用的通用切分标准，而是认为根据不同的语言处理任务需要多重切分标准和不同的词汇粒度。该方法已经应用于微软亚洲研究院开发的适应性汉语分词系统（MSRSeg）中。关于第一点和第三点的处理思路，A. Wu早在文献［Wu, 2003b］中就有详细的讨论。<br>自从薛念文等人提出由字构词的分词方法以后，该模型迅速成为汉语分词的主流方法。在2005年SIGHAN Bakeoff评测任务上，所有获得第一名的系统［Low et al., 2005; Tseng et al., 2005］都采用了基于字的判别式模型。为了提升该模型的词典词召回率（recall），张瑞强等人提出了基于“子词”（sub-word）的判别式模型方法：首先用最大匹配方法切分出常用词，然后将sub-word和字混合作为标注的基本单位［Zhang et al., 2006a, 2006b］。赵海等人还比较了不同词位数量对该模型的影响，他们的实验表明，基于6个词位的效果最好［Zhao et al., 2006a, 2010］。从近几年的研究情况来看，基于字的判别式模型仍然是汉语分词的主流方法。<br>基于词的判别式模型［Zhang and Clark, 2007］有效提升了基于词的分词方法的性能。孙薇薇比较了基于字的判别式模型和基于词的判别式模型的性能，并利用Bagging将两种方法结合，取得了不错的效果［Sun, 2010］。<br>另外值得指出的是，将汉语分词与词性标注两项任务同时进行，以达到同时提升两项任务性能的目的，一直是这一领域研究的一个重要方向，这种方法往往需要耗费更多的时间代价。关于这方面的研究工作，除了20世纪90年代到2000年前后发表的一些早期论文以外，近年来又发表了一些研究成果，包括［Jiang et al., 2008; Zhang and Clark, 2010; Kruengkrai et al., 2009］，有兴趣的读者可以参考这些文献。</p>
<h2 id="分词方法比较"><a href="#分词方法比较" class="headerlink" title="分词方法比较"></a>分词方法比较</h2><p>自开展汉语自动分词方法研究以来，人们提出的各类方法不下几十种甚至上百种，不同方法的性能各不相同，尤其在不同领域、不同主题和不同类型的汉语文本上，性能表现出明显的差异。刘源等（1994）对早期的汉语分词方法和分词规范做了全面介绍；李东（2003）曾对最大匹配分词算法、全切分方法、最短路径方法和基于规则与统计方法相结合的分词方法等多种方法进行了实验对比，并对各种方法的优缺点进行了简要分析；黄昌宁等（2007）对2007年之前近10年的汉语自动分词技术做了全面回顾和归纳。这些优秀的工作都在汉语自动分词技术研究中产生了重要影响。<br>为了对近几年来提出的一些具有代表性的统计分词方法有一个比较直观的对比，我们在2005年SIGHAN Bakeoff评测语料上进行了性能对比实验。实验语料包括三个不同分词标准的数据集：AS、CITYU和MSR。这些语料的统计情况见表7-5。<br>![SIGHAN Bakeoff 2005的三种分词语料](/images/读书笔记/统计自然语言处理/SIGHAN Bakeoff 2005的三种分词语料.png)<br>由于在PKU语料的训练集和测试集中，阿拉伯数字和英文字母使用了不同的编码。在训练集中它们都是以全角方式存在的，而在测试集中它们却是以半角形式出现的。在已发表的文献中，大多数研究者在使用PKU测试语料时都进行了编码一致性转换［Xiong et al., 2009］，但也有一些并未做这种转换。因此，我们能够看到的在PKU语料上的测试结果并没有统一的可比性，所以以下只给出在AS、CITYU和MSR三种语料上的测试结果。<br>为了公平地对比各种分词方法，我们采用SIGHAN规定的“封闭测试”原则：模型训练和测试过程中，仅允许使用SIGHAN提供的数据集进行训练和测试，其他任何语料、词典、人工知识和语言学规则都不能使用。评价指标包括：准确率（P）、召回率（R）、F-测度（F-measure，简写为F）、未登录词的召回率（$R_{OOV}$）和词典词的召回率（$R_{IV}$）。各指标的计算公式如下：<br><img src="/images/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/7.2.7%E5%90%84%E6%8C%87%E6%A0%87%E8%AE%A1%E7%AE%97%E5%85%AC%E5%BC%8F.png" alt="7.2.7各指标计算公式"><br>我们比较的系统包括（见表7-5）：2005年SIGHAN Bakeoff评测中在AS语料上取得第一名的系统［Asahara et al., 2005］（简记为①Asahara05），在MSR、AS和PKU语料上取得第一名的系统［Tseng et al., 2005］ （简记为②Tseng05），基于“子词”（sub-word）标注和CRF模型的分词系统［Zhang et al., 2006a, 2006b］（简记为③Zhang06），基于词感知机算法的分词系统［Zhang and Clark, 2007］ （简记为④Z&amp;C07），基于级联式线性模型的分词与词性标注联合系统［Jiang et al., 2008］（简记为⑤Jiang08），基于词和字混合模型的分词系统［Sun, 2010］（简记为⑥Sun10），以及7.2.5节介绍的基于字的生成式模型和区分式模型相结合的汉语分词系统［Wang et al., 2010a, 2012］ （简记为⑦Wang10）。<br><img src="/images/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E4%B8%83%E4%B8%AA%E5%88%86%E8%AF%8D%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%941.png" alt="七个分词系统性能对比1"><br><img src="/images/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E4%B8%83%E4%B8%AA%E5%88%86%E8%AF%8D%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%942.png" alt="七个分词系统性能对比2"><br>表7-6中“N/A”表示我们所参考的相关文献没有提供相应的数据。从表中的数据可以看出，这些系统的性能都达到了相当高的水平。</p>
<p>总之，随着自然语言处理技术整体水平的提高，尤其近几年来新的机器学习方法和大规模计算技术在汉语分词中的应用，分词系统的性能一直在不断提升。特别是在一些通用的书面文本上，如新闻语料，领域内测试（训练语料和测试语料来自同一个领域）的性能已经达到相当高的水平。但是，跨领域测试的性能仍然很不理想，例如用计算机领域或者医学领域的测试集测试用新闻领域的数据训练出来的模型。由于目前具有较好性能的分词系统都是基于有监督的学习方法训练出来的，需要大量有标注数据的支撑，而标注各个不同领域的语料需要耗费大量的人力和时间，因此，如何提升汉语自动分词系统的跨领域性能仍然是目前面临的一个难题。</p>
<p>另外，随着互联网和移动通信技术的发展，越来越多的非规范文本大量涌现，如微博、博客、手机短信等。正如7.1.3节的分析，这些网络文本与正规出版的书面文本有很大的不同，大多数情况下它们不符合语言学上的语法，并且存在大量网络新词和流行语，有很多还是非正常同音字或词的替换，例如，“温拿”、“卢瑟”等。这些文字的出现都给分词带来了一定的困难，传统的分词方法很难直接使用，而且这些网络新词的生命周期都不是很长，消亡较快，所以，很难通过语料规模的随时扩大和更新来解决这些问题。值得庆幸的是，研究人员已经关注到这些问题，并开始研究。</p>
<p>综上所述，汉语分词是中文信息处理研究的一项基础性工作，经过几十年的研究开发，已经取得了丰硕的成果，但仍面临若干颇具挑战性的难题，这就需要我们继续努力，与时俱进，为解决这些难题而不懈奋斗。</p>
<p>值得提及的是，藏语与汉语一样，词与词之间没有分隔标记，因此，藏文信息处理也存在分词的问题。近年来，部分专家已尝试将汉语分词方法用于藏文分词，并根据藏文自身的特点进行了相应的改进，取得了一定的进展。例如，史晓东等（2011）直接将一个基于HMM的汉语分词系统移植到了藏文分词，取得了91％的准确率。Liu et al.（2010a, 2011b）研究了藏文分词中的数字识别问题，并且实现了基于音节标注的藏文分词方法，将分词和紧缩词识别融合在一个统一的标注体系中。李亚超等（2013）借鉴汉语分词中由字构词的分词思想，提出了一种基于CRF的藏语紧缩词识别方法，并实现了基于字标注的藏文分词系统，紧缩词识别和藏文分词的F1性能均达到了95％。有关这些工作的具体内容，请参阅相关文献，这里不再一一叙述。</p>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p>《统计自然语言处理》（第二版）  宗成庆 著 清华大学出版社 <a href="/books/mobi/统计自然语言处理（第2版）.mobi">统计自然语言处理.mobi下载</a></p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/nlp/" rel="tag"><i class="fa fa-tag"></i> nlp</a>
          
            <a href="/tags/统计自然语言处理/" rel="tag"><i class="fa fa-tag"></i> 统计自然语言处理</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/04/15/统计自然语言处理-概率图模型/" rel="next" title="统计自然语言处理-概率图模型">
                <i class="fa fa-chevron-left"></i> 统计自然语言处理-概率图模型
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/04/23/统计自然语言处理-命名实体识别/" rel="prev" title="统计自然语言处理-命名实体识别">
                统计自然语言处理-命名实体识别 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
    </div>
  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.png" alt="Arvin_Zhang">
            
              <p class="site-author-name" itemprop="name">Arvin_Zhang</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">16</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">5</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">10</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/zhangweifeng919" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:zhangweifeng919@gmail.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#汉语自动分词中的基本问题"><span class="nav-number">1.</span> <span class="nav-text">汉语自动分词中的基本问题</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#汉语分词规范问题"><span class="nav-number">1.1.</span> <span class="nav-text">汉语分词规范问题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#歧义切分问题"><span class="nav-number">1.2.</span> <span class="nav-text">歧义切分问题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#未登录词问题"><span class="nav-number">1.3.</span> <span class="nav-text">未登录词问题</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#汉语分词方法"><span class="nav-number">2.</span> <span class="nav-text">汉语分词方法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#N-最短路径方法"><span class="nav-number">2.1.</span> <span class="nav-text">N-最短路径方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#基于词的n元语法模型的分词方法"><span class="nav-number">2.2.</span> <span class="nav-text">基于词的n元语法模型的分词方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#由字构词的汉语分词方法"><span class="nav-number">2.3.</span> <span class="nav-text">由字构词的汉语分词方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#基于词感知机算法的汉语分词方法"><span class="nav-number">2.4.</span> <span class="nav-text">基于词感知机算法的汉语分词方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#基于字的生成式模型和区分式模型相结合的汉语分词方法"><span class="nav-number">2.5.</span> <span class="nav-text">基于字的生成式模型和区分式模型相结合的汉语分词方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#其他分词方法"><span class="nav-number">2.6.</span> <span class="nav-text">其他分词方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#分词方法比较"><span class="nav-number">2.7.</span> <span class="nav-text">分词方法比较</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#参考资料"><span class="nav-number">3.</span> <span class="nav-text">参考资料</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Arvin_Zhang</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Site words total count&#58;</span>
    
    <span title="Site words total count"></span>
  
</div>











<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
    console.log("已经推送");
})();
</script>

        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  










  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item=>{
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: false,
        appId: 'kjMy4Wir9L7O2wDtjHexeyl8-gzGzoHsz',
        appKey: 'zruMkYmFF2lxyMAreQ8AdUWr',
        placeholder: '欢迎提问，一起讨论\(^o^)/~',
        avatar:'mm',
        guest_info:guest,
        pageSize:'10' || 10,
    });
  </script>



  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("kjMy4Wir9L7O2wDtjHexeyl8-gzGzoHsz", "zruMkYmFF2lxyMAreQ8AdUWr");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
