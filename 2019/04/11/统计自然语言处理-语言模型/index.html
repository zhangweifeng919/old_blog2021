<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.9.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">






  
  
  <link rel="stylesheet" media="all" href="/lib/Han/dist/han.min.css?v=3.3">




<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="nlp,统计自然语言处理,">










<meta name="description" content="语言模型（language model, LM）在自然语言处理中占有重要的地位，尤其在基于统计模型的语音识别、机器翻译、汉语自动分词和句法分析等相关研究中得到了广泛应用。目前主要采用的是n元语法模型（n-gram model），这种模型构建简单、直接，但同时也因为数据缺乏而必须采取平滑（smoothing）算法。">
<meta name="keywords" content="nlp,统计自然语言处理">
<meta property="og:type" content="article">
<meta property="og:title" content="统计自然语言处理-语言模型">
<meta property="og:url" content="https://zhangweifeng.top/2019/04/11/统计自然语言处理-语言模型/index.html">
<meta property="og:site_name" content="漂泊在学海">
<meta property="og:description" content="语言模型（language model, LM）在自然语言处理中占有重要的地位，尤其在基于统计模型的语音识别、机器翻译、汉语自动分词和句法分析等相关研究中得到了广泛应用。目前主要采用的是n元语法模型（n-gram model），这种模型构建简单、直接，但同时也因为数据缺乏而必须采取平滑（smoothing）算法。">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://zhangweifeng.top/images/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/Kneser-Ney%E5%B9%B3%E6%BB%91%E6%96%B9%E6%B3%95%E6%8E%A8%E5%AF%BC.png">
<meta property="og:image" content="https://zhangweifeng.top/images/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E5%B9%B3%E6%BB%91%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93.png">
<meta property="og:updated_time" content="2019-08-20T15:51:00.160Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="统计自然语言处理-语言模型">
<meta name="twitter:description" content="语言模型（language model, LM）在自然语言处理中占有重要的地位，尤其在基于统计模型的语音识别、机器翻译、汉语自动分词和句法分析等相关研究中得到了广泛应用。目前主要采用的是n元语法模型（n-gram model），这种模型构建简单、直接，但同时也因为数据缺乏而必须采取平滑（smoothing）算法。">
<meta name="twitter:image" content="https://zhangweifeng.top/images/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/Kneser-Ney%E5%B9%B3%E6%BB%91%E6%96%B9%E6%B3%95%E6%8E%A8%E5%AF%BC.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://zhangweifeng.top/2019/04/11/统计自然语言处理-语言模型/">





  <title>统计自然语言处理-语言模型 | 漂泊在学海</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">漂泊在学海</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <h1 class="site-subtitle" itemprop="description">学海无涯，回头是岸</h1>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://zhangweifeng.top/2019/04/11/统计自然语言处理-语言模型/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Arvin_Zhang">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="漂泊在学海">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">统计自然语言处理-语言模型</h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-11T16:13:35+08:00">
                2019-04-11
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2019-08-20T23:51:00+08:00">
                2019-08-20
              </time>
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/读书笔记/" itemprop="url" rel="index">
                    <span itemprop="name">读书笔记</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/04/11/统计自然语言处理-语言模型/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/04/11/统计自然语言处理-语言模型/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2019/04/11/统计自然语言处理-语言模型/" class="leancloud_visitors" data-flag-title="统计自然语言处理-语言模型">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

           

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  
                </span>
              

              

              
            </div>
          

          
              <div class="post-description">
                  语言模型（language model, LM）在自然语言处理中占有重要的地位，尤其在基于统计模型的语音识别、机器翻译、汉语自动分词和句法分析等相关研究中得到了广泛应用。目前主要采用的是n元语法模型（n-gram model），这种模型构建简单、直接，但同时也因为数据缺乏而必须采取平滑（smoothing）算法。
              </div>
          

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        <h1 id="n元语法"><a href="#n元语法" class="headerlink" title="n元语法"></a>n元语法</h1><p>一个语言模型通常构建为字符串s的概率分布p（s），这里p（s）试图反映的是字符串s作为一个句子出现的频率。<br>对于一个由l个基元（“基元”可以为字、词或短语等，为了表述方便，以后我们只用“词”来通指）构成的句子$s＝w_1w_2…w_l$，其概率计算公式可以表示为<br>\begin{align}<br>p(s) &amp;=p(w_1)p(w_2|w_1)p(w_3|w_1w_2)…p(w_l|w_1…w_{l-1}) \\<br>&amp;= \prod_{i=1}^lp(w_i | w_1…w_{i-1})<br>\end{align}<br>一般地，我们把前i-1个词$w_1w_2…w{i-1}$称为第i个词的“历史（history）”。在这种计算方法中，随着历史长度的增加，不同的历史数目按指数级增长。如果历史的长度为i-1，那么，就有$L_{i-1}$种不同的历史（假设L为词汇集的大小），而我们必须考虑在所有$L_{i-1}$种不同的历史情况下，产生第i个词的概率。这样的话，模型中就有$L_i$个自由参数$p(w_i|w_1，w_2，…，w_{i-1})$。假设L＝5000，i＝3，那么，自由参数的数目就是1250亿个［翁富良等，1998］！这使我们几乎不可能从训练数据中正确地估计出这些参数，实际上，绝大多数历史根本就不可能在训练数据中出现。因此，为了解决这个问题，可以将历史$w_1w_2…w_{i-1}$按照某个法则映射到等价类$E(w_1w_2…w_{i-1})$，而等价类的数目远远小于不同历史的数目。如果假定：<br>$$ p(w_i | w_1…w_{i-1})=p(w_i | E(w_1w_2…w_{i-1})$$<br>那么，自由参数的数目就会大大地减少。有很多方法可以将历史划分成等价类，其中，一种比较实际的做法是，将两个历史$w_{i-n＋2}…w_{i-1}w_i$和$v_{k-n＋2}…v_{k-1}v_k$映射到同一个等价类，当且仅当这两个历史最近的n-1（1≤n≤l）个词相同，即如果$E(w_1w_2…w_{i-1}w_i)＝E(v_1v_2…v_{k-1}v_k)$，当且仅当$(w_{i-n＋2}…w_{i-1}w_i)＝(v_{k-n＋2}…v_{k-1}v_k)$。<br>满足上述条件的语言模型称为n元语法或n元文法（n-gram）。通常情况下，n的取值不能太大，否则，等价类太多，自由参数过多的问题仍然存在。在实际应用中，取n＝3的情况较多。当n＝1时，即出现在第i位上的词$w_i$独立于历史时，一元文法被记作unigram，或uni-gram，或monogram；当n＝2时，即出现在第i位上的词$w_i$仅与它前面的一个历史词$w_{i-1}$有关，二元文法模型被称为一阶马尔可夫链（Markov chain），记作bigram或bi-gram；当n＝3时，即出现在第i位置上的词$w_i$仅与它前面的两个历史词$w_{i-2}w_{i-1}$有关，三元文法模型被称为二阶马尔可夫链，记作trigram或tri-gram。<br>以二元语法模型为例，根据前面的解释，我们可以近似地认为，一个词的概率只依赖于它前面的一个词，那么，<br>$$ p(s) = \prod_{i=1}^lp(w_i | w_1…w_{i-1}) \approx  p(w_i | w_{i-1})  \qquad  (5-3)$$<br>为了使得$p(w_i|w_{i－1})$对于i＝1有意义，我们在句子开头加上一个句首标记〈BOS〉，即假设$w_0$就是〈BOS〉。另外，为了使得所有的字符串的概率之和$\sum_s p(s)$等于1，需要在句子结尾再放一个句尾标记〈EOS〉，并且使之包含在等式（5-3）的乘积中（如果不做这样的处理，所有给定长度的字符串的概率和为1，而所有字符串的概率和为无穷大）。例如，要计算概率p（Mark wrote a book），我们可以这样计算：</p>
<p>$$ p(Mark wrote a book) =p(Mark |〈BOS〉) \times p(wrote |Mark) \\<br>\times p(a | wrote) \times p(book |a) \times p(〈EOS〉| book) $$<br>为了估计$p（w_i|w_{i－1}）$条件概率，可以简单地计算二元语法$w_{i-1}w_i$在某一文本中出现的频率，然后归一化。如果用$c（w_{i－1}w_i）$表示二元语法$w_{i-1}w_i$在给定文本中的出现次数，我们可以采用下面的计算公式：</p>
<p>$$p(w_i | w_{i-1})=\cfrac{c(w_{i-1}w_i)}{\sum_{w_i} c(w_{i-1}w_i)}  \qquad (5-4)$$</p>
<p>用于构建语言模型的文本称为训练语料（training corpus）。对于n元语法模型，使用的训练语料的规模一般要有几百万个词。公式（5-4）用于估计$p（w_i|w_{i-1}）$的方法称为$p（w_i|w_{i-1}）$的最大似然估计（maximum likelihood estimation, MLE）。</p>
<h1 id="语言模型性能评价"><a href="#语言模型性能评价" class="headerlink" title="语言模型性能评价"></a>语言模型性能评价</h1><p>评价一个语言模型最常用的度量就是根据模型计算出的测试数据的概率，或者利用第2章里曾经介绍的<a href="/2019/04/06/统计自然语言处理-预备知识/#交叉熵">交叉熵（cross-entropy）</a>和<a href="/2019/04/06/统计自然语言处理-预备知识/#困惑度">困惑度（perplexity）</a>等派生测度。对于一个平滑过的概率为$p（w_i|w_{i-n+1}^{i-1}）$的n元语法模型，用公式计算句子p（s）的概率。对于句子$（t_1，t_2，…，t_{l_T}）$构成的测试集T，可以通过计算T中所有句子概率的乘积来计算测试集的概率p（T）：</p>
<p>$$p(T)= \prod_{i=1}^{l_T}p(t_i)$$</p>
<p>交叉熵的测度可以利用预测和压缩的关系来进行计算。当给定一个语言模型，文本T的概率为p（T），可以给出一个压缩算法，该算法用$-\log_2 p（T）$个比特位来对文本T编码。根据第2章的介绍，在数据T上模型$p（wi|w_{i-n+1}^{i-1}）$的交叉熵$H_p（T）$定义为</p>
<p>$$H_p(T) = - \cfrac{1}{W_T} \log_2 p(T) \qquad (5 - 7)$$</p>
<p>这里的$W_T$是以词为单位度量的文本T的长度（可以包括句首标志〈BOS〉或句尾标志〈EOS〉）。式（5-7）计算出的值可以解释为：利用与模型p（wi|w_{i-n+1}^{i-1}）有关的压缩算法对数据集合中的$W_T$个词进行编码，每一个编码所需要的平均比特位数。<br>模型p的困惑度PPT（T）是模型分配给测试集T中每一个词汇的概率的几何平均值的倒数，它和交叉熵的关系为</p>
<p>$$PP_T（T）＝2^{HP（T）}$$</p>
<p>显然，交叉熵和困惑度越小越好，这是我们评估一个语言模型的基本准则。在英语文本中，n元语法模型计算的困惑度范围大约为50～1000之间（对应的交叉熵范围为6～10个比特位），具体值与文本的类型有关［Chen and Goodman,1998］。</p>
<h1 id="数据平滑"><a href="#数据平滑" class="headerlink" title="数据平滑"></a>数据平滑</h1><h2 id="问题的提出"><a href="#问题的提出" class="headerlink" title="问题的提出"></a>问题的提出</h2><p>在5.1节的例子中，如果依据给定的训练语料S计算句子DAVID READ A BOOK的概率，有如下计算公式：</p>
<p>$$p(read | David) = \cfrac{David \quad read}{\sum_w c(David \quad w)}= \cfrac{0}{1}$$</p>
<p>即p（David read a book）＝0。显然，这个结果不够准确，因为句子David read a book总有出现的可能，其概率应该大于0。<br>在语音识别中，实现目标就是找到转写句子s对于给定的声音信号A使概率$p（s|A）＝ \cfrac{p(A|s)p(s)}{p(A)}$最大。如果p（s）＝0，那么，p（s|A）也必然是0，这个结果意味着不管给定的语音信号多么清晰，字符串s也永远不可能成为转写结果。这样，在语音识别中，一旦出现使得p（s）＝0的字符串s，就会导致识别错误。在其他自然语言处理任务中也会出现类似的问题。因而，必须分配给所有可能出现的字符串一个非零的概率值来避免这种错误的发生。<br>平滑（smoothing）技术就是用来解决这类零概率问题的。术语“平滑”指的是为了产生更准确的概率来调整最大似然估计的一种技术，也常称为数据平滑（data smoothing）。“平滑”处理的基本思想是“劫富济贫”，即提高低概率（如零概率），降低高概率，尽量使概率分布趋于均匀。<br>例如，对于二元语法来说，一种最简单的平滑技术就是假设每个二元语法出现的次数比实际出现的次数多一次，不妨将该处理方法称为加1法，于是</p>
<p>$$ p(w_i | w_{i-1}) =\cfrac {1+c(w_{i-1} w_i)} {\sum_{w_i} [1+ c(w_{i-1}w_i)]} =<br>\cfrac{1+c(w_{i-1} w_i)}{|V| + \sum_{w_i}c(w_{i-1}w_i)}   \qquad (5-8)<br>$$</p>
<p>其中，V是所考虑的所有词汇的单词表，|V|为词汇表单词的个数。当然，如果V取无穷大，分母就是无穷大，所有的概率都趋于0。但实际上，词汇表总是有限的，可以大约固定在几万个或者几十万个。所有不在词汇表中的词可以映射为一个单个的区别于其他已知词汇的单词，通常将其称为未登录词或未知词。</p>
<h2 id="加法平滑方法"><a href="#加法平滑方法" class="headerlink" title="加法平滑方法"></a>加法平滑方法</h2><p>在实际应用中最简单的平滑技术之一就是加法平滑方法（additive smoothing），这种方法在上个世纪前半叶由G.J.Lidstone, W.E.Johnson和H.Jeffreys等人提出和改进，其基本思想是使式（5-8）给出的方法通用化，不是假设每一个n元语法发生的次数比实际统计次数多一次，而是假设它比实际出现情况多发生δ次，0≤δ≤1，那么，</p>
<p>$$ p_{add} (w_i | w_{i-n+1}^{i-1})= \cfrac{δ + c(w_{i-n+1}^i)}{δ|V| + \sum_{w_i} c(w_{i-n+1}^i )}<br> \qquad (5-9)<br>$$</p>
<p>G.J.Lidstone和H.Jeffreys曾提倡取δ＝1，但有些学者认为这种方法一般表现较差。</p>
<h2 id="古德-图灵（Good-Turing）估计法"><a href="#古德-图灵（Good-Turing）估计法" class="headerlink" title="古德-图灵（Good-Turing）估计法"></a>古德-图灵（Good-Turing）估计法</h2><p>Good-Turing估计法是很多平滑技术的核心。这种方法是1953年由I.J.Good引用图灵（Turing）的方法提出来的，其基本思路是：对于任何一个出现r次的n元语法，都假设它出现了$r^*$次，这里</p>
<p>$$ r^* = (r+1) \cfrac{n_{r+1}}{n_r}  \qquad (5 - 10)$$</p>
<p>其中，$n_r$是训练语料中恰好出现r次的n元语法的数目。要把这个统计数转化为概率，只需要进行归一化处理：对于统计数为r的n元语法，其概率为<br>$$ p_r = \cfrac{r^\ast}{N} $$<br>其中，$N= \sum_{r=0}^{\infty} n_r r^\ast$。请注意：<br>$N= \sum_{r=0}^{\infty} n_r r^* = \sum_{r=0}^{\infty}(r+1)n_{r+1} = \sum_{r=1}^\infty n_r r$<br>也就是说，N等于这个分布中最初的计数。这样，样本中所有事件的概率之和为<br>$$ \sum_{r&gt;0} n_r p_r = 1- \cfrac{n_1}{N} &lt; 1$$<br>因此，有$n_1/N$的概率剩余量可以分配给所有未见事件（r＝0的事件）。<br>Good-Turing方法不能直接用于估计$n_r＝0$的n-gram概率。W. A. Gale和G.Sampson曾对这种情况的平滑方法进行过专门研究［Gale and Sampson,1995］，这里不再赘述。另外，Good-Turing方法不能实现高阶模型与低阶模型的结合，而高低阶模型的结合通常是获得较好的平滑效果所必须的。通常情况下，Good-Turing方法作为一个基本方法，在后面将要介绍的几种平滑技术中得到了很好的利用。</p>
<h2 id="Katz平滑方法"><a href="#Katz平滑方法" class="headerlink" title="Katz平滑方法"></a>Katz平滑方法</h2><p>Katz平滑方法［Katz,1987］通过加入高阶模型与低阶模型的结合，扩展了Good-Turing估计方法。我们首先来说明一下二元语法模型的Katz平滑方法。对于一个出现次数为r＝c（$w_{i-1}^i$）的二元语法$w_{i-1}^i$，使用如下公式计算修正的计数：<br>$$ c_{Katz}(w_{i-1}^i)=<br>\begin{cases}<br>d_r r, &amp; \qquad r&gt;0  \\<br>\alpha(w_{i-1})p_{ML}(w_i), &amp; \qquad r=0<br>\end{cases}<br>\qquad (5-12)<br>$$<br>也就是说，所有具有非零计数r的二元语法都根据折扣率$d_r$被减值了，折扣率$d_r$近似地等于$\cfrac{r^*}{r}$，这个减值是由Good-Turing估计方法预测的。<br>从非零计数中减去的计数量，根据低一阶的分布，即一元语法模型，被分配给了计数为零的二元语法。那么，需要选择$α(w_{i-1})$值，使分布中总的计数$\sum_{w_i} c_{Katz}(w_{i-1}^i)$保持不变，即$\sum_{w_i}c_{katz}(w_{i-1}^i)$=$\sum_{w_i}c(w_{i-1}^i)$。$α(w_{i-1})$的适当值为<br>$$<br>\alpha(w_{i-1}^i) = \cfrac{1 - \sum_{w_i:c(w_{i-1}^i)&gt;0} p_{katz}(w_i | w_{i-1})}{ \sum_{w_i:c(w_{i-1}^i)=0} p_{ML}(w_i)}=\cfrac{1 - \sum_{w_i:c(w_{i-1}^i)&gt;0} p_{katz}(w_i | w_{i-1})}{1 - \sum_{w_i:c(w_{i-1}^i)&gt;0} p_{ML}(w_i)}<br>$$<br>要根据修正的计数计算概率$p_{katz}(w_i|w_{i-1})$，只需要归一化<br>$$<br>p_{katz}(w_i|w_{i-1}) = \cfrac {c_{katz}(w_{i-1}^i)}{\sum_{w_i}c_{katz}(w_{i-1}^i)}<br>$$<br>折扣率$d_r$可以按照如下办法计算：由于大的计数值是可靠的，因此它们不需要减值。尤其对于某些k，S.M.Katz取所有r&gt;k情况下的dr＝1，并且建议k＝5。对于r≤k情况下的折扣率，减值率由用于全局二元语法分布的Good-Turing估计方法计算，即公式（5-10）中的$n_r$表示在训练语料中恰好出现r次的二元语法的总数。$d_r$的选择遵循如下约束条件：①最终折扣量与Good-Truing估计预测的减值量成比例；②全局二元语法分布中被折扣的计数总量等于根据Good-Turing估计应该分配给次数为零的二元语法的总数。第一个约束条件相当于对于某些常数μ，r∈{1,2，…,k}有公式：<br>$$<br>1- d_r= \mu (1 - \cfrac{r^\ast}{r})<br>$$<br>Good-Truing估计方法预测出应该分配给计数为0的二元语法的计数总量为$n_00^\ast=n_0 \cfrac{n_1}{n_0}=n_1$,因此第二个约束条件相当于公式<br>$$\sum_{r=1}^{k} n_r(1- d_r)r =n_1$$<br>这些公式的唯一解<br>$$<br>d_r= \cfrac{   \cfrac{r^\ast}{r}-\cfrac{(k+1)n_{k+1}}{n_1}   }<br>{1 - \cfrac{(k+1)n_{k+1}}{n_1}}<br>$$<br>用类似的方法可定义高阶n元语法模型的Katz平滑算法。正如我们在式（5-12）中所看到的，二元语法模型是由一元语法模型定义的，那么，一般地，类似Jelinek-Mercer平滑方法，S.M.Katz的n元语法模型由Katz的n-1元语法模型定义。为结束递归，用最大似然估计的一元语法模型作为Katz的一元语法模型。<br>正如前面指出的，当使用Good-Turing估计时一般需要平滑$n_r$，比如，对于那些值非常小的$n_r$。然而，在Katz平滑方法中这种处理并不需要，因为只有当计数r≤k时才使用Good-Turing估计，而对于这些r值来说，$n_r$一般是比较合理的。<br>Katz平滑方法属于后备（back-off）平滑方法。这种方法的中心思想是，当某一事件在样本中出现的频率大于k时，运用最大似然估计经过减值来估计其概率。当某一事件的频率小于k时，使用低阶的语法模型作为代替高阶语法模型的后备，而这种代替必须受归一化因子α的作用。对于这种方法的另一种解释是，根据低阶的语法模型分配由于减值而节省下来的剩余概率给未见事件，这比将剩余概率平均分配给未见事件要合理［翁富良等，1998］。</p>
<h2 id="Jelinek-Mercer平滑方法"><a href="#Jelinek-Mercer平滑方法" class="headerlink" title="Jelinek-Mercer平滑方法"></a>Jelinek-Mercer平滑方法</h2><p>假定要在一批训练语料上构建二元语法模型，其中，有两对词的同现次数为0：</p>
<p>c（SEND THE）＝0</p>
<p>c（SEND THOU）＝0</p>
<p>那么，按照加法平滑方法和Good-Turing估计方法可以得到：</p>
<p>p（THE|SEND）＝p（THOU|SEND）</p>
<p>但是，直觉上我们认为应该有：</p>
<p>p（THE|SEND）＞p（THOU|SEND）</p>
<p>因为冠词THE要比单词THOU出现的频率高得多。为了利用这种情况，一种处理办法是在二元语法模型中加入一个一元模型。我们知道一元模型实际上只反映文本中单词的频率，最大似然一元模型为</p>
<p>$$P_{ML}(w_i) = \cfrac{w_i}{\sum_{w_i} c(w_i)}$$</p>
<p>那么，可以按照下面的方法将二元文法模型和一元文法模型进行线性插值：</p>
<p>$$p_{interp}（w_i|w_{i-1}）＝λp_{ML}（w_i|w_{i-1}）＋（1－λ）p_{ML}（w_i）$$</p>
<p>其中，0≤λ≤1。由于pML（THE|SEND）＝pML（THOU|SEND）＝0，根据假定pML（THE）≫pML（THOU），可以得到：</p>
<p>$$p_{interp}（THE|SEND）&gt;p_{interp}（THOU|SEND）$$</p>
<p>这正是我们希望得到的。</p>
<p>一般来讲，使用低阶的n元模型向高阶n元模型插值是有效的，因为当没有足够的语料估计高阶模型的概率时，低阶模型往往可以提供有用的信息。F.Jelinek和R.L.Mercer曾于1980年提出了通用的插值模型，而Peter F.Brown等人给出了实现这种插值的一种很好的办法［Brown et al.，1992a］：</p>
<p>$$ p_{interp}(w_i| w_{i-n+1}^{i-1}) = \lambda_{w_{i-n+1}^{i-1}}p_{ML}(w_i | w_{i-n+1}^{i-1})<br>+(1-\lambda_{w_{i-n+1}^{i-1}})p_{interp}(w_i | w_{i-n+2}^{i-1})   \qquad (5-13)<br>$$</p>
<p>式（5-13）的含义是：第n阶平滑模型可以递归地定义为n阶最大似然估计模型和n-1阶平滑模型之间的线性插值。为了结束递归，可以用最大似然分布作为平滑的1阶模型，或者用均匀分布作为平滑的0阶模型：</p>
<p>$$p_{unif}(w_i) = \cfrac{1}{|V|}$$</p>
<p>给定一个固定的$p_{ML}$，可以使用Baum-Welch算法有效地搜索出$\lambda_{w_{i-n+1}^{i-1}}$，使某些数据的概率最大。为了得到有意义的结果，估计$\lambda_{w_{i-n+1}^{i-1}}$的语料应该与计算$p_{ML}$的语料不同。在留存插值方法（held-out interpolation）中，保留一部分训练语料来达到这个目的，这部分留存语料不参与计算$p_{ML}$。而F.Jelinek和R.L.Mercer提出了一种叫做删除插值法（deleted interpolation）或删除估计法（deleted estimation）的处理技术，训练语料的不同部分在训练$p_{ML}$或$\lambda_{w_{i-n+1}^{i-1}}$时作变换，从而使结果平均。<br>需要注意的是，对于不同的历史$w_{i-n+1}^{i-1}$，最优的$\lambda_{w_{i-n+1}^{i-1}}$也不同。例如，对于出现过几千次的一段上下文，较高的λ值是比较合适的，因为高阶的分布是非常可靠的。而对于一个只出现过一次的历史，λ的值应较低。独立地训练每一个参数$\lambda_{w_{i-n+1}^{i-1}}$是不合适的，因为需要巨大规模的语料来精确地训练这么多独立的参数。为此，F.Jelinek和R.L.Mercer建议把$\lambda_{w_{i-n+1}^{i-1}}$划分成适当数量的几部分或几段（bucket），并令同一部分中所有的$\lambda_{w_{i-n+1}^{i-1}}$具有相同的值，从而减少需要估计的独立参数的数量。理想情况下，应该根据先验知识把那些应该有相似值的$\lambda_{w_{i-n+1}^{i-1}}$归并在一起。Lait R.Bahl, F.Jelinek和R.L.Mercer建议根据被插值的高阶分布中总的计数$\sum_{w_i}c(w_{i-n+1}^i) $（相应历史的统计数量）来选择这些$\lambda_{w_{i-n+1}^{i-1}}$的集合。根据上面提到的，这个总数应当与高阶分布的权重相关，计数越大，$\lambda_{w_{i-n+1}^{i-1}}$也应该越大。Lait R.Bahl等人甚至建议，把可能的统计总数的范围划分成若干部分，与同一部分相关联的所有$\lambda_{w_{i-n+1}^{i-1}}$放在同一段中。Chen and Goodman （1996）的研究工作表明，根据分布$\cfrac{\sum_{w_i}c(w_{i-n+1}^i)}{|w_i : c(w_{i-n+1}^i) &gt; 0|}$中每个非零元素的平均统计值来分段，比使用$\sum_{w_i} c(w_{i-n+1}^i)$值分段获得的效果要好。</p>
<h2 id="Witten-Bell平滑方法"><a href="#Witten-Bell平滑方法" class="headerlink" title="Witten-Bell平滑方法"></a>Witten-Bell平滑方法</h2><p>Witten-Bell平滑方法是由T.C.Bell, J.G.Cleary和I.H.Witten提出来的一种数据平滑方法［Bell et al.,1990;Witten and Bell,1991］，它可以认为是Jelinek-Mercer平滑算法的一个实例。特别地，n阶平滑模型被递归地定义为n阶最大似然模型和n-1阶平滑模型的线性插值，就像式（5-13）所描述的：</p>
<p>$$p_{WB}(w_i | w_{i-n+1}^{i-1})= \lambda_{w_{i-n+1}^{i-1}}p_{ML}(w_i | w_{i-n+1}^{i-1}<br>+(1 - \lambda_{w_{i-n+1}^{i-1}})p_{WB}(w_i | w_{i-n+2}^{i-1})<br>\qquad (5-14)<br>$$</p>
<p>为计算Witten-Bell平滑算法的参数$\lambda_{w_{i-n+1}^{i-1}}$，需要知道历史$w_{i-n+1}^{i-1}$后接的不同单词的数目，并把这个值记作$N_{1+}(w_{i-n+1}^{i-1} \cdot)$，规范地定义为</p>
<p>$$ N_{1+}(w_{i-n+1}^{i-1} \cdot) = |  { w_i : c(w_{i-n+1}^{i-1} w_i) }|$$</p>
<p>其中，符号$N_{1+}$表示出现过一次或多次的单词的数目，点“·”表示统计过程中的自由变量。可以通过式（5-16）定义Witten-Bell平滑参数的$\lambda_{w_{i-n+1}^{i-1}}$：</p>
<p>$$ 1- \lambda_{w_{i-n+1}^{i-1}} = \cfrac{N_{1+}(w_{i-n+1}^{i-1} \cdot)}{N_{1+}(w_{i-n+1}^{i-1} \cdot)<br>+\sum_{w_i}c(w_{i-n+1}^i)}  \qquad (5-16)$$</p>
<p>代入式（5-14）后得到</p>
<p>$$  p_{WB}(w_i | w_{i-n+1}^{i-1}) =\cfrac{c(w_{i-n+1}^{i-1})+N_{1+}(w_{i-n+1}^{i-1} \cdot)p_{WB}(w_i | w_{i-n+2}^{i-1})}<br>{\sum_{w_i}c(w_{i-n+1}^{i-1}) + N_{1+}(w_{i-n+1}^{i-1} \cdot)}  \qquad (5-17)$$</p>
<p>为了引出Witten-Bell平滑方法，可以将式（5-13）解释为：使用高阶模型的概率为$\lambda_{w_{i-n+1}^{i-1}}$，使用低阶模型的概率为$1-\lambda_{w_{i-n+1}^{i-1}}$。如果在训练语料中对应的n元文法出现次数大于1，则使用高阶模型；否则，后退到低阶模型。这样处理似乎是合理的。于是，把$1-\lambda_{w_{i-n+1}^{i-1}}$理解为没有在训练语料中历史$w_{i-n+1}^{i-1}$之后被观察到单词出现在该历史之后的概率。要估计这些新单词在某历史后出现的频率，设想按顺序来考察训练语料，统计在历史$w_{i-n+1}^{i-1}$之后出现的新单词的次数，即在历史之后没有出现过的单词的数目。显然，这个计数就是历史$w_{i-n+1}^{i-1}$之后出现的不同单词的数目N_{1+}(w_{i-n+1}^{i-1} \cdot)。等式（5-16）可以看作这个过程的近似。<br>Good-Turing估计提供了另外一种观点来估计那些出现在历史之后的新单词的概率。Good-Turing估计法预测一个在训练语料中没有出现的事件的概率为$\cfrac{n_1}{N}$，就是恰好仅出现过一次的事件的那一小部分计数。把这个数值改写成前面的表示方式，我们得到：<br>$$ \cfrac{N_{1}(w_{i-n+1}^{i-1} \cdot)} {\sum_{w_i}c(w_{i-n+1}^{i})}$$<br>其中，<br>$$N_1(w_{i-n+1}^{i-1})＝|{w_i:c(w_i)＝1}|$$<br>等式（5-16）可以看作Good-Turing估计的近似，最少出现一次的单词的数目代替了恰好只出现一次的单词的数目。</p>
<h2 id="绝对值减法"><a href="#绝对值减法" class="headerlink" title="绝对值减法"></a>绝对值减法</h2><p>绝对减值法（absolute discounting）［Ney et al.,1994］类似于Jelinek-Mercer平滑算法，涉及高阶和低阶模型的插值问题。然而，这种方法不是采用将高阶最大似然分布乘以因子的方法，而是通过从每个非零计数中减去一个固定值D≤1的方法来建立高阶分布。也就是说，不采用公式（5-13）</p>
<p>$$ p_{interp}(w_i| w_{i-n+1}^{i-1}) = \lambda_{w_{i-n+1}^{i-1}}p_{ML}(w_i | w_{i-n+1}^{i-1})<br>+(1-\lambda_{w_{i-n+1}^{i-1}})p_{interp}(w_i | w_{i-n+2}^{i-1})   \qquad (5-13)<br>$$</p>
<p>而是采用</p>
<p>$$p_{abs}(w_i | w_{i-n+1}^{i-1})=\cfrac{\max \{c(w_{i-n+1}^i)-D,0\}}{\sum_{w_i}c(w_{i-n+1}^i)}<br>+(1-\lambda_{w_{i-n+1}^{i-1}})p_{abs}(w_i | w_{i-n+2}^{i-1}) \qquad (5-18)<br>$$<br>为使概率分布之和等于1，取<br>$$<br>1- \lambda_{w_{i-n+1}^{i-1}}=\cfrac{D}{\sum_{w_i}c(w_{i-n+1}^i)} N_{1+}(w_{i-n+1}^{i-1} \cdot)<br>$$<br>其中，$N_{1+}(w_{i-n+1}^{i-1} \cdot)$和等式（5-15）中的定义一样，这里假设0≤D≤1。H.Ney等人（1994）提出了通过训练语料上被删除的估计值来设置D值的方法。他们采用以下估计<br>$$<br>D= \cfrac{n_1}{n_1+2n_2}<br>$$<br>其中，n1和n2是训练语料中分别出现一次和两次的n元语法模型的总数，n是被插值的高阶模型的阶数。<br>实际上，可以通过Good-Turing估计推导到绝对减值算法。Church and Gale（1991）根据实验指出，对于具有较大计数（r≥3）的n元语法模型，其Good-Turing减值（$r-r^*$）的均值在很大程度上是关于r的常数。而且，等式（5-19）中的比例因子类似等式（5-16）中为Witten-Bell平滑算法给出的模拟因子，可以看作是对同一个值的近似，即出现在一个历史后面的新词的概率。</p>
<h2 id="Kneser-Ney平滑方法"><a href="#Kneser-Ney平滑方法" class="headerlink" title="Kneser-Ney平滑方法"></a>Kneser-Ney平滑方法</h2><p>R.Kneser和H.Ney于1995年提出了一种扩展的绝对减值算法［Kneser and Ney,1995］，用一种新的方式建立与高阶分布相结合的低阶分布。在前面的算法中，通常用平滑后的低阶最大似然分布作为低阶分布。然而，只有当高阶分布中具有极少的或没有计数时，低阶分布在组合模型中才是一个重要的因素。因此，在这种情况下，应最优化这些参数，以得到较好的性能。</p>
<p>例如，要在一批语料上建立一个二元文法模型，有一个非常普通的单词FRANCISCO，这个单词只出现在单词SAN的后面。由于c（FRANCISCO）较大，因此，一元文法概率p（FRANCISCO）也会较大，像绝对减值算法等这类平滑算法就会相应地为出现在新的二元文法历史后面的单词FRANCISCO分配一个高的概率值。然而，从直观上说，这个概率值不应该很高，因为在训练语料中单词FRANCISCO只跟在唯一的历史后面。也就是说，单词FRANCISCO应该接受一个较小的一元文法概率，因为只有上一个词是SAN时这个单词才会出现。在这种情况下，二元文法概率模型可能表现更好。</p>
<p>以此类推，使用的一元文法的概率不应该与单词出现的次数成比例，而是与它前面的不同单词的数目成比例。我们可以设想按顺序遍历训练语料，在前面语料的基础上建立二元文法模型来预测现在的单词。那么，只要当前的二元文法没有在前面的语料中出现，一元文法的概率将会是影响当前二元文法概率的较大因素。如果一旦这种事件发生，就要给相应的一元文法分配一个计数，那么，分配给每个一元文法计数的数目就是它前面不同单词的数目。实际上，在Kneser-Ney平滑方法中，二元文法模型中的一元文法概率就是按这种方式计算的。然而，在文献［Kneser and Ney,1995］中这种计算方法却是以完全不同的方式提出来的，其推导过程是，选择的低阶分布必须使得到的高阶平滑分布的边缘概率与训练语料的边缘概率相匹配。例如，对于二元文法模型，选择一个平滑的分布pKN，使其对所有的wi，满足一元文法边缘概率的约束条件：<br>$$<br>\sum_{w_{i-1}} p_{KN}(w_{i-1}w_i)=\cfrac{c(w_i)}{\sum_{w_i}c(w_i)} \qquad (5-21)<br>$$</p>
<p>等式（5-21）左边是平滑的二元文法分布$p_{KN}$中wi的一元文法边缘概率，等式右边是训练语料中$w_i$的一元文法频率。<br>Chen and Goodman（1998）提出了一种不同的推导方法。他们假设模型具有式（5-18）的形式：</p>
<p>$$p_{abs}(w_i | w_{i-n+1}^{i-1})=\cfrac{\max \{c(w_{i-n+1}^i)-D,0\}}{\sum_{w_i}c(w_{i-n+1}^i)} \\<br>+\cfrac{D}{\sum_{w_i}c(w_{i-n+1}^i)} N_{1+}(w_{i-n+1}^{i-1} \cdot) p_{KN}(w_i | w_{i-n+2}^{i-1}) \qquad (5-22)<br>$$<br>这与R.Kneser和H.Ney在论文［Kneser and Ney,1995］中使用的形式不同，原文中使用的形式是：<br>$$<br>p_{KN}(w_i | w_{i-n+1}^{i-1})=<br>\begin{cases}<br>\cfrac{\max \{c(w_{i-n+1}^i)-D,0\}}{\sum_{w_i}c(w_{i-n+1}^i)}， &amp; c(w_{i-n+1}^i)&gt;0 \\<br>\gamma(w_{i-n+1}^{i-1})p_{KN}(w_i | w_{i-n+2}^{i-1})，&amp; c(w_{i-n+1}^i)=0<br>\end{cases}<br>$$<br>这里，选择\gamma(w_{i-n+1}^{i-1})使分布之和等于1。也就是说，S.F.Chen等人对所有单词的低阶分布进行插值，而不是只对那些高阶分布中计数为零的单词插值。这样做的原因是因为它不但可以得到比原公式更清晰的推导过程，而且不需要近似。<br>现在的目的是找到一元文法分布$p_{KN}(w_i)$，使其满足等式（5-21）给出的约束条件。展开等式（5-21），可以得到<br>$$<br>\cfrac{c(w_i)}{\sum_{w_i}c(w_i)} = \sum_{w_{i-1}}p_{KN}(w_i | w_{i-1})p(w_{i-1})<br>$$<br>对于$p(w_{i-1})$，可以简单地取训练语料中的概率分布：<br>$$<br>p(w_{i-1})=\cfrac{c(w_{i-1})}{\sum_{w_{i-1}}c(w_{i-1})}<br>$$<br>于是，得到<br>$$<br>c(w_i)=\sum_{w_{i-1}}c(w_{i-1})p_{KN}(w_i | w_{i-1})<br>$$<br>将其代入式（5-22），得到<br><img src="/images/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/Kneser-Ney%E5%B9%B3%E6%BB%91%E6%96%B9%E6%B3%95%E6%8E%A8%E5%AF%BC.png" alt="Kneser-Ney平滑方法推导"></p>
<h2 id="算法总结"><a href="#算法总结" class="headerlink" title="算法总结"></a>算法总结</h2><p>正如R.Kneser和H.Ney（1995）指出的，大多数平滑算法可以用下面的等式表示：</p>
<p>$$p_{smooth}(w_i | w_{i-n+1}^{i-1}) =<br>\begin{cases}<br>\alpha(w_i | w_{i-n+1}^{i-1}),  &amp;  c(w_{i-n+1}^{i}) &gt;0) \\<br>\gamma(w_{i-n+1}^{i-1})p_{smooth}(w_i | w_{i-n+2}^{i-1}), &amp; c(w_{i-n+1}^{i}) =0)<br>\end{cases}  \qquad (5-12)<br>$$</p>
<p>也就是说，如果n阶语言模型具有非零的计数，就使用分布$α(w_i| w_{i-n+1}^{i-1})$；否则，就后退到低阶分布$p_{smooth}(w_i|w_{i-n+2}^{i-1})$，选择比例因子$γ(w_{i-n+1}^{i-1})$使条件概率分布之和等于1。通常称符合这种框架的平滑算法为后备模型（back-off model）。前面介绍的Katz平滑算法是后备平滑算法的一个典型例子。<br>有些平滑算法采用高阶和低阶n元文法模型的线性插值，表达成等式（5-13）的形式：</p>
<p>$$p_{smooth}(w_i| w_{i-n+1}^{i-1})＝\lambda_{w_{i-n+1}^{i-1}}p_{ML}(w_i| w_{i-n+1}^{i-1})＋(1- \lambda_{w_{i-n+1}^{i-1}})p_{smooth}(wi|w_{i-n+2}^{i-1})$$</p>
<p>这个等式可以重写成如下形式：</p>
<p>$$p_{smooth}(w_i| w_{i-n+1}^{i-1})＝α′(w_i| w_{i-n+1}^{i-1})＋\gamma({w_{i-n+1}^{i-1}})p_{smooth}(wi|w_{i-n+2}^{i-1})$$</p>
<p>其中，</p>
<p>$$α′(w_i| w_{i-n+1}^{i-1})=\lambda_{w_{i-n+1}^{i-1}}p_{ML}(w_i| w_{i-n+1}^{i-1})$$</p>
<p>且$γ(w_{i-n+1}^{i-1})＝1-\lambda_{w_{i-n+1}^{i-1}}$。那么，通过取</p>
<p>$$\alpha(w_i| w_{i-n+1}^{i-1})＝α′(w_i| w_{i-n+1}^{i-1})＋\gamma({w_{i-n+1}^{i-1}})p_{smooth}(wi|w_{i-n+2}^{i-1})  \qquad (5-25)$$<br>可以看出，这些模型都可以写成公式（5-24）的形式。这种形式的模型称为插值模型（interpolated model）。<br>后备模型和插值模型的根本区别在于，在确定非零计数的n元文法的概率时，插值模型使用低阶分布的信息，而后备模型却不是这样。但不管是后备模型还是插值模型，都使用了低阶分布来确定计数为零的n元语法的概率。<br>Chen and Goodman（1998）使用等式（5-24）的符号概括了该式所代表的所有后备平滑算法，归纳成表5-2。<br><img src="/images/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E5%B9%B3%E6%BB%91%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93.png" alt="平滑算法总结"><br>对于插值模型，表中用省略号“…”表示等式（5-25）中后面一项$γ(w_{i-n+1}^{i-1})p_{smooth}（w_i| w_{i-n+2}^{i-1})$的略写形式。$p_{ML}()$是$p_{ML}(w_i| w_{i-n+1}^{i-1})$的缩写。<br>容易看出，建立插值算法的后备算法是比较容易的，只需要用</p>
<p>$$α（w_i| w_{i-n+1}^{i-1}）＝α′（w_i|w_{i-n+1}^{i-1}）$$</p>
<p>来替代等式（5-25），然后，适当调整γ（w_{i-n+1}^{i-1}）使概率之和等于1即可。</p>
<h1 id="其他平滑方法"><a href="#其他平滑方法" class="headerlink" title="其他平滑方法"></a>其他平滑方法</h1><p>本节首先介绍两种不为广泛使用的、但在理论上却很有趣的平滑方法：Church-Gale平滑方法和贝叶斯平滑方法，然后介绍S.F.Chen和J.Goodman提出的修正的Kneser-Ney平滑方法。<br>（略）</p>
<h1 id="平滑方法比较"><a href="#平滑方法比较" class="headerlink" title="平滑方法比较"></a>平滑方法比较</h1><p>前面介绍了语言模型中常用的一些平滑方法，包括加法平滑、Jelinek-Mercer平滑、Katz平滑、Witten-Bell平滑、绝对减值平滑和Kneser-Ney平滑，以及Church-Gale平滑和修正的Kneser-Ney平滑方法等。那么，现在的问题是这些平滑方法在实现效果上有什么差异？其平滑效果与数据量和参数设置有怎样的关系？对此，S.F.Chen和J.Goodman做了大量的对比实验，利用布朗语料（Brown Corpus）、北美商务新闻语料（The North American Business News Corpus）、Switchboard语料和广播新闻语料 〔1〕，以测试语料的交叉熵和语音识别结果的词错误率（word error rate）为评价指标，对加法平滑方法、Jelinek-Mercer平滑方法、Katz平滑方法、Witten-Bell平滑方法、绝对减值平滑方法和Kneser-Ney平滑方法以及修正的Kneser-Ney平滑方法做了全面系统的对比测试，得到了若干重要的结论，对实用语言模型的开发具有重要的参考价值。<br>在S.F.Chen和J.Goodman的对比实验中，采用留存插值方法（held-out interpolation）的Jelinek-Mercer平滑方法作为对比的基线算法（baseline）。根据他们的对比测试，不管训练语料规模多大，对于二元语法和三元语法而言，Kneser-Ney平滑方法和修正的Kneser-Ney平滑方法的效果都好于其他所有的平滑方法。一般情况下，Katz平滑方法和Jelinek-Mercer平滑方法也有较好的表现，但与Kneser-Ney平滑方法和修正的Kneser-Ney平滑方法相比稍有逊色。在稀疏数据的情况下，Jelinek-Mercer平滑方法优于Katz平滑方法；而在有大量数据的情况下，Katz平滑方法则优于Jelinek-Mercer平滑方法。插值的绝对减值平滑方法和后备的Witten-Bell平滑方法的表现最差。除了对于很小的数据集以外，插值的绝对减值平滑方法一般优于基线算法，而Witten-Bell平滑方法则表现较差，对于较小的数据集，该方法比基线算法差得多。对于大规模数据集而言，这两种方法都比基线算法优越得多，甚至可以与Katz平滑方法和Jelinek-Mercer平滑方法相匹敌。<br>S.F.Chen和J.Goodman的实验还表明，平滑方法的相对性能与训练语料的规模、n元语法模型的阶数和训练语料本身有较大的关系，其效果可能会随着这些因素的不同而出现很大的变化。例如，对于较小规模的训练语料来说，后备的Witten-Bell平滑方法表现很差，而对于大规模数据集来说，其平滑效果却极具竞争力。<br>根据S.F.Chen和J.Goodman的实验和分析，下列因素对于平滑算法的性能有一定的影响［Chen and Goodman,1998］：<br>●　影响最大的因素是采用修正的后备分布，例如Kneser-Ney平滑方法所采用的后备分布。这可能是Kneser-Ney平滑方法及其各种版本的平滑算法优于其他平滑方法的基本原因。<br>●　绝对减值优于线性减值。正如前面指出的，对于较低的计数来说，理想的平均减值上升很快，而对于较大的计数，则变得比较平缓。Good-Turing估计可以用于预测这些平均减值，甚至比绝对减值还好。<br>●　从性能上来看，对于较低的非零计数，插值模型大大地优于后备模型，这是因为低阶模型在为较低计数的n元语法确定恰当的减值时提供了有价值的信息。<br>●　增加算法的自由参数，并在留存数据上优化这些参数，可以改进算法的性能。<br>修正的Kneser-Ney平滑方法之所以获得了最好的平滑效果，就是得益于上述各方面因素的综合。</p>
<h1 id="语言模型的自适应方法"><a href="#语言模型的自适应方法" class="headerlink" title="语言模型的自适应方法"></a>语言模型的自适应方法</h1><p>在自然语言处理系统中，语言模型的性能好坏直接影响整个系统的性能。尽管语言模型的理论基础已比较完善，但在实际应用中常常会遇到一些难以处理的问题。其中，模型对跨领域的脆弱性（brittleness across domains）和独立性假设的无效性（false independence assumption）是两个最明显的问题。也就是说，一方面在训练语言模型时所采用的语料往往来自多种不同的领域，这些综合性语料难以反映不同领域之间在语言使用规律上的差异，而语言模型恰恰对于训练文本的类型、主题和风格等都十分敏感；另一方面，n元语言模型的独立性假设前提是一个文本中的当前词出现的概率只与它前面相邻的n-1个词相关，但这种假设在很多情况下是明显不成立的。另外，香农实验（Shannon-style experiments）表明，相对而言，人更容易运用特定领域的语言知识、常识和领域知识进行推理以提高语言模型的性能（预测文本的下一个成分）［Rosenfeld,2000］。因此，为了提高语言模型对语料的领域、主题、类型等因素的适应性，［Kupiec,1989］和［Kuhn and De Mori, 1990］等提出了自适应语言模型（adaptive language model）的概念。在随后的这些年里，人们相继提出了一系列的语言模型自适应方法，并进行了大量实践。</p>
<p>本节主要介绍三种语言模型自适应方法：基于缓存的语言模型（cache-based LM）［Kuhn and De Mori, 1990, 1992］、基于混合方法的语言模型（mixture-based LM）［Kneser and Steinbiss, 1993; Clarkson, 1999］和基于最大熵 〔2〕的语言模型［Rosenfeld, 1996; Berger et al.,1996］。</p>
<h2 id="基于缓存的语言模型"><a href="#基于缓存的语言模型" class="headerlink" title="基于缓存的语言模型"></a>基于缓存的语言模型</h2><p>基于缓存的语言模型自适应方法针对的问题是，在文本中刚刚出现过的一些词在后边的句子中再次出现的可能性往往较大，比标准的n元语法模型预测的概率要大。针对这种现象，cache-based自适应方法的基本思路是，语言模型通过n元语法的线性插值求得：<br>$$<br>\hat{p}(w_i | w_1^{i-1})=\lambda \hat{p}<em>{Cache}(w_i | w</em>{i-n+1}^i) + (1-\lambda)\hat{p}<em>{n-gram}(w_i | w</em>{i-n+1}^{i-1})<br>$$<br>插值系数λ可以通过EM算法求得。<br>常用的方法是，在缓存中保留前面的K个单词，每个词$w_i$的概率（缓存概率）用其在缓存中出现的相对频率计算得出：<br>$$\hat{p}<em>{Cache}(w_i | w_1^{i-1}) = \cfrac{1}{K} \sum</em>{j=i-k}^{i-1}I_{ \{ w_j=w_i \} } \qquad (5-27)$$<br>其中，$I_ε$为指示器函数（indicator function）。如果ε表示的情况出现，则$I_ε$＝1；否则，$I_ε$＝0。<br>然而，这种方法有明显的缺陷。例如，缓存中一个词的重要性独立于该词与当前词的距离，这似乎是不合理的。人们希望越是临近的词，对缓存概率的贡献越大。P.R.Clarkson（1999）的研究表明，缓存中每个词对当前词的影响随着与该词距离的增大呈指数级衰减，因此，式（5-27）可以写成：<br>$$\hat{p}<em>{Cache}(w_i | w_1^{i-1}) = \beta \sum</em>{j=i-1}^{i-1}I_{ \{ w_j=w_i \} } e^{-\alpha(i-j)}<br>\qquad (5-28)$$<br>其中，α为衰减率；β为归一化常数，以使得$\sum_{w_i \in V} \hat{p}_{Cache}(w_i | w_1^{i-1})＝1$，V为词汇表。<br>文献［Clarkson,1999］将式（5-27）称为“正常的基于缓存的语言模型（regular cache-based LM）”，而将式（5-28）称为“衰减的基于缓存的语言模型（decaying cache-based LM）”。Kuhn and De Mori（1990，1992）的实验表明，cache-based自适应方法减低了语言模型的困惑度，而P.R.Clarkson（1999）的实验表明，式（5-28）比式（5-27）对降低语言模型的困惑度效果更好。<br>黄非等（1999）提出了利用特定领域中少量自适应语料，在原词表中通过分离通用领域词汇和特定领域词汇，并自动检测词典外领域关键词实现词典自适应，然后结合基于缓存的方法实现语言模型的自适应方法。曲卫民等（2003）通过采用TF-IDF公式代替原有的简单频率统计法，建立基于记忆的扩展二元模型，并采用权重过滤法以节省模型计算量，实现了对基于缓存记忆的语言模型自适应方法的改进。张俊林等（2005）也对基于记忆的语言模型进行了扩展，利用汉语义类词典，将与缓存中所保留词汇语义上相近或者相关的词汇也引入缓存，在一定程度上提高了原有模型的性能。</p>
<h2 id="基于混合方法的语言模型"><a href="#基于混合方法的语言模型" class="headerlink" title="基于混合方法的语言模型"></a>基于混合方法的语言模型</h2><p>基于混合方法的自适应语言模型针对的问题是，由于大规模训练语料本身是异源的（heterogenous），来自不同领域的语料无论在主题（topic）方面，还是在风格（style）方面，或者同时在这两方面都有一定的差异，而测试语料一般是同源的（homogeneous），因此，为了获得最佳性能，语言模型必须适应各种不同类型的语料对其性能的影响。<br>基于混合方法的自适应语言模型的基本思想是，将语言模型划分成n个子模型$M_1，M_2，…，M_n$，整个语言模型的概率通过下面的线性插值公式计算得到：</p>
<p>$$\hat{p}<em>{Cache}(w_i | w_1^{i-1}) = \sum</em>{j=1}^{n}\lambda_j\hat{p}_{M_j}(w_i | w_1^{i-1}) $$</p>
<p>其中，$0≤λ_j≤1$，。λ值可以通过EM算法计算出来。<br>基于这种思想，该适应方法针对测试语料的实现过程包括下列步骤［Rosenfeld,2000］：<br>（1）对训练语料按来源、主题或类型等（不妨按主题）聚类；<br>（2）在模型运行时识别测试语料的主题或主题的集合；<br>（3）确定适当的训练语料子集，并利用这些语料建立特定的语言模型；<br>（4）利用针对各个语料子集的特定语言模型和线性插值公式（5-29），获得整个语言模型。<br>根据文献［Kneser and Steinbiss,1993］，基于二元模型和110万词的语料进行训练，基于混合方法的自适应方法使语言模型的困惑度降低了10％。</p>
<h2 id="基于最大熵的语言模型"><a href="#基于最大熵的语言模型" class="headerlink" title="基于最大熵的语言模型"></a>基于最大熵的语言模型</h2><p>上面介绍的两种语言模型自适应方法采用的思路都是分别建立各个子模型，然后，将子模型的输出组合起来。基于最大熵的语言模型却采用不同的实现思路，即通过结合不同信息源的信息构建一个语言模型。每个信息源提供一组关于模型参数的约束条件，在所有满足约束的模型中，选择熵最大的模型。<br>作为一个例子，考虑两个语言模型M1和M2，假设M1是标准的二元模型，表示为f函数：</p>
<p>$$ \hat{p}<em>{M_1}(w_i | w_1^{i-1}) = f(w_i | w</em>{i-1}) \qquad (5-30)$$</p>
<p>$M_2$是距离为2的二元模型（distance-2 bigram），定义为g函数：</p>
<p>$$ \hat{p}<em>{M_2}(w_i | w_1^{i-1}) = g(w_i | w</em>{i-2}) \qquad (5-31)$$</p>
<p>几乎可以肯定地说，这两个概率是不一样的。可以用线性插值方法取这两个概率的平均值，用后备方法（backing-off）选择其中一个进行数据平滑。最大熵原则将所有的信息源组合成一个模型，对于该模型的约束并不是让式（5-30）和式（5-31）对于所有可能的历史都成立，而是放宽限制，使它们在训练数据上平均成立即可。因此，式（5-30）和式（5-31）被分别改写成：<br>$$<br>E(\hat{p}<em>{M_1}(w_i | w_1^{i-1}) | w</em>{i-1} =a ) =f (w_i ,a) \qquad (5-32)<br>$$<br>$$<br>E(\hat{p}<em>{M_2}(w_i | w_1^{i-1}) | w</em>{i-2} =b ) =g (w_i ,b) \qquad (5-32)<br>$$<br>如果约束条件是一致的（相互之间不矛盾），那么，总有模型满足这些条件。下一步要做的就是利用通用迭代算法（generalized iterative scaling, GIS）［Darroch and Ratcliff,1972］选择使熵最大的模型。<br>为了考虑远距离的文本历史信息，以弥补一般语言模型仅仅利用近距离历史的不足，R.Rosenfeld（1996）提出了利用触发器对（trigger pair）作为信息承载成分的思想。根据文献［Rosenfeld,1996］的定义，如果一个词序列A与另一个词序列B密切相关，那么，A→B被看作一个触发器对，其中，A为触发器，B为被触发的序列。当A出现在一个文本中时，它触发B，从而引起B的概率估计发生变化。如果把B看作当前词，A为历史h中的某个特征，那么，可以将一个二值的触发器对A→B形式化为一种约束函数$f_{A→B}$：</p>
<p>$$f_{A \to B}(h,w)=<br>\begin{cases}<br>1, &amp; 如果A \in h ,w=B \\<br>0, &amp; 否则<br>\end{cases}<br>$$<br>余下的问题就是选择触发器对和估计概率p（h, w）或p（w|h）。关于触发器对的选择和抽取方法以及概率估算方法，很多学者都针对不同的应用目的做过大量的研究工作［Rosenfeld, 1996; Troncoso and Kawahara,2005; Lau et al.,1993］，这里不再一一详述。<br>由于最大熵模型能够较好地将来自不同信息源的模型结合起来，获得性能较好的语言模型，因此，有些学者研究将基于主题的语言模型（topic-based LM）（主题条件约束）与n元语法模型相结合，用于对话语音识别、信息检索（information retrieval, IR）和隐含语义分析（latent semantic analysis, LSA）等［Khudanpur and Wu,1999; Gildea and Hofmann,1999; Gotoh and Renals,2000］。<br>综上所述，语言模型的自适应方法是改进和提高语言模型性能的重要手段之一。由于语言模型广泛地应用于自然语言处理的各个方面，而其性能表现与语料本身的状况（领域、主题、风格等）以及选用的统计基元等密切相关，因此，其自适应方法也要针对具体问题和应用目的（机器翻译、信息检索、语义消歧等）综合考虑。<br>美国SRI International和卡内基-梅隆大学（Carnegie Mellon University, CMU）分别实现了语言模型的工具软件，并在互联网上开源发布，为语言模型的推广使用和进一步研究，起到了非常重要的推动作用。两套语言模型工具的网址分别为：</p>
<p><a href="http://www.speech.sri.com/projects/srilm/" target="_blank" rel="noopener">http://www.speech.sri.com/projects/srilm/</a></p>
<p><a href="http://mi.eng.cam.ac.uk/~prc14/toolkit.html" target="_blank" rel="noopener">http://mi.eng.cam.ac.uk/~prc14/toolkit.html</a></p>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p>《统计自然语言处理》（第二版）  宗成庆 著 清华大学出版社 <a href="/books/mobi/统计自然语言处理（第2版）.mobi">统计自然语言处理.mobi下载</a></p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/nlp/" rel="tag"><i class="fa fa-tag"></i> nlp</a>
          
            <a href="/tags/统计自然语言处理/" rel="tag"><i class="fa fa-tag"></i> 统计自然语言处理</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/04/11/统计自然语言处理-语料库与语言知识库/" rel="next" title="统计自然语言处理-语料库与语言知识库">
                <i class="fa fa-chevron-left"></i> 统计自然语言处理-语料库与语言知识库
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/04/15/统计自然语言处理-概率图模型/" rel="prev" title="统计自然语言处理-概率图模型">
                统计自然语言处理-概率图模型 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
    </div>
  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.png" alt="Arvin_Zhang">
            
              <p class="site-author-name" itemprop="name">Arvin_Zhang</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">24</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">6</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">20</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/zhangweifeng919" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:zhangweifeng919@gmail.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#n元语法"><span class="nav-number">1.</span> <span class="nav-text">n元语法</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#语言模型性能评价"><span class="nav-number">2.</span> <span class="nav-text">语言模型性能评价</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#数据平滑"><span class="nav-number">3.</span> <span class="nav-text">数据平滑</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#问题的提出"><span class="nav-number">3.1.</span> <span class="nav-text">问题的提出</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#加法平滑方法"><span class="nav-number">3.2.</span> <span class="nav-text">加法平滑方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#古德-图灵（Good-Turing）估计法"><span class="nav-number">3.3.</span> <span class="nav-text">古德-图灵（Good-Turing）估计法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Katz平滑方法"><span class="nav-number">3.4.</span> <span class="nav-text">Katz平滑方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Jelinek-Mercer平滑方法"><span class="nav-number">3.5.</span> <span class="nav-text">Jelinek-Mercer平滑方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Witten-Bell平滑方法"><span class="nav-number">3.6.</span> <span class="nav-text">Witten-Bell平滑方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#绝对值减法"><span class="nav-number">3.7.</span> <span class="nav-text">绝对值减法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Kneser-Ney平滑方法"><span class="nav-number">3.8.</span> <span class="nav-text">Kneser-Ney平滑方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#算法总结"><span class="nav-number">3.9.</span> <span class="nav-text">算法总结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#其他平滑方法"><span class="nav-number">4.</span> <span class="nav-text">其他平滑方法</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#平滑方法比较"><span class="nav-number">5.</span> <span class="nav-text">平滑方法比较</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#语言模型的自适应方法"><span class="nav-number">6.</span> <span class="nav-text">语言模型的自适应方法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#基于缓存的语言模型"><span class="nav-number">6.1.</span> <span class="nav-text">基于缓存的语言模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#基于混合方法的语言模型"><span class="nav-number">6.2.</span> <span class="nav-text">基于混合方法的语言模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#基于最大熵的语言模型"><span class="nav-number">6.3.</span> <span class="nav-text">基于最大熵的语言模型</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#参考资料"><span class="nav-number">7.</span> <span class="nav-text">参考资料</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2019 &mdash; <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Arvin_Zhang</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Site words total count&#58;</span>
    
    <span title="Site words total count"></span>
  
</div>











<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
    console.log("已经推送");
})();
</script>

        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  










  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item=>{
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: false,
        appId: 'kjMy4Wir9L7O2wDtjHexeyl8-gzGzoHsz',
        appKey: 'zruMkYmFF2lxyMAreQ8AdUWr',
        placeholder: '欢迎提问，一起讨论\(^o^)/~',
        avatar:'mm',
        guest_info:guest,
        pageSize:'10' || 10,
    });
  </script>



  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("kjMy4Wir9L7O2wDtjHexeyl8-gzGzoHsz", "zruMkYmFF2lxyMAreQ8AdUWr");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

</body>
</html>
