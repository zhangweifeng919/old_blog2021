<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">






  
  
  <link rel="stylesheet" media="all" href="/lib/Han/dist/han.min.css?v=3.3">




<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="nlp,统计自然语言处理,">










<meta name="description" content="概率图模型（probabilistic graphical models）在概率模型的基础上，使用了基于图的方法来表示概率分布（或者概率密度、密度函数），是一种通用化的不确定性知识表示和处理方法。">
<meta name="keywords" content="nlp,统计自然语言处理">
<meta property="og:type" content="article">
<meta property="og:title" content="统计自然语言处理-概率图模型">
<meta property="og:url" content="https://zhangweifeng.top/2019/04/15/统计自然语言处理-概率图模型/index.html">
<meta property="og:site_name" content="漂泊在学海">
<meta property="og:description" content="概率图模型（probabilistic graphical models）在概率模型的基础上，使用了基于图的方法来表示概率分布（或者概率密度、密度函数），是一种通用化的不确定性知识表示和处理方法。">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://zhangweifeng.top/images/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E5%9B%BE%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%A0%91%E5%BD%A2%E7%BB%93%E6%9E%84.png">
<meta property="og:image" content="https://zhangweifeng.top/images/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%B8%AD%E5%9B%BE%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%BC%94%E5%8F%98.png">
<meta property="og:image" content="https://zhangweifeng.top/images/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C.png">
<meta property="og:image" content="https://zhangweifeng.top/images/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E5%8D%97%E6%B5%B7%E6%96%B0%E9%97%BB%E7%9A%84%E5%8F%AF%E8%83%BD%E6%80%A7%E5%A4%9A%E5%A4%A7.png">
<meta property="og:image" content="https://zhangweifeng.top/images/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BE%8B%E5%AD%90.png">
<meta property="og:image" content="https://zhangweifeng.top/images/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B%E5%9B%BE%E8%A7%A3.png">
<meta property="og:image" content="https://zhangweifeng.top/images/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E6%A0%BC%E6%9E%B6%E7%AE%97%E6%B3%95%E7%A4%BA%E6%84%8F%E5%9B%BE.png">
<meta property="og:image" content="https://zhangweifeng.top/images/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E5%89%8D%E5%90%91%E5%8F%98%E9%87%8F%E7%9A%84%E5%BD%92%E7%BA%B3%E5%85%B3%E7%B3%BB.png">
<meta property="og:image" content="https://zhangweifeng.top/images/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E5%90%8E%E5%90%91%E5%8F%98%E9%87%8F%E7%9A%84%E5%BD%92%E7%BA%B3%E5%85%B3%E7%B3%BB.png">
<meta property="og:image" content="https://zhangweifeng.top/images/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1HMM.png">
<meta property="og:image" content="https://zhangweifeng.top/images/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/xi%E4%B8%8E%E5%89%8D%E5%90%8E%E5%90%91%E5%8F%98%E9%87%8F%E4%B9%8B%E9%97%B4%E7%9A%84%E5%85%B3%E7%B3%BB.png">
<meta property="og:image" content="https://zhangweifeng.top/images/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/HMM%E7%9A%84%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.png">
<meta property="og:image" content="https://zhangweifeng.top/images/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/HHMM%E7%9A%84%E6%A0%91%E7%8A%B6%E7%BB%93%E6%9E%84.png">
<meta property="og:image" content="https://zhangweifeng.top/images/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/HHMM%E7%9A%84%E5%BD%A2%E5%BC%8F%E5%8C%96%E6%8F%8F%E8%BF%B0.png">
<meta property="og:image" content="https://zhangweifeng.top/images/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E5%9B%BE.png">
<meta property="og:image" content="https://zhangweifeng.top/images/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/HMM%E4%B8%8EMEMM%E4%BE%9D%E5%AD%98%E5%AF%B9%E7%85%A7%E5%9B%BE.png">
<meta property="og:image" content="https://zhangweifeng.top/images/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/CRF%E7%9A%84%E9%93%BE%E5%BC%8F%E7%BB%93%E6%9E%84%E5%9B%BE.png">
<meta property="og:updated_time" content="2019-08-20T15:51:00.158Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="统计自然语言处理-概率图模型">
<meta name="twitter:description" content="概率图模型（probabilistic graphical models）在概率模型的基础上，使用了基于图的方法来表示概率分布（或者概率密度、密度函数），是一种通用化的不确定性知识表示和处理方法。">
<meta name="twitter:image" content="https://zhangweifeng.top/images/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E5%9B%BE%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%A0%91%E5%BD%A2%E7%BB%93%E6%9E%84.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://zhangweifeng.top/2019/04/15/统计自然语言处理-概率图模型/">





  <title>统计自然语言处理-概率图模型 | 漂泊在学海</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">漂泊在学海</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <h1 class="site-subtitle" itemprop="description">学海无涯，回头是岸</h1>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://zhangweifeng.top/2019/04/15/统计自然语言处理-概率图模型/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Arvin_Zhang">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="漂泊在学海">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">统计自然语言处理-概率图模型</h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-15T17:58:35+08:00">
                2019-04-15
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2019-08-20T23:51:00+08:00">
                2019-08-20
              </time>
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/读书笔记/" itemprop="url" rel="index">
                    <span itemprop="name">读书笔记</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/04/15/统计自然语言处理-概率图模型/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/04/15/统计自然语言处理-概率图模型/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2019/04/15/统计自然语言处理-概率图模型/" class="leancloud_visitors" data-flag-title="统计自然语言处理-概率图模型">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

           

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  
                </span>
              

              

              
            </div>
          

          
              <div class="post-description">
                  概率图模型（probabilistic graphical models）在概率模型的基础上，使用了基于图的方法来表示概率分布（或者概率密度、密度函数），是一种通用化的不确定性知识表示和处理方法。
              </div>
          

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        <h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>概率图模型（probabilistic graphical models）在概率模型的基础上，使用了基于图的方法来表示概率分布（或者概率密度、密度函数），是一种通用化的不确定性知识表示和处理方法。在概率图模型的表达中，结点表示变量，结点之间直接相连的边表示相应变量之间的概率关系。当概率分布P被表示成概率图模型之后，可以用来回答与概率分布P有关的问题，如计算条件概率P（Y|E＝e）：在证据e给定的条件下，Y出现的边缘概率；推断使$P（X_1，X_2，L，X_n|e）$最大的$（X_1，X_2，L，X_n）$的分布，即推断最大后验概率时的分布$\arg\max_XP（X|e）$。例如，假设S为一个汉语句子，X是句子S切分出来的词序列，那么，汉语句子的分词过程可以看成是推断使P（X|S）最大的词序列X的分布。而在词性标注中，可以看作在给定序列X的情况下，寻找一组最可能的词性标签分布T，使得后验概率P（T|X）最大。<br>根据图模型（graphical models）的边是否有向，概率图模型通常被划分成有向概率图模型和无向概率图模型。我们可以粗略地将图模型表示成图6-1所示的树形结构。<br><img src="/images/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E5%9B%BE%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%A0%91%E5%BD%A2%E7%BB%93%E6%9E%84.png" alt="图模型的树形结构"><br>动态贝叶斯网络（dynamic Bayesian networks, DBN）用于处理随时间变化的动态系统中的推断和预测问题。其中，隐马尔可夫模型（hidden Markov model, HMM）在语音识别、汉语自动分词与词性标注和统计机器翻译等若干语音语言处理任务中得到了广泛应用；卡尔曼滤波器则在信号处理领域有广泛的用途。马尔可夫网络（Markov network）又称马尔可夫随机场（Markov random field, MRF）。马尔可夫网络下的条件随机场（conditional random field, CRF）广泛应用于自然语言处理中的序列标注、特征选择、机器翻译等任务，波尔兹曼机（Boltzmann machine）近年来被用于依存句法分析［Garg and Henderson, 2011］和语义角色标注［庄涛，2012］等。<br>图6-2从纵横两个维度更加清晰地诠释了自然语言处理中概率图模型的演变过程［Sutton and McCallum, 2007］。横向：由点到线（序列结构）、到面（图结构）。以朴素贝叶斯模型为基础的隐马尔可夫模型用于处理线性序列问题，有向图模型用于解决一般图问题；以逻辑回归模型（即自然语言处理中ME模型）为基础的线性链式条件随机场用于解决“线式”序列问题，通用条件随机场用于解决一般图问题。纵向：在一定条件下生成式模型（generative model）转变为判别式模型（discriminative model），朴素贝叶斯模型演变为逻辑回归模型，隐马尔可夫模型演变为线性链式条件随机场，生成式有向图模型演变为通用条件随机场。<br><img src="/images/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%B8%AD%E5%9B%BE%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%BC%94%E5%8F%98.png" alt="自然语言处理中图模型的演变"><br>生成式模型（或称产生式模型）与区分式模型（或称判别式模型）的本质区别在于模型中观测序列x和状态序列y之间的决定关系，前者假设y决定x，后者假设x决定y。生成模型以“状态（输出）序列y按照一定的规律生成观测（输入）序列x”为假设，针对联合分布p（x, y）进行建模，并且通过估计使生成概率最大的生成序列来获取y。生成式模型是所有变量的全概率模型，因此可以模拟（“生成”）所有变量的值。在这类模型中一般都有严格的独立性假设，特征是事先给定的，并且特征之间的关系直接体现在公式中。这类模型的优点是：处理单类问题时比较灵活，模型变量之间的关系比较清楚，模型可以通过增量学习获得，可用于数据不完整的情况。其弱点在于模型的推导和学习比较复杂。典型的生成式模型有：n元语法模型、HMM、朴素的贝叶斯分类器、概率上下文无关文法等。<br>判别式模型则符合传统的模式分类思想，认为y由x决定，直接对后验概率p（y|x）进行建模，它从x中提取特征，学习模型参数，使得条件概率符合一定形式的最优。在这类模型中特征可以任意给定，一般特征是通过函数表示的。这种模型的优点是：处理多类问题或分辨某一类与其他类之间的差异时比较灵活，模型简单，容易建立和学习。其弱点在于模型的描述能力有限，变量之间的关系不清楚，而且大多数区分式模型是有监督的学习方法，不能扩展成无监督的学习方法。代表性的区分式模型有：最大熵模型、条件随机场、支持向量机、最大熵马尔可夫模型（maximum-entropy Markov model, MEMM）、感知机（perceptron）等。<br>下面将简要介绍贝叶斯网络和马尔可夫网络的基本概念。由于自然语言处理中需要解决的问题大多数属于“线”的序列结构，因此我们分别以HMM（生成式）和线性链式CRF（判别式）为例来介绍自然语言处理中的概率图模型。其中，HMM以朴素贝叶斯（naΪve Bayes）为基础，CRF以逻辑回归为基础。</p>
<h1 id="贝叶斯网络"><a href="#贝叶斯网络" class="headerlink" title="贝叶斯网络"></a>贝叶斯网络</h1><p>贝叶斯网络又称为信度网络或信念网络（belief networks），是一种基于概率推理的数学模型，其理论基础是贝叶斯公式。贝叶斯网络的概念最初是由Judea Pearl于1985年提出来的 〔1〕，其目的是通过概率推理处理不确定性和不完整性问题。<br>形式上，一个贝叶斯网络就是一个有向无环图（directed acyclic graph, DAG），结点表示随机变量，可以是可观测量、隐含变量、未知参量或假设等；结点之间的有向边表示条件依存关系，箭头指向的结点依存于箭头发出的结点（父结点）。两个结点没有连接关系表示两个随机变量能够在某些特定情况下条件独立，而两个结点有连接关系表示两个随机变量在任何条件下都不存在条件独立。条件独立是贝叶斯网络所依赖的一个核心概念。每一个结点都与一个概率函数相关，概率函数的输入是该结点的父结点所表示的随机变量的一组特定值，输出为当前结点表示的随机变量的概率值。概率函数值的大小实际上表达的是结点之间依存关系的强度。假设父结点有n个布尔变量，概率函数可表示成由2n个条目构成的二维表，每个条目是其父结点各变量可能的取值（“T”或“F”）与当前结点真值的组合。例如，如果一篇文章是关于南海岛屿的新闻（将这一事件记作“News”），文章可能包含介绍南海岛屿历史的内容（这一事件记作“History”），但一般不会有太多介绍旅游风光的内容（将事件“有介绍旅游风光的内容”记作“Sightseeing”）。我们可以构造一个简单的贝叶斯网络，如图6-3所示。<br><img src="/images/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C.png" alt="一个简单的贝叶斯网络"><br>在这个例子中，“文章是关于南海岛屿的新闻”这一事件直接影响“有介绍旅游风光的内容”这一事件。如果分别用N、H、S表示这三个事件，每个变量都有两种可能的取值“T”（表示“有、是”或“包含”）和“F”（表示“没有”、“不是”或“不含”），于是可以对这三个事件之间的关系用贝叶斯网络建模。<br>三个事件的联合概率函数为：<br>$$P（H，S，N）＝P（H|S，N）×P（S|N）×P（N）$$<br>这个模型可以回答如下类似的问题：如果一篇文章中含有南海岛屿历史相关的内容，该文章是关于南海新闻的可能性有多大？<br><img src="/images/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E5%8D%97%E6%B5%B7%E6%96%B0%E9%97%BB%E7%9A%84%E5%8F%AF%E8%83%BD%E6%80%A7%E5%A4%9A%E5%A4%A7.png" alt="南海新闻的可能性多大"><br>构造贝叶斯网络是一项复杂的任务，涉及表示、推断和学习三个方面的问题［Koller and Friedman, 2009］。<br>（1）表示：在某一随机变量的集合$x＝\{X_1，L，X_n\}$上给出其联合概率分布P。在贝叶斯网络表示中的主要问题是，即使在随机变量仅有两种取值的简单情况下，一个联合概率分布也需要对$x_1$，L，$x_n$的所有2n种不同取值下的概率情况进行说明，这无论从计算代价和人的认知能力方面，还是从统计方法学习如此多参数的可能性方面，几乎都是难以做到或者代价昂贵的事情。<br>（2）推断：由于贝叶斯网络是变量及其关系的完整模型，因此可以回答关于变量的询问，如当观察到某些变量（证据变量）时，推断另一些变量子集的变化。在已知某些证据的情况下计算变量的后验分布的过程称作概率推理。常用的精确推理方法包括变量消除法（variable elimination）和团树（clique tree）法。变量消除法的基本任务是计算条件概率$p（X_Q|X_E＝x）$，其中，$X_Q$是询问变量的集合，$X_E$为已知证据的变量集合。其基本思想是通过分步计算不同变量的边缘分布按顺序逐个消除未观察到的非询问变量［Zhang and Poole, 1996］。团树法使用更全局化的数据结构调度各种操作，以获得更加有益的计算代价。<br>常用的近似推理算法有重要性抽样法（importance sampling）、随机马尔可夫链蒙特卡罗（Markov chain Monte Carlo, MCMC）模拟法、循环信念传播法（loopy belief propagation）和泛化信念传播法（generalized belief propagation）等。<br>（3）学习：参数学习的目的是决定变量之间相互关联的量化关系，即依存强度估计。也就是说，对于每个结点X来说，需要计算给定父结点条件下X结点的概率，这些概率分布可以是任意形式的，通常是离散分布或高斯分布。常用的参数学习方法包括最大似然估计法、最大后验概率法、期望最大化方法（EM）和贝叶斯估计方法。在贝叶斯图模型中使用较多的是贝叶斯估计法。<br>除了参数学习以外，还有一项任务是寻找变量之间的图关系，即结构学习。在很简单的情况下贝叶斯网络可以由专家构造，但是在多数实用系统中人工构造一个贝叶斯网络的结构几乎是不可能的，因为这一过程过于复杂，必须从大量数据中学习网络结构和局部分布的参数。自动学习贝叶斯网络的图结构一直是机器学习领域研究的一项颇具挑战性的任务。<br>由于贝叶斯网络是一种不定性因果关联模型，能够在已知有限的、不完整、不确定信息的条件下进行学习和推理，因此广泛应用于故障诊断和维修决策等领域。在自然语言处理中已有专家将其应用于汉语自动分词和词义消歧等任务［卢志茂等，2004］。</p>
<h1 id="马尔科夫模型"><a href="#马尔科夫模型" class="headerlink" title="马尔科夫模型"></a>马尔科夫模型</h1><p>在介绍隐马尔可夫模型之前，先来介绍马尔可夫模型。</p>
<p>我们知道，随机过程又称随机函数，是随时间而随机变化的过程。马尔可夫模型（Markov model）描述了一类重要的随机过程。我们常常需要考察一个随机变量序列，这些随机变量并不是相互独立的，每个随机变量的值依赖于这个序列前面的状态。如果一个系统有N个有限状态$S＝{s_1，s_2，…，s_N}$，那么随着时间的推移，该系统将从某一状态转移到另一状态。$Q＝（q_1，q_2，…，q_T）$为一个随机变量序列，随机变量的取值为状态集S中的某个状态，假定在时间t的状态记为$q_t$。对该系统的描述通常需要给出当前时刻t的状态和其前面所有状态的关系：系统在时间t处于状态$s_j$的概率取决于其在时间1,2，…，t-1的状态，该概率为<br>$$P（q_t＝s_j|q_{t－1}＝s_i，q_{t－2}＝s_k，…）$$<br>如果在特定条件下，系统在时间t的状态只与其在时间t-1的状态相关，即<br>$$P（q_t＝s_j|q_{t－1}＝s_i，q_{t－2}＝s_k，…）= P(q_t = s_j | q_{t-1} =s_j)\qquad (6-1)$$<br>则该系统构成一个离散的一阶马尔可夫链（Markov chain）。<br>进一步，如果只考虑式（6-1）独立于时间t的随机过程：<br>$$P(q_t= s_j | q_{t-1} =s_i) = a_{ij} ,1 \le i,j \le N \qquad (6-2)$$<br>该随机过程为马尔可夫模型。其中，状态转移概率aij必须满足以下条件：<br>$$<br>a_{ij} \ge 0 \qquad (6-3) \\<br>\sum_{j=1}^N a_{ij}=1 \qquad (6-4) \\<br>$$<br>显然，有N个状态的一阶马尔可夫过程有$N^2$次状态转移，其$N^2$个状态转移概率可以表示成一个状态转移矩阵。例如，一段文字中名词、动词、形容词三类词性出现的情况可由三个状态的马尔可夫模型描述：<br>状态s1：名词<br>状态s2：动词<br>状态s3：形容词<br>假设状态之间的转移矩阵如下：<br>$$<br>A= [a_{ij}]=\left[<br>\begin{matrix}<br>0.3 &amp; 0.5 &amp; 0.2 \\<br>0.5 &amp; 0.3 &amp; 0.2 \\<br>0.4 &amp; 0.2 &amp; 0.4 \\<br>\end{matrix}<br>\right]<br>$$<br>如果在该段文字中某一句子的第一个词为名词，那么根据这一模型M，在该句子中这三类词的出现顺序为O＝“名动形名”的概率为<br>$$<br>\begin{aligned} P(O | M)<br>&amp;=P\left(s_{1}, s_{2}, s_{3}, s_{1} | M\right) \\<br>&amp;=P\left(s_{1}\right) \cdot P\left(s_{2} | s_{1}\right) \cdot P\left(s_{3} | s_{2}\right) \cdot P\left(s_{1} | s_{3}\right) \\<br>&amp;=1 \times a_{12} \times a_{23} \times a_{31} \\<br>&amp;=0.5 \times 0.2 \times 0.4 \\<br>&amp;=0.04 \end{aligned}<br>$$<br>系统初始化时可以定义一个初始状态的概率向量$\pi_i≥0$,$\sum_{i=1}^N \pi_i=1$<br>马尔可夫模型又可视为随机的有限状态机。如图6-4所示，圆圈表示状态，状态之间的转移用带箭头的弧表示，弧上的数字为状态转移的概率，初始状态用标记为start的输入箭头表示，假设任何状态都可作为终止状态。图6-4中省略了转移概率为0的弧，对于每个状态来说，发出弧上的概率和为1。从图可以看出，马尔可夫模型可以看作是一个转移弧上有概率的非确定的有限状态自动机。<br><img src="/images/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BE%8B%E5%AD%90.png" alt="马儿可夫模型的例子"><br>一个马尔可夫链的状态序列的概率可以通过计算形成该状态序列的所有状态之间转移弧上的概率乘积而得出，即<br>$$<br>\begin{aligned} P\left(q_{1}, q_{2}, \cdots, q_{T}\right) &amp;=P\left(q_{1}\right) P\left(q_{2} | q_{1}\right) P\left(q_{3} | q_{1}, q_{2}\right) \cdots P\left(q_{T} | q_{1}, q_{2}, \cdots, q_{T-1}\right) \\<br> &amp;=P\left(q_{1}\right) P\left(q_{2} | q_{1}\right) P\left(q_{3} | q_{2}\right) \cdots P\left(q_{T} | q_{T-1}\right) \\<br>  &amp;=\pi_{q_{1}} \prod_{t=1}^{T-1} a_{q_{i} q_{i+1}} \end{aligned}<br>$$<br>其中，$\pi_{q_1}＝P（q_1）$。根据图6-4给出的状态转移概率，我们可以得到：<br>$$<br>\begin{aligned} P(h, e, l, p) &amp;=P\left(q_{1}=h\right) \times P\left(q_{2}=e | q_{1}=h\right) \\<br> &amp; \times P\left(q_{3}=l | q_{2}=e\right) \times P\left(q_{4}=p | q_{3}=l\right) \\<br>  &amp;=1.0 \times 0.3 \times 0.6 \times 1.0 \\<br>   &amp;=0.18 \end{aligned}<br>$$<br>根据第5章介绍的n元语法模型，当n＝2时，实际上就是一个马尔可夫模型。但是，当n≥3时，就不是一个马尔可夫模型，因为它不符合马尔可夫模型的基本约束。不过，对于n≥3的n元语法模型确定数量的历史来说，可以通过将状态空间描述成多重前面状态的交叉乘积的方式，将其转换成马尔可夫模型。在这种情况下，可将其称为m阶马尔可夫模型，这里的m是用于预测下一个状态的前面状态的个数，那么，n元语法模型就是n-1阶马尔可夫模型。</p>
<h1 id="隐马尔可夫模型"><a href="#隐马尔可夫模型" class="headerlink" title="隐马尔可夫模型"></a>隐马尔可夫模型</h1><p>在马尔可夫模型中，每个状态代表了一个可观察的事件，所以，马尔可夫模型有时又称作可视马尔可夫模型（visible Markov model，VMM），这在某种程度上限制了模型的适应性。在隐马尔可夫模型（HMM）中，我们不知道模型所经过的状态序列，只知道状态的概率函数，也就是说，观察到的事件是状态的随机函数，因此，该模型是一个双重的随机过程。其中，模型的状态转换过程是不可观察的，即隐蔽的，可观察事件的随机过程是隐蔽的状态转换过程的随机函数。<br>可以用下面的图6-5说明隐马尔可夫模型的基本原理。<br><img src="/images/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B%E5%9B%BE%E8%A7%A3.png" alt="隐马儿可夫模型图解"></p>
<p>我们可以通过如下例子来说明HMM的含义。假定一暗室中有N个口袋，每个口袋中有M种不同颜色的球。一个实验员根据某一概率分布随机地选取一个初始口袋，从中根据不同颜色的球的概率分布，随机地取出一个球，并向室外的人报告该球的颜色。然后，再根据口袋的概率分布选择另一个口袋，根据不同颜色的球的概率分布从中随机选择另外一个球。重复进行这个过程。对于暗室外边的人来说，可观察的过程只是不同颜色的球的序列，而口袋的序列是不可观察的。在这个过程中，每个口袋对应于HMM中的状态，球的颜色对应于HMM中状态的输出符号，从一个口袋转向另一个口袋对应于状态转换，从口袋中取出球的颜色对应于从一个状态输出的观察符号。<br>通过上例可以看出，一个HMM由如下几个部分组成：<br>（1）模型中状态的数目N（上例中口袋的数目）；<br>（2）从每个状态可能输出的不同符号的数目M（上例中球的不同颜色的数目）；<br>（3）状态转移概率矩阵A＝｛$a_{ij}$｝（$a_{ij}$为实验员从一个口袋（状态$s_i$）转向另一个口袋（$s_j$）取球的概率），其中，<br>$$<br>\begin{aligned}<br>a_{i j} &amp;=P\left(q_{t}=s_{j} | q_{t-1}=s_{i}\right), \quad 1 \leqslant i, j \leqslant N \\<br>a_{i j} &amp;\geqslant 0  \\<br>\sum_{j=1}^{N} &amp; a_{i j} =1<br>\end{aligned}<br>$$<br>（4）从状态$s_j$观察到符号$v_k$的概率分布矩阵B＝${b_j（k）}（b_j（k）$为实验员从第j个口袋中取出第k种颜色的球的概率），其中，<br>$$<br>\begin{array}{l}{b_{j}(k)=P\left(O_{t}=v_{k} | q_{t}=s_{j}\right), \quad 1 \leqslant j \leqslant N ; 1 \leqslant k \leqslant M} \\<br>{b_{j}(k) \geqslant 0} \\<br>{\sum_{k=1}^{M} b_{j}(k)=1}\end{array}<br>$$<br>观察符号的概率又称符号发射概率（symbol emission probability）。</p>
<p>（5）初始状态概率分布$\pi＝{\pi_i}$，其中，<br>$$<br>\begin{array}{l}{\pi_{i}=P\left(q_{1}=s_{i}\right), \quad 1 \leqslant i \leqslant N} \\<br>{\pi_{i} \geqslant 0} \\<br>{\sum_{i=1}^{N} \pi_{i}=1}\end{array}<br>$$<br>一般地，一个HMM记为一个五元组μ＝（S，K，A，B，π），其中，S为状态的集合，K为输出符号的集合，π，A和B分别是初始状态的概率分布、状态转移概率和符号发射概率。为了简单，有时也将其记为三元组μ＝（A，B，π）。<br>当考虑潜在事件随机地生成表面事件时，HMM是非常有用的。假设给定模型μ＝（A，B，π），那么，观察序列$O＝O_1O_2…O_T$可以由下面的步骤直接产生：<br>（1）根据初始状态的概率分布πi选择一个初始状态q1＝si；<br>（2）设t＝1；<br>（3）根据状态si的输出概率分布bi（k）输出Ot＝vk；<br>（4）根据状态转移概率分布aij，将当前时刻t的状态转移到新的状态qt＋1＝sj；<br>（5）t＝t＋1，如果t＜T，重复执行步骤（3）和（4），否则，结束算法。<br>HMM中有三个基本问题：<br>（1）估计问题：给定一个观察序列$O＝O_1O_2…O_T$和模型μ＝（A，B，π），如何快速地计算出给定模型μ情况下，观察序列O的概率，即P（O|μ）？<br>（2）序列问题：给定一个观察序列$O＝O_1O_2…O_T$和模型μ＝（A，B，π），如何快速有效地选择在一定意义下“最优”的状态序列$Q＝q_1q_2…q_T$，使得该状态序列“最好地解释”观察序列？<br>（3）训练问题或参数估计问题：给定一个观察序列$O＝O_1O_2…O_T$，如何根据最大似然估计来求模型的参数值？即如何调节模型μ＝（A，B，π）的参数，使得P（O|μ）最大？<br>下面描述的前后向算法及参数估计将给出这三个问题的解决方案。</p>
<h2 id="求解观察序列的概率"><a href="#求解观察序列的概率" class="headerlink" title="求解观察序列的概率"></a>求解观察序列的概率</h2><p>给定一个观察序列$O＝O_1O_2…O_T$和模型μ＝（A，B，π），要快速地计算出给定模型μ情况下观察序列O的概率，即P（O|μ）。这就是解码（decoding）问题。<br>对于任意的状态序列$Q＝q_1q_2…q_T$，有<br>$$<br>\begin{aligned} P(O | Q, \mu) &amp;=\prod_{t=1}^{T-1} P\left(O_{t} | q_{t}, q_{t+1}, \mu\right) \\<br> &amp;=b_{q_{1}}\left(O_{1}\right) \times b_{q_{2}}\left(O_{2}\right) \times \cdots \times b_{q_{T}}\left(O_{T}\right) \end{aligned}<br>$$<br>并且<br>$$<br>P(Q | \mu)=\pi_{q_{1}} a_{q_{1} q_{2}} a_{q_{2} q_{3}} \cdots a_{q_{T-1} q_{T}}<br>$$<br>由于<br>$$<br>P(O, Q | \mu)=P(O | Q, \mu) P(Q | \mu)<br>$$<br>因此<br>$$<br>\begin{aligned} P(O | \mu) &amp;=\sum_{Q} P(O, Q | \mu) \\<br>&amp;=\sum_{Q} P(O | Q, \mu) P(Q | \mu) \\<br>&amp;=\sum_{Q} \pi_{q_{1}} b_{q_{1}}\left(O_{1}\right) \prod_{t=1}^{T-1} a_{q_{t} q_{t+1}} b_{q_{t+1}}\left(O_{t+1}\right) \end{aligned}<br>$$<br>上述推导方式很直接，但面临一个很大的困难是，必须穷尽所有可能的状态序列。如果模型μ＝（A，B，π）中有N个不同的状态，时间长度为T，那么，有NT个可能的状态序列。这样，计算量会出现“指数爆炸”。当T很大时，几乎不可能有效地执行这个算法。为此，人们提出了前向算法或前向计算过程（forward procedure），利用动态规划的方法来解决这一问题，使“指数爆炸”问题可以在时间复杂度为O（$N^2T$）的范围内解决。<br>HMM中的动态规划问题一般用格架（trellis或lattice）的组织形式描述。对于一个在某一时间结束在一定状态的HMM，每一个格能够记录该HMM所有输出符号的概率，较长子路径的概率可以由较短子路径的概率计算出来，如图6-6所示［Manning and Schütze,1999］。<br><img src="/images/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E6%A0%BC%E6%9E%B6%E7%AE%97%E6%B3%95%E7%A4%BA%E6%84%8F%E5%9B%BE.png" alt="格架算法示意图"><br>为了实现前向算法，需要定义一个前向变量$α<em>t（i）$。<br>定义6-1　前向变量αt（i）是在时间t，HMM输出了序列$O_1O_2…O_t$，并且位于状态$s_i$的概率：<br>$$\alpha</em>{t}(i)=P\left(O_{1} O_{2} \cdots O_{t}, q_{t}=s_{i} | \mu\right)$$<br>前向算法的主要思想是，如果可以快速地计算前向变量$α<em>t（i）$，那么，就可以根据$α_t（i）$计算出P（O|μ），因为P（O|μ）是在所有状态qT下观察到序列$O＝O_1O_2…O_T$的概率：<br>$$P(O | \mu)=\sum</em>{x_{i}} P\left(O_{1} O_{2} \cdots O_{T}, q_{T}=s_{i} | \mu\right)=\sum_{i=1}^{N} \alpha_{T}(i)$$<br>在前向算法中，采用动态规划的方法计算前向变量$\alpha_t（i）$。其实现思想基于如下观察：在时间t＋1的前向变量可以根据在时间t时的前向变量$\alpha_t（1），\alpha_t（2），\cdots，\alpha_t（N）$的值来归纳计算：<br>$$<br>\alpha_{t+1}(j)=\left(\sum_{i=1}^{N} \alpha_{t}(i) a_{i j}\right) b_{j}\left(O_{t+1}\right)<br>（6-14）<br>$$<br>在格架结构中，$α<em>{t＋1}（j）$存放在$（s_j，t＋1）$处的结点上，表示在已知观察序列$O_1O_2…O_tO</em>{t＋1}$的情况下，从时间t到达下一个时间t＋1时状态为$s_j$的概率。图6-7描述了式（6-14）的归纳关系。<br><img src="/images/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E5%89%8D%E5%90%91%E5%8F%98%E9%87%8F%E7%9A%84%E5%BD%92%E7%BA%B3%E5%85%B3%E7%B3%BB.png" alt="前向变量的归纳关系"><br>这里$s_i$可以是HMM的任意状态。根据前向变量$\alpha_t（i）$的定义，从某一个状态$s_i$出发完成第一步的概率就是$\alpha_t（i）$，而实现第二步的概率为$\alpha_{ij}×b_j（O_{t＋1}）$。因此，从初始时间到t＋1整个过程的概率为：$\alpha_t（i）×a_{ij}×b_j（O_{t＋1}）$。由于HMM可以从不同的$s_i$转移到$s_j$，一共有N个不同的状态，因此，得到了式（6-14）。<br>根据式（6-14）给出的归纳关系，可以按时间顺序和状态顺序依次计算前向变量$\alpha_1（x），\alpha_2（x），…，\alpha_T（x）$（x为HMM的状态变量）。由此，得到如下前向算法。<br><strong>前向算法（forward procedure）</strong><br>初始化：<br>$$<br>\alpha_{1}(\mathrm{i})=\pi b_i (O_1), 1 \le i \le N<br>$$<br>归纳计算：<br>$$<br>\alpha_{i+1}(j)=\left(\sum_{i=1}^{N} \alpha_{t}(i) a_{i j}\right) b_{j}\left(O_{i+1}\right), \quad 1 \leqslant t \leqslant T-1<br>$$<br>求和终结：<br>$$<br>P(O | \mu)=\sum_{i=1}^{N} \alpha_{T}(i)<br>$$<br>在初始化步骤中，πi是初始状态$s_i$的概率，$b_i（O1）$是在$s_i$状态输出$O_1$的概率，那么，$π<em>ib_i（O1）$就是在时刻t＝1时，HMM在$s_i$状态输出序列$O_1$的概率，即前向变量$\alpha_1（i）$。一共有N个状态，因此，需要初始化N个前向变量$\alpha_1（1），\alpha_1（2），…，\alpha_1（N）$。<br>现在我们来分析前向算法的时间复杂性。由于每计算一个$\alpha_t（i）$必须考虑t-1时的所有N个状态转移到状态$s_i$的可能性，其时间复杂性为O（N），那么，对应每个时间t，要计算N个前向变量，$\alpha_1（1），\alpha_1（2），…，\alpha_1（N）$。，因此，时间复杂性为$O（N）×N＝O（N^2）$。因而，在1,2，…，T整个过程中，前向算法的总时间复杂性为$O（N^2T）$。<br>对于求解HMM中的第一个问题，即在给定一个观察序列$O＝O_1O_2…O_T$和模型μ＝（A，B，π）情况下，快速计算P（O|μ）的问题还可以采用另外一种实现方法，即后向算法。<br>后向变量$β_t（i）$是在给定了模型μ＝（A，B，π），并且在时间t状态为$s_i$的条件下，HMM输出观察序列$O</em>{t＋2}…O_T$的概率：<br>$$<br>\beta_{t}(i)=P\left(O_{t+1} O_{t+2} \cdots O_{T} | q_{t}=s_{i}, \mu\right)<br>$$<br>与计算前向变量一样，可以用动态规划的算法计算后向变量。类似地，在时间t状态为si的条件下，HMM输出观察序列$O_{t＋1}O_{t＋2}…O_T$的过程可以分解为以下两个步骤：<br>（1）从时间t到时间t＋1，HMM由状态$s_i$到状态$s_j$，并从$s_j$输出O_{t＋1}；<br>（2）在时间t＋1的状态为$s_j$的条件下，HMM输出观察序列$O_{t＋2}…O_T$。<br>第一步中输出$O_{t＋1}$的概率为：$a_{ij}×b_j（O_{t＋1}）$；第二步中根据后向变量的定义，HMM输出观察序列为$O_{t＋2}…O_T$的概率就是后向变量$\beta_{t＋1}（j）$。于是，得到如下归纳关系：<br>$$<br>\beta_{t}(i)=\sum_{j=1}^{N} a_{i j} b_{j}\left(O_{t+1}\right) \beta_{t+1}(j) \quad (6-16)<br>$$<br>式（6-16）的归纳关系可以由图6-8来描述。<br><img src="/images/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E5%90%8E%E5%90%91%E5%8F%98%E9%87%8F%E7%9A%84%E5%BD%92%E7%BA%B3%E5%85%B3%E7%B3%BB.png" alt="后向变量的归纳关系"><br>根据后向变量的归纳关系，按T，T-1，…，2,1顺序依次计算$\beta_T（x），\beta_{T-1}（x），…，\beta_1（x）$（x为HMM的状态），就可以得到整个观察序列$O＝O_1O_2…O_T$的概率。下面的后向算法用于实现这个归纳计算的过程。</p>
<p><strong>后向算法（backward procedure）</strong><br>初始化：<br>$$<br>\beta_{\mathrm{T}}(\mathrm{i})=1, \quad 1 \leq \mathrm{i} \leq \mathrm{N}<br>$$<br>归纳计算<br>$$<br>\beta_{t}(i)=\sum_{j=1}^{N} a_{i j} b_{j}\left(O_{t+1}\right) \beta_{t+1}(j), \quad T-1 \geqslant t \geqslant 1 ; 1 \leqslant i \leqslant N<br>$$<br>求和终结<br>$$<br>P(O | \mu)=\sum_{i=1}^{N} \pi_{i} b_{i}\left(O_{1}\right) \beta_{1}(i)<br>$$<br>类似于前向算法的分析，可知后向算法的时间复杂度也是$O（N^2T）$。<br>更一般地，实际上我们可以采用前向算法和后向算法相结合的方法来计算观察序列的概率：<br>$$<br>\begin{aligned} P\left(O, q_{t}=s_{i} | \mu\right) &amp;=P\left(O_{1} \cdots O_{T}, q_{t}=s_{i} | \mu\right) \\<br>&amp;=P\left(O_{1} \cdots O_{t}, q_{t}=s_{i}, O_{t+1} \cdots O_{T} | \mu\right) \\<br>&amp;=P\left(O_{1} \cdots O_{t}, q_{t}=s_{i} | \mu\right) \times P\left(O_{t+1} \cdots O_{T} | O_{1} \cdots O_{t}q_{t}=s_{i}, \mu\right) \\<br>&amp;=P\left(O_{1} \cdots O_{t}, q_{t}=s_{i} | \mu\right) \times P\left(O_{t+1} \cdots O_{T} | q_{t}=s_{i},\mu\right) \\<br> &amp;=\alpha_{t}(i) \beta_{t}(i) \end{aligned} (6-17)<br>$$<br>因此<br>$$<br>P(O | \mu)=\sum_{i=1}^{N} \alpha_{t}(i) \times \beta_{t}(i), \quad 1 \leqslant t \leqslant T<br>\qquad (6-18)<br>$$</p>
<h2 id="维比特算法"><a href="#维比特算法" class="headerlink" title="维比特算法"></a>维比特算法</h2><p>维特比（Viterbi）算法用于求解HMM中的第二个问题，即给定一个观察序列$O＝O_1O_2…O_T$和模型μ＝（A，B，π），如何快速有效地选择在一定意义下“最优”的状态序列$Q＝q_1q_2…q_T$，使得该状态序列“最好地解释”观察序列。这个问题的答案并不是唯一的，因为它取决于对“最优状态序列”的理解。一种理解是，使该状态序列中每一个状态都单独地具有最大概率，即要使得<br>$$<br>\gamma_{t}(i)=P\left(q_{t}=s_{i} | O, \mu\right)<br>$$<br>根据贝叶斯公式，有<br>$$<br>\gamma_{t}(i)=P\left(q_{t}=s_{i} | O, \mu\right)=\cfrac{P\left(q_{t}=s_{i}, O | \mu \right)}{P(O | \mu)}<br>$$<br>参考式（6-17）和式（6-18），并且$P（q_t＝s_i，O|μ）＝P（O, q_t＝s_i|μ）$，因此，<br>$$<br>\gamma_{t}(i)=\frac{\alpha_{t}(i) \beta_{t}(i)}{\sum_{i=1}^{N} \alpha_{t}(i) \times \beta_{t}(i)}<br>$$<br>有了$\gamma_t（i）$，那么，在时间t的最优状态为<br>$$<br>hat(q_t)= \underset{1 \le i \le N}{\arg \max} [ \gamma_t(i) ]<br>$$<br>根据这种对“最优状态序列”的理解，如果只考虑使每个状态的出现都单独达到最大概率，而忽略了状态序列中两个状态之间的关系，很可能导致两个状态$hat(q_t)$和$hat(q_{t+1})$之间的转移概率为0，即$a_{hat(q_t) hat(q_{t+1})}=0$。那么，在这种情况下，所谓的“最优状态序列”根本就不是合法的序列。因此，我们常常采用另一种对“最优状态序列”的理解：在给定模型μ和观察序列O的条件下，使条件概率P（Q|O，μ）最大的状态序列，即<br>$$<br>\hat{Q}=\underset{Q}{\operatorname{argmax}} P(Q | O, \mu)<br>$$<br>这种理解避免了前一种理解引起的“断序”的问题。根据这种理解，优化的不是状态序列中的单个状态，而是整个状态序列，不合法的状态序列的概率为0，因此，不可能被选为最优状态序列。<br>维特比算法运用动态规划的搜索算法求解这种最优状态序列。为了实现这种搜索，首先定义一个维特比变量$<br>\delta_{\mathrm{t}}(\mathrm{i})<br>$。<br>定义6-3　维特比变量$\delta_t（i）$是在时间t时，HMM沿着某一条路径到达状态$s_i$，并输出观察序列$O_1O_2…O_t$的最大概率：<br>$$<br>\delta_{t}(i)=\max <em>{q</em>{1}, q_{2}, \ldots, q_{i-1}} P\left(q_{1}, q_{2}, \cdots, q_{t}=s_{i}, O_{1} O_{2} \cdots O_{t} | \mu\right)<br>$$<br>与前向变量类似，$\delta_{t+1}(i)$有如下递归关系：<br>$$<br>\delta_{t+1}(i)=\max_{j}\left[\delta_{t}(j) \cdot a_{j i}\right] \cdot b_{i}\left(O_{t+1}\right)<br>$$<br>这种递归关系使我们能够运用动态规划搜索技术。为了记录在时间t时，HMM通过哪一条概率最大的路径到达状态$s_i$，维特比算法设置了另外一个变量$\psi_{\mathrm{t}}(\mathrm{i})$用于路径记忆，让$\psi_{\mathrm{t}}(\mathrm{i})$记录该路径上状态$s_i$的前一个（在时间t-1的）状态。根据这种思路，给出如下维特比算法。<br><strong>算法6-3　维特比算法（Viterbi algorithm）</strong><br>注：$ hat(Q)= \hat{Q}$<br>初始化：<br>$$<br>\begin{aligned} \delta_{1}(i) &amp;=\pi_{i} b_{i}\left(O_{1}\right), \quad 1 \leqslant i \leqslant N \\<br>\psi_{1}(i) &amp;=0 \end{aligned}<br>$$<br>归纳计算：<br>$$<br>\delta_{t}(j)=\max_{1 \leqslant \leqslant N}\left[\delta_{i-1}(i) \cdot a_{i j}\right] \cdot b_{j}\left(O_{t}\right), \quad 2 \leqslant t \leqslant T ; 1 \leqslant j \leqslant N<br>$$<br>记忆回退路径：<br>$$<br>\psi_{t}(j)=\max_{1 \leqslant \leqslant N}\left[\delta_{i-1}(i) \cdot a_{i j}\right] \cdot b_{j}\left(O_{t}\right), \quad 2 \leqslant t \leqslant T ; 1 \leqslant i \leqslant N<br>$$<br>终结：<br>$$<br>hat(Q_T)=\arg \max_{1 \leqslant \leqslant N} [ \delta_T(i)] \\<br>hat(P(hat(Q_T)))= \max_{1 \leqslant \leqslant N}[ \delta_T(i)]<br>$$<br>路径回溯：<br>$$<br>hat(q_t)=\psi_{t+1}\left(\hat{q}_{t+1}\right) \quad, \quad t=T-1, T-2, \cdots, 1<br>$$<br>维特比算法的时间复杂性与前向算法、后向算法的时间复杂性一样，也是$O（N^2T）$。<br>在实际应用中，往往不只是搜索一个最优状态序列，而是搜索n个最佳（n-best）路径，因此，在格架的每个结点上常常需要记录m个最佳（m-best, m＜n）状态。</p>
<h2 id="HMM的参数估计"><a href="#HMM的参数估计" class="headerlink" title="HMM的参数估计"></a>HMM的参数估计</h2><p>参数估计问题是HMM面临的第三个问题，即给定一个观察序列O＝O1O2…OT，如何调节模型μ＝（A，B，π）的参数，使得P（O|μ）最大化：<br>$$<br>\underset{\mu}{\operatorname{argmax}} P\left(O_{\text { training }} | \mu\right)<br>$$<br><img src="/images/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1HMM.png" alt="最大似然估计HMM"><br>但实际上，由于HMM中的状态序列Q是观察不到的（隐变量），因此，这种最大似然估计的方法不可行。所幸的是，期望最大化（expectation maximization, EM）算法可以用于含有隐变量的统计模型的参数最大似然估计。其基本思想是，初始时随机地给模型的参数赋值，该赋值遵循模型对参数的限制，例如，从某一状态出发的所有转移概率的和为1。给模型参数赋初值以后，得到模型$μ_0$，然后，根据$μ_0$可以得到模型中隐变量的期望值。例如，从$μ_0$得到从某一状态转移到另一状态的期望次数，用期望次数来替代式（6-23）中的实际次数，这样可以得到模型参数的新估计值，由此得到新的模型$μ_1$。从$μ_1$又可以得到模型中隐变量的期望值，然后，重新估计模型的参数，执行这个迭代过程，直到参数收敛于最大似然估计值。</p>
<p>这种迭代爬山算法可以局部地使P（O|μ）最大化。Baum-Welch算法或称前向后向算法（forward-backward algorithm）用于具体实现这种EM方法。下面我们介绍这种算法。<br>给定HMM的参数μ和观察序列$O＝O_1O_2…O_T$，在时间t位于状态$s_i$，时间t＋1位于状态$s_j$的概率$ξ<em>t（i, j）$＝$P（q_t＝s_i，q</em>{t＋1}＝s_j|O，μ）$（1≤t≤T,1≤i, j≤N）可以由下面的公式计算获得：<br>$$<br>\begin{align}<br>\xi_{t}(i, j)&amp; =\frac{P\left(q_{t}=s_{i}, q_{t+1}=s_{j}, O | \mu\right)}{P(O | \mu)} \\<br>&amp; =\frac{\alpha_{t}(i) a_{i j} b_{j}\left(O_{t+1}\right) \beta_{t+1}(j)}{P(O | \mu)} \\<br>&amp;=\frac{\alpha_{t}(i) a_{i j} b_{j}\left(O_{t+1}\right) \beta_{t+1}(j)}{\sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{t}(i) a_{i j} b_{j}\left(O_{t+1}\right) \beta_{t+1}(j)}<br>\end{align}<br>$$<br>图6-9给出了式（6-24）所表达的前向变量$\alpha_t（i）$、后向变量$\beta_{t＋1}（j）$与概率$\xi_t（i, j）$之间的关系。<br><img src="/images/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/xi%E4%B8%8E%E5%89%8D%E5%90%8E%E5%90%91%E5%8F%98%E9%87%8F%E4%B9%8B%E9%97%B4%E7%9A%84%E5%85%B3%E7%B3%BB.png" alt="xi与前后向变量之间的关系"><br>给定HMM μ和观察序列$O＝O_1O_2…O_T$，在时间t位于状态$s_i$的概率$\gamma_t（i）$为<br>$$<br>\gamma_{t}(i)=\sum_{j=1}^{N} \hat{\xi}_{t}(i, j)<br>$$<br>由此，μ的参数可以由下面的公式重新估计：<br><img src="/images/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/HMM%E7%9A%84%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.png" alt="HMM的参数估计"><br>根据上述思路，给出如下前向后向算法。</p>
<p><strong>前向后向算法（forward-backward algorithm）</strong></p>
<p>步1　初始化：随机地给参数$\pi_i$，$a_{ij}$，$b_j（k）$赋值，使其满足如下约束：<br>$$<br>\begin{array}{l}{\sum_{i=1}^{N} \pi_{i}=1} \\<br>{\sum_{j=1}^{N} a_{i j}=1, \quad 1 \leqslant i \leqslant N} \\<br>{\sum_{k=1}^{M} b_{j}(k)=1, \quad 1 \leqslant j \leqslant N}\end{array}<br>$$<br>由此得到模型$μ<em>0$。令i＝0，执行下面的EM估计。<br>步2　EM计算：<br>E-步骤：由模型$μ_i$根据式（6-24）和式（6-25）计算期望值$\xi_t（i, j）$和$\gamma（i）$；<br>M-步骤：用E-步骤得到的期望值，根据式（6-26）、（6-27）和（6-28）重新估计参数$\pi_i$，$a</em>{ij}$，$b_j（k）$的值，得到模型$μ<em>{i＋1}$。<br>步3　循环计算：<br>令i＝i＋1。重复执行EM计算，直到$\pi_i$，$a</em>{ij}$，$b_j（k）$收敛。</p>
<p>HMM在自然语言处理研究中有着非常广泛的应用。需要提醒的是，除了上述讨论的理论问题以外，在实际应用中还有若干实现技术上的问题需要注意。例如，多个概率连乘引起的浮点数下溢问题。在Viterbi算法中只涉及乘法运算和求最大值问题，因此，可以对概率相乘的算式取对数运算，使乘法运算变成加法运算，这样一方面避免了浮点数下溢的问题，另一方面，提高了运算速度。在前向后向算法中，也经常采用如下对数运算的方法判断参数$\pi_i$，$a_{ij}$，$b_j（k）$是否收敛：</p>
<p>$$| \log \mathrm{P}\left(\mathrm{O} | \mu_{\mathrm{i}+1}\right)-\log \mathrm{P}\left(\mathrm{O} | \mu_{\mathrm{i}}\right) |&lt;\varepsilon$$<br>其中，ε为一个足够小的实数值。但是，在前向后向算法中执行EM计算时有加法运算，这就使得EM计算中无法采用对数运算，在这种情况下，可以设置一个辅助的比例系数，将概率值乘以这个比例系数以放大概率值，避免浮点数下溢。在每次迭代结束重新估计参数值时，再将比例系数取消。</p>
<p>关于隐马尔可夫模型的实现工具，可参阅网站：<a href="http://htk.eng.cam.ac.uk/。" target="_blank" rel="noopener">http://htk.eng.cam.ac.uk/。</a></p>
<h1 id="层次化的隐马尔可夫模型"><a href="#层次化的隐马尔可夫模型" class="headerlink" title="层次化的隐马尔可夫模型"></a>层次化的隐马尔可夫模型</h1><p>在自然语言处理等应用中，由于处理序列具有递归特性，尤其当序列长度较大时，隐马尔可夫模型的复杂度将会急剧增大，因此，Shai Fine等人提出了层次化隐马尔可夫模型（hierarchical hidden Markov models, HHMM）［Fine et al., 1998］ 〔2〕。<br>层次化的隐马尔可夫模型是由多层随机过程构成的。在HHMM中每个状态本身就是一个独立的HHMM，因此一个HHMM的状态产生一个观察序列，而不是一个观察符号。HHMM通过状态转移递归地产生观察序列，一个状态可以激活下层状态中的某一个状态，而被激活的状态又可以激活再下层的状态，直至到达某个特定的状态这一递归过程结束。该特定状态称为生产状态（production state），只有生产状态才能通过常规的HMM机制，即根据输出符号的概率分布产生可观察的输出符号。不直接产生可观察符号的隐藏状态称作内部状态。不同层次之间的状态转移叫垂直转移（vertical transition），同一层次上状态之间的转移叫水平转移（horizontal transition）。当状态转移到达某个生产状态，产生一个观察输出后，终止状态控制转移过程返回到激活该层状态转移的上层状态。这一递归转移过程将形成一个生产状态序列，而每个生产状态生成一个观察输出符号，因此生产状态序列将为顶层状态生成一个观察输出序列。状态及其状态之间的垂直转移形成了HHMM的树状结构，如图6-10所示。<br><img src="/images/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/HHMM%E7%9A%84%E6%A0%91%E7%8A%B6%E7%BB%93%E6%9E%84.png" alt="HHMM的树状结构"><br>图6-10中，$q_1^1$为根状态，双圈表示终止状态，用于控制转移过程返回到激活该层状态的上层状态。其他状态为内部状态。为了简化起见图中没有画出生产状态。<br><img src="/images/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/HHMM%E7%9A%84%E5%BD%A2%E5%BC%8F%E5%8C%96%E6%8F%8F%E8%BF%B0.png" alt="HHMM的形式化描述"><br>根据上述解释，HHMM按如下方式产生一个观察序列：从根状态开始根据初始概率分布$\Pi^{\mathrm{q}^{1}}$随机选择一个子状态。对于每一个内部状态q，根据q的初始概率向量$\Pi^q$随机地选择一个子状态，这一过程重复进行，递归地激活和选择一个子状态，直至到达一个生产状态$q^D$，该生产状态根据输出概率向量$B^{q^D}$产生一个观察符号。然后，终止状态控制递归过程返回激活$q^D$的上层状态。在一个字符串的递归生成过程中，从一个内部状态开始根据同一层的状态转移矩阵选择下一个状态，新选择的状态启动一个新的字符串递归生成过程。当所有的递归生成过程完成之后，返回到根状态时，观察序列的产生过程就结束了。这里假设从根状态出发在有限的步骤内可以到达HHMM的所有状态，也就是说，HHMM是强联通的。<br>像隐马尔可夫模型一样，HHMM中也有如下三个基本问题需要解决：<br>（1）快速地计算观察序列的概率：给定一个HHMM及其参数设置$λ＝{λ^{q^d}}$，快速地计算模型λ生成序列$\bar{O}$的概率P（$\bar{O}$|λ）。<br>（2）求解模型最有可能的状态序列：给定一个HHMM及其参数设置$λ＝{λ^{q^d}}$和观察序列$\bar{O}$，求解一个最有可能的状态序列使其最好地解释观察序列$\bar{O}$。<br>（3）估计模型的参数：给定一个HHMM的结构和一个或多个观察序列（{$\bar{O}_t$）}，求解最有可能的模型参数λ<em>使得λ</em>＝argmaxλP（{$\bar{O}$t}|λ）。<br>由于HHMM的层次化结构和多解特性，求解HHMM的这三个问题远比解决HMM的三个问题困难得多。例如，对于一个给定的观察序列，其最有可能的状态序列是一个由激活状态构成的多解结构，而不是一个最有可能到达状态的简单序列。不再详细介绍关于HHMM三个问题的求解方法，有兴趣的读者可以参阅［Fine et al., 1998］等相关文献。</p>
<h1 id="马尔可夫网络"><a href="#马尔可夫网络" class="headerlink" title="马尔可夫网络"></a>马尔可夫网络</h1><p>马尔可夫网络与贝叶斯网络有类似之处，也可用于表示变量之间的依赖关系。但是，它又与贝叶斯网络有所不同。一方面，它可以表示贝叶斯网络无法表示的一些依赖关系，如循环依赖；另一方面，它不能表示贝叶斯网络能够表示的某些关系，如推导关系 〔3〕。<br>马尔可夫网络是一组有马尔可夫性质的随机变量的联合概率分布模型，它由一个无向图G和定义于G上的势函数组成。一个无向图G＝（V，E），每个顶点$x_i$∈V表示在集合X上的一个随机变量，每条边${x_i，x_j}$∈E（i≠j）表示直接相连的两个随机变量$x_i$和$x_j$之间的一种依赖关系。为了便于叙述，首先给出如下定义。<br>定义6-4（子图）　假设两个图分别为G＝〈V，E〉和$G_s$＝〈$V_s$，$E_s$〉，如果$V_s$⊆V并且$E_s$⊆E，那么，称$G_s$为G的子图。<br>如果一个子图中的任意两个结点之间都有边相连，那么这个子图就是一个完全子图（complete subgraph），一个全子图又称为一个团（clique）。一个团的完全子图称作子团。如图6-11中，结点$x_1$和$x_4$及其边$x_1x_4$构成一个完全子图，结点$x_3$和$x_4$及其边$x_3x_4$，以及结点$x_1、x_3、x_4$及其边$x_1x_3$，$x_1x_4$和$x_3x_4$也分别是一个完全子图，而结点$x_2、x_3、x_4$构成的图则不是完全子图。<br><img src="/images/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E5%9B%BE.png" alt="一个简单的图"><br>在无向图中，不用条件概率密度对模型进行参数化，而是使用一种称为团势能（clique potentials）的参数化因子。所谓团势能又称团势能函数（clique potential function）或简称势函数，是定义在一个团上的非负实函数。每个团都对应着一个势函数，表示团的一个状态。<br>一般用$x_C$来表示团C中所有的结点，用$\phi（x_C）$表示团势能。如图6-11中两个团可以表示为$x_{C_1}＝{x_1，x_2}$，$x_{C_2}＝{x_1，x_3，x_4}$。由于定义中要求势能函数$\phi（x_C）$非负，所以一般将$\phi（x_C）$定义为：$\phi（x_C）＝exp{－E（x_C）}$，其中$E（x_C）$称为$x_C$的能量函数（energy function）。<br>如果分布$P_{\phi}（x_1，x_2，…，x_n）$的图模型可以表示为一个马尔可夫网络H，当C是H上完全子图的集合时，我们说H上的分布$P_{\phi}（x_1，x_2，…，x_n）$可以用C的团势能函数$\phi（x_C）$进行因子化：$\phi ＝{\phi_1（x_{C_1}），…，\phi_K（x_{C_K}）}$。$P_\phi（x_1，x_2，…，x_n）$可以看作H上的一个吉布斯分布（Gibbs distribution），其概率分布密度为：<br>$$<br>p\left(x_{1}, x_{2}, \cdots, x_{n}\right)=\frac{1}{Z} \prod_{i=1}^{K} \phi_{i}\left(\mathbf{x}<em>{C</em>{i}}\right)<br>$$<br>其中，Z是一个归一化常量，称为划分函数（partition function）。<br>其中，$x_{C_i} \subseteq {x_1，x_2，…，x_n}$（1≤i≤K），并且满足$\bigcup_{i=1}^{K} x_{C_i}=\{x_1,x_2,…,x_n \}$。<br>显然，在无向图模型中每个$C_i$对应于一个团，而相应的吉布斯分布就是整个图模型的概率分布。图6-11中的两个团$x_{C_1}＝{x_1，x_2}$和$x_{C_2}＝{x_1，x_3，x_4}$就可以定义相应的吉布斯分布，因为满足条件$x_{C_1}  \cup x_{C_2}＝{x_1，x_2，x_3，x_4}$。<br>因子化的乘积运算可以变成加法运算：</p>
<p>$$<br>p\left(x_{1}, x_{2}, \cdots, x_{n}\right)=\frac{1}{Z} \exp \left\{-\sum_{i=1}^{K} E_{C_{i}}\left(x_{C_{i}}\right)\right\}=\frac{1}{Z} \exp \{-E(\mathbf{x})\}<br>$$</p>
<p>其中，$\sum_{i=1}^{K} E_{C_{i}}\left(x_{C_{i}}\right)$</p>
<h1 id="最大熵模型"><a href="#最大熵模型" class="headerlink" title="最大熵模型"></a>最大熵模型</h1><p>最大熵原理最早由E. T. Jaynes于1957年提出 〔4〕，1996年被应用于自然语言处理［Berger et al., 1996］。</p>
<h2 id="最大熵原理"><a href="#最大熵原理" class="headerlink" title="最大熵原理"></a>最大熵原理</h2><p>最大熵模型的基本原理是：在只掌握关于未知分布的部分信息的情况下，符合已知知识的概率分布可能有多个，但使熵值最大的概率分布最真实地反映了事件的分布情况，因为熵定义了随机变量的不确定性，当熵最大时，随机变量最不确定，最难准确地预测其行为。也就是说，在已知部分信息的前提下，关于未知分布最合理的推断应该是符合已知信息最不确定或最大随机的推断。<br>对于自然语言处理中某个歧义消解问题，若用A表示待消歧问题所有可能候选结果的集合，B表示当前歧义点所在上下文信息构成的集合，则称（a, b）为模型的一个特征。一般定义{0, 1}域上的一个二值函数来表示特征：<br>$$<br>f(a, b)=<br>\begin{cases}<br>1, \text{如果} (a,b) \in (A,B),\text{且满足某种条件} \\<br>0, \text{其他情况}<br>\end{cases}<br>$$<br>在不引起混淆的情况下，有时也直接把特征函数f（a, b）称作特征。我们可以把“判定歧义问题为某种可能的结果a∈A”看作一个事件，该歧义点所在上下文出现的某些信息看作这个事件发生的条件b∈B。那么，建立最大熵模型的目的就是计算判定结果a的条件概率p（a|b），即利用条件最大熵模型选择条件概率p（a|b）最大的候选结果作为最终的判定结果：<br>$$<br>\hat{p}(a | b)=\underset{p \in P}{\operatorname{argmax}} H(p)<br>$$<br>其中，P是指所建模型中所有与已知样本中的概率分布相吻合的概率分布的集合。由于<br>$$<br>\begin{aligned} H(p) &amp;=H(A | B) \\<br>&amp;=\sum_{b \in B} p(b) H(A | B=b) \\<br>&amp;=-\sum_{a, b} p(b) p(a | b) \log p(a | b) \end{aligned}<br>$$<br>而所建立模型的概率分布p（b）必须符合已知训练样本中的概率分布$\hat{p}$（b），即$\hat{p}$（b）＝p（b），因此，可将式（6-30）写为：<br>$$<br>H(p)=-\sum_{a, b} \hat{p}(b) p(a | b) \log p(a | b)<br>$$<br>那么，式（6-29）可以写为：<br>$$<br>\begin{aligned} \hat{p}(a | b) &amp;=\underset{p \in P}{\operatorname{argmax}} H(p) \\<br>&amp;=\underset{p \in P}{\operatorname{argmax}}\left(-\sum_{a, b} \hat{p}(b) p(a | b) \log p(a | b)\right) \end{aligned}<br>(6-32)<br>$$<br>实际上，式（6-32）就是我们求最大值的目标函数。接下来的问题就是如何确定满足条件的概率分布集合P。我们知道，在训练样本中上下文信息与歧义点实际结果的经验分布$\hat{p}$（a, b）可由下面的公式估计：<br>$$<br>\hat{p}(a, b) \approx \frac{\operatorname{Count}(a, b)}{\sum_{A \cdot B} \text { Count }(a, b)}<br>$$<br>其中，Count（a，b）为（a，b）在训练样本中出现的次数。<br>如果存在某个特征$f_i$（a，b），它在训练样本中关于经验概率分布$\hat{p}$（a，b）的数学期望为：<br>$$<br>E_{\hat{p}}\left(f_{i}\right)=\sum_{A, B} \hat{p}(a, b) f_{i}(a, b) \qquad (6-34)<br>$$<br>而特征$f_i$（a，b）关于所建立的理论模型p（a，b）的数学期望为：<br>$$<br>E_{p}\left(f_{i}\right)=\sum_{A ,B} p(a, b) f_{i}(a, b)<br>$$<br>由于p（a，b）＝p（a）p（b|a），且理论上所建立的模型应该与训练样本中的概率分布一致，如果用$\hat{p}$（a）表示a在训练样本中的概率分布，那么，p（a）＝$\hat{p}$（a），可将式（6-35）写为：<br>$$<br>E_{p}\left(f_{i}\right)=\sum_{A , B} \hat{p}(a) p(b | a) f_{i}(a, b)<br>$$<br>如果特征$f_i$对于模型是有用的，那么，式（6-35）所表示的数学期望与$f_i$在训练样本中的数学期望应该相同，即<br>$$<br>E_{p}\left(f_{i}\right)=E_{\hat{p}}\left(f_{i}\right)<br>\quad (6-37)<br>$$<br>这一约束条件实际上就是“所建立模型的概率分布应该与已知样本中的概率分布相吻合”的数学表达。<br>假设存在k个特征fi （i＝1,2，…，k），它们都在建模过程中对输出有影响，那么，所建立的模型p应该属于这k个特征约束下所产生的所有模型的集合P：<br>$$<br>P=\left\{p | E_{p}\left(f_{i}\right)=E_{\hat{p}}\left(f_{i}\right), i \in\{1,2, \cdots, k\}\right\}<br>\quad (6-38)$$<br>这样，问题就变成了在满足式（6-37）和式（6-38）表示的约束条件下求解目标函数（6-32）的最优解。拉格朗日乘子法可用于解决这一问题。可以证明，满足上述条件的最优解具有如下形式：<br>$$<br>\hat{p}(a | b)=\frac{1}{Z(b)} \exp \left(\sum_{i=1}^{l} \lambda_{i} \cdot f_{i}(a, b)\right)<br>\quad (6-39)<br>$$<br>其中，<br>$$<br>Z(b)=\sum_{A} \exp \left(\sum_{i=1}^{l} \lambda_{i} \cdot f_{i}(a, b)\right)<br>\quad (6-40)<br>$$<br>为归一化因子，使$\sum_{a} \hat{p}(a | b)=1$。l＝k＋1（见下面6.7.2节的说明），$\lambda_i$为特征$f_i$的权重。</p>
<h2 id="最大熵模型的参数训练"><a href="#最大熵模型的参数训练" class="headerlink" title="最大熵模型的参数训练"></a>最大熵模型的参数训练</h2><p>最大熵模型参数训练的任务就是选取有效的特征$f_i$及其权重$\lambda_i$。由于可以利用歧义点所在的上下文信息（如词形、词性、窗口大小等）作为特征条件，而歧义候选往往有多个，因此，各种特征条件和歧义候选可以组合出很多特征函数，必须对其进行筛选。常用的筛选方法有：①从候选特征集中选择那些在训练数据中出现频次超过一定阈值的特征；②利用互信息作为评价尺度从候选特征集中选择满足一定互信息要求的特征；③利用增量式特征选择方法［Pietra et al., 1997］从候选特征集中选择特征。第三种方法比较复杂，一般不用。<br>对于参数λ，常用的获取方法是通用迭代算法（generalized iterative scaling, GIS）。GIS算法要求对训练样本集中每个实例的任意（a, b）∈A×B，特征函数之和为常数，即对每个实例的k个特征函数均满足$\sum_{i=1}^{k} f_{i}(a, b)$＝C（C为一常数）。如果该条件不能满足，则在训练集中取：<br>$$C=\max_{a \in A, b \in B} \sum_{i=1}^{k} f_{i}(a, b)$$<br>并增加一个特征$f_l：f_l$（a, b）＝C－$\sum_{i=1}^{k} f_{i}(a, b)$。其中，l＝k＋1。与其他特征函数不一样，$f_l$（a, b）的取值范围为：0～C。<br>GIS算法的描述如下：<br>（1）初始化：λ［1..l］＝0；<br>（2）根据公式（6-34）计算每个特征函数$f_i$的训练样本期望值：$E_{\hat{p}}（f_i）$；<br>（3）执行如下循环，迭代计算特征函数的模型期望值$E_p（f_i）$：<br>①利用公式（6-40）和公式（6-39）计算概率$\hat{p}$（a|b）；<br>②若满足终止条件，则结束迭代；否则，修正λ：<br>$\lambda^{(\mathrm{n}+1)}=\lambda^{(\mathrm{n})}+\frac{1}{C} \ln \left(\frac{E_{\hat{p}}\left(f_{i}\right)}{E_{p^{(n)}}\left(f_{i}\right)}\right)$（其中，n为循环迭代的次数。）<br>继续下轮迭代。<br>（4）算法结束，确定λ，算出每个$\hat{p}$（a|b）。<br>迭代终止的条件可以为限定的迭代次数，也可以是对数似然（L（p））的变化值小于某个阈值ε：<br>$$<br>\begin{aligned} &amp;\left|L_{n+1}-L_{n}\right|&lt;\varepsilon \\<br> L(p) &amp;=\sum_{a , b} \hat{p}(a, b) \log p(a | b) \end{aligned}<br>$$<br>$\hat{p}$（a, b）为（a, b）在训练样本中出现的概率。</p>
<p>由于λ的收敛速度受C取值的影响，因此，人们改进了GIS算法，限于篇幅这里不再详细介绍，有兴趣的读者可以参阅文献［Berger, 1997; Pietra et al., 1997; Ratnaparkhi, 1998］。</p>
<p>关于最大熵模型的实现工具，可参阅如下网站：<br>·OpenNLP（Java版）工具包：<a href="http://incubator.apache.org/opennlp/" target="_blank" rel="noopener">http://incubator.apache.org/opennlp/</a><br>·张乐实现的最大熵工具包（C++版）：<a href="http://homepages.inf.ed.ac.uk/lzhang10/maxent_toolkit.html" target="_blank" rel="noopener">http://homepages.inf.ed.ac.uk/lzhang10/maxent_toolkit.html</a><br>·林德康实现的最大熵工具包（C++版）：<a href="http://webdocs.cs.ualberta.ca/~lindek/downloads.htm" target="_blank" rel="noopener">http://webdocs.cs.ualberta.ca/~lindek/downloads.htm</a><br>·MALLET（Java版，通用的自然语言处理工具包，包括分类、序列标注等机器学习算法）：<a href="http://mallet.cs.umass.edu/" target="_blank" rel="noopener">http://mallet.cs.umass.edu/</a><br>·NLTK（Python版，通用的自然语言处理工具包，很多工具是从MALLET中包装、转成的Python接口）：<a href="http://nltk.org/" target="_blank" rel="noopener">http://nltk.org/</a></p>
<h1 id="最大熵马尔可夫模型"><a href="#最大熵马尔可夫模型" class="headerlink" title="最大熵马尔可夫模型"></a>最大熵马尔可夫模型</h1><p>最大熵马尔可夫模型（maximum-entropy Markov model, MEMM）又称条件马尔可夫模型（conditional Markov model, CMM），由Andrew McCallum, Dayne Freitag和Fernando Pereira三人于2000年提出［McCallum et al., 2000］。它结合了隐马尔可夫模型和最大熵模型的共同特点，被广泛应用于处理序列标注问题。</p>
<p>文献［McCallum et al., 2000］认为，在HMM模型中存在两个问题：①在很多序列标注任务中，尤其当不能枚举观察输出时，需要用大量的特征来刻画观察序列。如在文本中识别一个未见的公司名字时，除了传统的单词识别方法以外，还需要用到很多特征信息，如大写字母、结尾词、词性、格式、在文本中的位置等。也就是说，我们需要用特征对观察输出进行参数化。②在很多自然语言处理任务中，需要解决的问题是在已知观察序列的情况下求解状态序列，HMM采用生成式的联合概率模型（状态序列与观察序列的联合概率$P（S_T，O_T））$来求解这种条件概率问题$P（S_T|O_T）$（参见6.4节），这种方法不适合处理用很多特征描述观察序列的情况。为此，MEMM直接采用条件概率模型$P（S_T|O_T）$，从而使观察输出可以用特征表示，借助最大熵框架进行特征选取。<br>HMM与MEMM的区别可以简要地用图6-12说明。<br><img src="/images/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/HMM%E4%B8%8EMEMM%E4%BE%9D%E5%AD%98%E5%AF%B9%E7%85%A7%E5%9B%BE.png" alt="HMM与MEMM依存对照图"><br>图6-12（a）为传统HMM的依存关系图，实线箭头表示所指的结点依赖于箭头起始结点，虚线箭头表示箭头所指的结点是起始结点条件。图6-12（b）为MEMM的依存关系图。在HMM μ中解码过程求解的是$\underset{S_{T}}{\operatorname{argmax}} P\left(O_{T} | S_{T}, \mu\right)$，而在MEMM M中解码器求解的是$\underset{S_{T}}{\operatorname{argmax}} P\left(S_{T} | O_{T}, M\right)_{\circ}$。在HMM中，当前时刻的观察输出只取决于当前状态，而在MEMM中，当前时刻的观察输出还可能取决于前一时刻的状态。</p>
<p>假设已知观察序列$O_1O_2…O_T$，要求解状态序列$S_1S_2…S_T$，并使条件概率P（$S_1S_2…S_T|O_1O_2…O_T）$最大。在MEMM中，将这一概率因子化为马尔可夫转移概率，该转移概率依赖于当前时刻的观察和前一时刻的状态：<br>$$P\left(S_{1} \cdots S_{T} | O_{1} \cdots O_{T}\right)=\prod_{t=1}^{T} P\left(S_{t} | S_{t-1}, O_{t}\right)$$<br>对于前一时刻每个可能的状态取值$S_{t－1}＝s′$和当前观察输出$O_t＝o$，当前状态取值$S_t＝s$的概率通过最大熵分类器建模：<br>$$<br>P\left(s | s^{\prime}, o\right)=P_{j}(s | o)=\frac{1}{Z(o, s)} \exp \left(\sum_{a} \lambda_{a} f_{a}(o, s)\right)<br>$$<br>其中，Z（o, s′）为归一化因子，fa（o, s）为特征函数，$λ<em>a$为特征函数的权重，可以利用GIS算法从训练样本中估计出来。$f_a（o, s）$可以通过a＝〈b, r〉定义，其中，b是当前观察的{0，1}二值特征，r是状态取值。<br>$$<br>f</em>{a}\left(o_{t}, s_{t}\right)=f_{\langle b, r)}\left(o_{t}, s_{t}\right)=<br>\begin{cases}<br>1, \quad  b\left(o_{t}\right)=\text { True, } s_{t}=r \\<br>0, \text{其他}<br>\end{cases}<br>$$<br>HMM中用于参数估计的Baum-Welch算法修改后可用于MEMM的状态转移概率估计。在Viterbi算法中，如果t时刻到达状态s时产生观察序列的前向概率为$\alpha_t（s）$，那么，在MEMM中，将$\alpha_t（s）$定义为在时刻t的状态为s，给定到达t时刻为止的观察序列时的前向概率：<br>$$<br>\alpha_{t+1}(s)=\sum_{g^{\prime} \in S} \alpha_{t}\left(s^{\prime}\right) \cdot P_{s^{\prime}}\left(s | o_{t+1}\right)<br>$$<br>相应的后向概率$\beta_t（s）$为在给定t时刻之后的观察序列时，从t时刻s状态开始的概率：<br>$$<br>\beta_{t}\left(s^{\prime}\right)=\sum_{s \in S} P\left(s | s^{\prime}, o_{t}\right) \cdot \beta_{t+1}(s)<br>$$<br>关于MEMM的详细介绍，有兴趣的读者请参阅［McCallum et al., 2000］。<br>MEMM是有向图和无向图的混合模型，其主体还是有向图框架。与HMM相比，MEMM的最大优点在于它允许使用任意特征刻画观察序列，这一特性有利于针对特定任务充分利用领域知识设计特征。MEMM与HMM和条件随机场（conditional random fields, CRFs）模型（见6.9节）相比，MEMM的参数训练过程非常高效，在HMM和CRF模型的训练中，需要利用前向后向算法作为内部循环，而在MEMM中估计状态转移概率时可以逐个独立进行。MEMM的缺点在于存在标记偏置问题（label bias problem），其中一个原因是熵低的状态转移分布会忽略它们的观察输出，而另一个原因是MEMM像HMM一样，其参数训练过程是自左向右依据前面已经标注的标记进行的，一旦在实际测试时前面的标记不能确定时，MEMM往往难以处理。</p>
<h1 id="条件随机场"><a href="#条件随机场" class="headerlink" title="条件随机场"></a>条件随机场</h1><p>条件随机场（conditional random fields, CRFs）由J. Lafferty等人（2001）提出，近几年来在自然语言处理和图像处理等领域中得到了广泛的应用。</p>
<p>CRF是用来标注和划分序列结构数据的概率化结构模型。言下之意，就是对于给定的输出标识序列Y和观测序列X，条件随机场通过定义条件概率P（Y|X），而不是联合概率分布P（X，Y）来描述模型。CRF也可以看作一个无向图模型或者马尔可夫随机场（Markov random field）［Wallach, 2004］。</p>
<p>定义6-5（条件随机场）　设G＝（V，E）为一个无向图，V为结点集合，E为无向边的集合。$Y＝{Y_v|v∈V}$，即V中的每个结点对应于一个随机变量$Y_v$，其取值范围为可能的标记集合{y}。如果以观察序列X为条件，每一个随机变量Yv都满足以下马尔可夫特性：<br>$$<br>p\left(Y_{v} | X, Y_{w}, w \neq v\right)=p\left(Y_{v} | X, Y_{w}, w \sim v\right)<br>$$<br>其中，w～v表示两个结点在图G中是邻近结点。那么，（X，Y）为一个条件随机场。<br>理论上，只要在标记序列中描述了一定的条件独立性，G的图结构可以是任意的。对序列进行建模可以形成最简单、最普通的链式结构（chain-structured）图，结点对应标记序列Y中的元素（图6-13）。或者更直观一点，把CRF的链式结构图画为如图6-14所示。<br><img src="/images/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/CRF%E7%9A%84%E9%93%BE%E5%BC%8F%E7%BB%93%E6%9E%84%E5%9B%BE.png" alt="CRF的链式结构图"><br>显然，观察序列X的元素之间并不存在图结构，因为这里只是将观察序列X作为条件，并不对其作任何独立性假设。</p>
<p>在给定观察序列X时，某个特定标记序列Y的概率可以定义为［Lafferty et al., 2001］：<br>$$<br>\exp \left(\sum_{j} \lambda_{j} t_{j}\left(y_{i-1}, y_{i}, X, i\right)+\sum_{k} \mu_{k} s_{k}\left(y_{i}, X, i\right)\right)<br>$$<br>其中，$t_j（y_{i-1}，y_i，X，i）$是转移函数，表示对于观察序列X其标注序列在i及i－1位置上标记的转移概率；$s_k（y_i，X，i）$是状态函数，表示对于观察序列X其i位置的标记概率；$\lambda_j$和$\mu_k$分别是$t_j$和$s_k$的权重，需要从训练样本中估计出来。<br>参照最大熵模型的做法，在定义特征函数时可以定义一组关于观察序列的{0，1}二值特征b（X, i）来表示训练样本中某些分布特性，例如，<br>$$<br>b(X, i)=<br>\begin{cases}<br>{1} , \text{X的位置为某个特定的词}  \\<br>{0} , \text{否则}<br>\end{cases}<br>$$<br>转移函数可以定义为如下形式：<br>$$<br>t_{j}\left(y_{i-1}, y_{i}, X, i\right)=<br>\begin{cases}<br>{b(X, i)} , y_{i-1} \text{和} y_i 满足某种搭配条件   \\<br>{0} , \text{否则}  \\<br>\end{cases}<br>$$<br>为了便于描述，可以将状态函数书写成如下形式：<br>$$<br>s\left(y_{i}, X, i\right)=s\left(y_{i-1}, y_{i}, X, i\right)<br>$$<br>这样，特征函数可以统一表示为：<br>$$<br>F_{j}(Y, X)=\sum_{i=1}^{n} f_{j}\left(y_{i-1}, y_{i}, X, i\right)<br>$$<br>其中，每个局部特征函数$f_j（y_{i－1}，y_i，X，i）$表示状态特征s$（y_{i－1}，y_i，X，i）$或转移函数t$（y_{i－1}，y_i，X，i）$。<br>由此，条件随机场定义的条件概率可以由下式给出：<br>$$<br>p(Y | X, \lambda)=\frac{1}{Z(X)} \exp \left(\lambda_{j} \cdot F_{j}(Y, X)\right)<br>$$<br>其中，分母Z（X）为归一化因子：<br>$$<br>Z(X)=\sum_{Y} \exp \left(\lambda_{j} \cdot F_{j}(Y, X)\right)<br>$$<br>条件随机场模型也需要解决三个基本问题：特征的选取、参数训练和解码。其中，参数训练过程可在训练数据集上基于对数似然函数的最大化进行，具体算法请参阅文献［Lafferty et al., 2001; Wallach, 2004］。</p>
<p>相对于HMM，CRF的主要优点在于它的条件随机性，只需要考虑当前已经出现的观测状态的特性，没有独立性的严格要求，对于整个序列内部的信息和外部观测信息均可有效利用，避免了MEMM和其他针对线性序列模型的条件马尔可夫模型会出现的标识偏置问题。CRF具有MEMM的一切优点，两者的关键区别在于，MEMM使用每一个状态的指数模型来计算给定前一个状态下当前状态的条件概率，而CRF用单个指数模型来计算给定观察序列与整个标记序列的联合概率。因此，不同状态的不同特征权重可以相互交替代换［Lafferty, 2001］。</p>
<p>关于条件随机场模型的实现工具，可参阅如下网站：</p>
<p>·CRF++（C++版）：<a href="http://crfpp.googlecode.com/svn/trunk/doc/index.html" target="_blank" rel="noopener">http://crfpp.googlecode.com/svn/trunk/doc/index.html</a></p>
<p>·CRFSuite（C语言版）：<a href="http://www.chokkan.org/software/crfsuite/" target="_blank" rel="noopener">http://www.chokkan.org/software/crfsuite/</a></p>
<p>·MALLET（Java版，通用的自然语言处理工具包，包括分类、序列标注等机器学习算法）：<a href="http://mallet.cs.umass.edu/" target="_blank" rel="noopener">http://mallet.cs.umass.edu/</a></p>
<p>·NLTK（Python版，通用的自然语言处理工具包，很多工具是从MALLET中包装转成的Python接口）：<a href="http://nltk.org/" target="_blank" rel="noopener">http://nltk.org/</a></p>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p>《统计自然语言处理》（第二版）  宗成庆 著 清华大学出版社 <a href="/books/mobi/统计自然语言处理（第2版）.mobi">统计自然语言处理.mobi下载</a></p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/nlp/" rel="tag"><i class="fa fa-tag"></i> nlp</a>
          
            <a href="/tags/统计自然语言处理/" rel="tag"><i class="fa fa-tag"></i> 统计自然语言处理</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/04/11/统计自然语言处理-语言模型/" rel="next" title="统计自然语言处理-语言模型">
                <i class="fa fa-chevron-left"></i> 统计自然语言处理-语言模型
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/04/18/统计自然语言处理-自动分词/" rel="prev" title="统计自然语言处理-自动分词">
                统计自然语言处理-自动分词 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
    </div>
  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.png" alt="Arvin_Zhang">
            
              <p class="site-author-name" itemprop="name">Arvin_Zhang</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">16</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">5</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">10</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/zhangweifeng919" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:zhangweifeng919@gmail.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#概述"><span class="nav-number">1.</span> <span class="nav-text">概述</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#贝叶斯网络"><span class="nav-number">2.</span> <span class="nav-text">贝叶斯网络</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#马尔科夫模型"><span class="nav-number">3.</span> <span class="nav-text">马尔科夫模型</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#隐马尔可夫模型"><span class="nav-number">4.</span> <span class="nav-text">隐马尔可夫模型</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#求解观察序列的概率"><span class="nav-number">4.1.</span> <span class="nav-text">求解观察序列的概率</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#维比特算法"><span class="nav-number">4.2.</span> <span class="nav-text">维比特算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#HMM的参数估计"><span class="nav-number">4.3.</span> <span class="nav-text">HMM的参数估计</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#层次化的隐马尔可夫模型"><span class="nav-number">5.</span> <span class="nav-text">层次化的隐马尔可夫模型</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#马尔可夫网络"><span class="nav-number">6.</span> <span class="nav-text">马尔可夫网络</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#最大熵模型"><span class="nav-number">7.</span> <span class="nav-text">最大熵模型</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#最大熵原理"><span class="nav-number">7.1.</span> <span class="nav-text">最大熵原理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#最大熵模型的参数训练"><span class="nav-number">7.2.</span> <span class="nav-text">最大熵模型的参数训练</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#最大熵马尔可夫模型"><span class="nav-number">8.</span> <span class="nav-text">最大熵马尔可夫模型</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#条件随机场"><span class="nav-number">9.</span> <span class="nav-text">条件随机场</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#参考资料"><span class="nav-number">10.</span> <span class="nav-text">参考资料</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Arvin_Zhang</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Site words total count&#58;</span>
    
    <span title="Site words total count"></span>
  
</div>











<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
    console.log("已经推送");
})();
</script>

        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  










  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item=>{
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: false,
        appId: 'kjMy4Wir9L7O2wDtjHexeyl8-gzGzoHsz',
        appKey: 'zruMkYmFF2lxyMAreQ8AdUWr',
        placeholder: '欢迎提问，一起讨论\(^o^)/~',
        avatar:'mm',
        guest_info:guest,
        pageSize:'10' || 10,
    });
  </script>



  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("kjMy4Wir9L7O2wDtjHexeyl8-gzGzoHsz", "zruMkYmFF2lxyMAreQ8AdUWr");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
