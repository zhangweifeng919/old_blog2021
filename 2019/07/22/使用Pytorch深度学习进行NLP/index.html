<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.9.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">






  
  
  <link rel="stylesheet" media="all" href="/lib/Han/dist/han.min.css?v=3.3">




<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Pytorch,nlp,">










<meta name="description" content="对pytorch 官网上Deep learning for NLP with pytorch章节的翻译，链接https://pytorch.org/tutorials/beginner/deep_learning_nlp_tutorial.html">
<meta name="keywords" content="Pytorch,nlp">
<meta property="og:type" content="article">
<meta property="og:title" content="使用Pytorch深度学习进行NLP">
<meta property="og:url" content="https://zhangweifeng.top/2019/07/22/使用Pytorch深度学习进行NLP/index.html">
<meta property="og:site_name" content="漂泊在学海">
<meta property="og:description" content="对pytorch 官网上Deep learning for NLP with pytorch章节的翻译，链接https://pytorch.org/tutorials/beginner/deep_learning_nlp_tutorial.html">
<meta property="og:locale" content="zh-Hans">
<meta property="og:updated_time" content="2019-08-20T15:51:00.155Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="使用Pytorch深度学习进行NLP">
<meta name="twitter:description" content="对pytorch 官网上Deep learning for NLP with pytorch章节的翻译，链接https://pytorch.org/tutorials/beginner/deep_learning_nlp_tutorial.html">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://zhangweifeng.top/2019/07/22/使用Pytorch深度学习进行NLP/">





  <title>使用Pytorch深度学习进行NLP | 漂泊在学海</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">漂泊在学海</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <h1 class="site-subtitle" itemprop="description">学海无涯，回头是岸</h1>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://zhangweifeng.top/2019/07/22/使用Pytorch深度学习进行NLP/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Arvin_Zhang">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="漂泊在学海">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">使用Pytorch深度学习进行NLP</h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-07-22T11:20:57+08:00">
                2019-07-22
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2019-08-20T23:51:00+08:00">
                2019-08-20
              </time>
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/文档阅读笔记/" itemprop="url" rel="index">
                    <span itemprop="name">文档阅读笔记</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/07/22/使用Pytorch深度学习进行NLP/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/07/22/使用Pytorch深度学习进行NLP/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2019/07/22/使用Pytorch深度学习进行NLP/" class="leancloud_visitors" data-flag-title="使用Pytorch深度学习进行NLP">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

           

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  
                </span>
              

              

              
            </div>
          

          
              <div class="post-description">
                  对pytorch 官网上Deep learning for NLP with pytorch章节的翻译，链接https://pytorch.org/tutorials/beginner/deep_learning_nlp_tutorial.html
              </div>
          

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        <p>作者：Robert Guthrie</p>
<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>这个教程会教您使用pytorch进行深度学习编程的关键思想。这里有很多概念，比如图计算和自动梯度计算并不是pytorch独有的，在其他的深度学习的工具中也同样存在。<br>我写这个教程主要针对NLP问题，和教那些之前没有用过相关的深度学习框架（比如 TensorFlow, Theano, Keras, Dynet）的同学。假设我们需要解决的问题是NLP中的核心问题，比如词性标注，语言模型等，也假设我们已经在介绍人工智能的课程中熟悉了神经网络，通常这些课程包含了反向传播算法和前馈神经网络。注意到它们是线性和非线性的链组成的。假设你已经有了必要的知识储备，这个教程将会教你开始写深度学习的代码。<br>注意：教程主要关注点在于模型，不是数据，所以我在例子中会使用少量的低纬度数据，你可以看到当训练时，权重是如何改变的。如果你想测试你的真实数据，你可以复制模型然后使用它。</p>
<h2 id="pytorch-介绍"><a href="#pytorch-介绍" class="headerlink" title="pytorch 介绍"></a>pytorch 介绍</h2><h3 id="介绍torch-的tensor库"><a href="#介绍torch-的tensor库" class="headerlink" title="介绍torch 的tensor库"></a>介绍torch 的tensor库</h3><p>深度学习是在tensor上计算的。tensor是矩阵的一般化，可以被2个以上索引。我们之后会看到这个到底什么意思，现在我们可以使用tensor 做些什么。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Author: Robert Guthrie</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.autograd <span class="keyword">as</span> autograd</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">1</span>) <span class="comment"># 设置随机数生成器的种子</span></span><br></pre></td></tr></table></figure>

<h3 id="创建Tensors"><a href="#创建Tensors" class="headerlink" title="创建Tensors"></a>创建Tensors</h3><p>使用torch.Tensor()函数可以从Python数组中创建Tensors</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># torch.tensor(data) creates a torch.Tensor object with the given data.</span></span><br><span class="line">V_data = [<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>]</span><br><span class="line">V = torch.tensor(V_data)</span><br><span class="line">print(V)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Creates a matrix</span></span><br><span class="line">M_data = [[<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>], [<span class="number">4.</span>, <span class="number">5.</span>, <span class="number">6</span>]]</span><br><span class="line">M = torch.tensor(M_data)</span><br><span class="line">print(M)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a 3D tensor of size 2x2x2.</span></span><br><span class="line">T_data = [[[<span class="number">1.</span>, <span class="number">2.</span>], [<span class="number">3.</span>, <span class="number">4.</span>]],</span><br><span class="line">          [[<span class="number">5.</span>, <span class="number">6.</span>], [<span class="number">7.</span>, <span class="number">8.</span>]]]</span><br><span class="line">T = torch.tensor(T_data)</span><br><span class="line">print(T)</span><br></pre></td></tr></table></figure>

<p>Out:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tensor([<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>])</span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>],</span><br><span class="line">        [<span class="number">4.</span>, <span class="number">5.</span>, <span class="number">6.</span>]])</span><br><span class="line">tensor([[[<span class="number">1.</span>, <span class="number">2.</span>],</span><br><span class="line">         [<span class="number">3.</span>, <span class="number">4.</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">5.</span>, <span class="number">6.</span>],</span><br><span class="line">         [<span class="number">7.</span>, <span class="number">8.</span>]]])</span><br></pre></td></tr></table></figure>

<p>什么是3D tensor ? 设想一下，如果你有一个向量，索引这个向量你会得到一个标量，如果你有一个矩阵，索引这个矩阵你会得到一个向量，如果你有一个3D tensor，索引这个tensor 你会得到一个矩阵！<br>关于术语的注意：当我说tensor时，指的是torch.tensor对象，向量和矩阵是torch中的一维和二维特殊例子。当我提及3Dtensor时，我指的就是3D tensor。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Index into V and get a scalar (0 dimensional tensor)</span></span><br><span class="line">print(V[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># Get a Python number from it</span></span><br><span class="line">print(V[<span class="number">0</span>].item())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Index into M and get a vector</span></span><br><span class="line">print(M[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Index into T and get a matrix</span></span><br><span class="line">print(T[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>

<p>Out</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor(<span class="number">1.</span>)</span><br><span class="line"><span class="number">1.0</span></span><br><span class="line">tensor([<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>])</span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">2.</span>],</span><br><span class="line">        [<span class="number">3.</span>, <span class="number">4.</span>]])</span><br></pre></td></tr></table></figure>

<p>你也可以创建其他类型的tensor,默认情况下的类型是float，为了创建整数类型，你可以使用torch.LongTensor()函数，你可以参考其他文档了解更多的数据类型，但是一般情况下Float和Long 是最常使用的类型。<br>你可以使用torch.randn()创建指定维度的随机的数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn((<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>))</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure>

<p>Out:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[<span class="number">-1.5256</span>, <span class="number">-0.7502</span>, <span class="number">-0.6540</span>, <span class="number">-1.6095</span>, <span class="number">-0.1002</span>],</span><br><span class="line">         [<span class="number">-0.6092</span>, <span class="number">-0.9798</span>, <span class="number">-1.6091</span>, <span class="number">-0.7121</span>,  <span class="number">0.3037</span>],</span><br><span class="line">         [<span class="number">-0.7773</span>, <span class="number">-0.2515</span>, <span class="number">-0.2223</span>,  <span class="number">1.6871</span>,  <span class="number">0.2284</span>],</span><br><span class="line">         [ <span class="number">0.4676</span>, <span class="number">-0.6970</span>, <span class="number">-1.1608</span>,  <span class="number">0.6995</span>,  <span class="number">0.1991</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">0.8657</span>,  <span class="number">0.2444</span>, <span class="number">-0.6629</span>,  <span class="number">0.8073</span>,  <span class="number">1.1017</span>],</span><br><span class="line">         [<span class="number">-0.1759</span>, <span class="number">-2.2456</span>, <span class="number">-1.4465</span>,  <span class="number">0.0612</span>, <span class="number">-0.6177</span>],</span><br><span class="line">         [<span class="number">-0.7981</span>, <span class="number">-0.1316</span>,  <span class="number">1.8793</span>, <span class="number">-0.0721</span>,  <span class="number">0.1578</span>],</span><br><span class="line">         [<span class="number">-0.7735</span>,  <span class="number">0.1991</span>,  <span class="number">0.0457</span>,  <span class="number">0.1530</span>, <span class="number">-0.4757</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">-0.1110</span>,  <span class="number">0.2927</span>, <span class="number">-0.1578</span>, <span class="number">-0.0288</span>,  <span class="number">0.4533</span>],</span><br><span class="line">         [ <span class="number">1.1422</span>,  <span class="number">0.2486</span>, <span class="number">-1.7754</span>, <span class="number">-0.0255</span>, <span class="number">-1.0233</span>],</span><br><span class="line">         [<span class="number">-0.5962</span>, <span class="number">-1.0055</span>,  <span class="number">0.4285</span>,  <span class="number">1.4761</span>, <span class="number">-1.7869</span>],</span><br><span class="line">         [ <span class="number">1.6103</span>, <span class="number">-0.7040</span>, <span class="number">-0.1853</span>, <span class="number">-0.9962</span>, <span class="number">-0.8313</span>]]])</span><br></pre></td></tr></table></figure>

<h3 id="操作tensor"><a href="#操作tensor" class="headerlink" title="操作tensor"></a>操作tensor</h3><p>你可以使用你喜欢的方式操作tensor</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>])</span><br><span class="line">y = torch.tensor([<span class="number">4.</span>, <span class="number">5.</span>, <span class="number">6.</span>])</span><br><span class="line">z = x + y</span><br><span class="line">print(z)</span><br></pre></td></tr></table></figure>

<p>Out:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([<span class="number">5.</span>, <span class="number">7.</span>, <span class="number">9.</span>])</span><br></pre></td></tr></table></figure>

<p>你可以查看这个<a href>文档</a>获取完整的操作列表，它不仅仅只包含了数学的运算。<br>其中一个在后面会用到的操作就是连接。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># By default, it concatenates along the first axis (concatenates rows)</span></span><br><span class="line">x_1 = torch.randn(<span class="number">2</span>, <span class="number">5</span>)</span><br><span class="line">y_1 = torch.randn(<span class="number">3</span>, <span class="number">5</span>)</span><br><span class="line">z_1 = torch.cat([x_1, y_1])</span><br><span class="line">print(z_1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Concatenate columns:</span></span><br><span class="line">x_2 = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">y_2 = torch.randn(<span class="number">2</span>, <span class="number">5</span>)</span><br><span class="line"><span class="comment"># second arg specifies which axis to concat along</span></span><br><span class="line">z_2 = torch.cat([x_2, y_2], <span class="number">1</span>)</span><br><span class="line">print(z_2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># If your tensors are not compatible, torch will complain.  Uncomment to see the error</span></span><br><span class="line"><span class="comment"># torch.cat([x_1, x_2])</span></span><br></pre></td></tr></table></figure>

<p>Out</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">-0.8029</span>,  <span class="number">0.2366</span>,  <span class="number">0.2857</span>,  <span class="number">0.6898</span>, <span class="number">-0.6331</span>],</span><br><span class="line">        [ <span class="number">0.8795</span>, <span class="number">-0.6842</span>,  <span class="number">0.4533</span>,  <span class="number">0.2912</span>, <span class="number">-0.8317</span>],</span><br><span class="line">        [<span class="number">-0.5525</span>,  <span class="number">0.6355</span>, <span class="number">-0.3968</span>, <span class="number">-0.6571</span>, <span class="number">-1.6428</span>],</span><br><span class="line">        [ <span class="number">0.9803</span>, <span class="number">-0.0421</span>, <span class="number">-0.8206</span>,  <span class="number">0.3133</span>, <span class="number">-1.1352</span>],</span><br><span class="line">        [ <span class="number">0.3773</span>, <span class="number">-0.2824</span>, <span class="number">-2.5667</span>, <span class="number">-1.4303</span>,  <span class="number">0.5009</span>]])</span><br><span class="line">tensor([[ <span class="number">0.5438</span>, <span class="number">-0.4057</span>,  <span class="number">1.1341</span>, <span class="number">-0.1473</span>,  <span class="number">0.6272</span>,  <span class="number">1.0935</span>,  <span class="number">0.0939</span>,  <span class="number">1.2381</span>],</span><br><span class="line">        [<span class="number">-1.1115</span>,  <span class="number">0.3501</span>, <span class="number">-0.7703</span>, <span class="number">-1.3459</span>,  <span class="number">0.5119</span>, <span class="number">-0.6933</span>, <span class="number">-0.1668</span>, <span class="number">-0.9999</span>]])</span><br></pre></td></tr></table></figure>

<h3 id="重排tensor形状"><a href="#重排tensor形状" class="headerlink" title="重排tensor形状"></a>重排tensor形状</h3><p>使用.view()函数可以改变tensor的形状，这个函数经常使用，因为神经网络的输入格式是确定的。经常需要改变输入的形状<br>后才满足输入的格式要求</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">print(x)</span><br><span class="line">print(x.view(<span class="number">2</span>, <span class="number">12</span>))  <span class="comment"># Reshape to 2 rows, 12 columns</span></span><br><span class="line"><span class="comment"># Same as above.  If one of the dimensions is -1, its size can be inferred</span></span><br><span class="line">print(x.view(<span class="number">2</span>, <span class="number">-1</span>))</span><br></pre></td></tr></table></figure>

<p>Out:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[ <span class="number">0.4175</span>, <span class="number">-0.2127</span>, <span class="number">-0.8400</span>, <span class="number">-0.4200</span>],</span><br><span class="line">         [<span class="number">-0.6240</span>, <span class="number">-0.9773</span>,  <span class="number">0.8748</span>,  <span class="number">0.9873</span>],</span><br><span class="line">         [<span class="number">-0.0594</span>, <span class="number">-2.4919</span>,  <span class="number">0.2423</span>,  <span class="number">0.2883</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">-0.1095</span>,  <span class="number">0.3126</span>,  <span class="number">1.5038</span>,  <span class="number">0.5038</span>],</span><br><span class="line">         [ <span class="number">0.6223</span>, <span class="number">-0.4481</span>, <span class="number">-0.2856</span>,  <span class="number">0.3880</span>],</span><br><span class="line">         [<span class="number">-1.1435</span>, <span class="number">-0.6512</span>, <span class="number">-0.1032</span>,  <span class="number">0.6937</span>]]])</span><br><span class="line">tensor([[ <span class="number">0.4175</span>, <span class="number">-0.2127</span>, <span class="number">-0.8400</span>, <span class="number">-0.4200</span>, <span class="number">-0.6240</span>, <span class="number">-0.9773</span>,  <span class="number">0.8748</span>,  <span class="number">0.9873</span>,</span><br><span class="line">         <span class="number">-0.0594</span>, <span class="number">-2.4919</span>,  <span class="number">0.2423</span>,  <span class="number">0.2883</span>],</span><br><span class="line">        [<span class="number">-0.1095</span>,  <span class="number">0.3126</span>,  <span class="number">1.5038</span>,  <span class="number">0.5038</span>,  <span class="number">0.6223</span>, <span class="number">-0.4481</span>, <span class="number">-0.2856</span>,  <span class="number">0.3880</span>,</span><br><span class="line">         <span class="number">-1.1435</span>, <span class="number">-0.6512</span>, <span class="number">-0.1032</span>,  <span class="number">0.6937</span>]])</span><br><span class="line">tensor([[ <span class="number">0.4175</span>, <span class="number">-0.2127</span>, <span class="number">-0.8400</span>, <span class="number">-0.4200</span>, <span class="number">-0.6240</span>, <span class="number">-0.9773</span>,  <span class="number">0.8748</span>,  <span class="number">0.9873</span>,</span><br><span class="line">         <span class="number">-0.0594</span>, <span class="number">-2.4919</span>,  <span class="number">0.2423</span>,  <span class="number">0.2883</span>],</span><br><span class="line">        [<span class="number">-0.1095</span>,  <span class="number">0.3126</span>,  <span class="number">1.5038</span>,  <span class="number">0.5038</span>,  <span class="number">0.6223</span>, <span class="number">-0.4481</span>, <span class="number">-0.2856</span>,  <span class="number">0.3880</span>,</span><br><span class="line">         <span class="number">-1.1435</span>, <span class="number">-0.6512</span>, <span class="number">-0.1032</span>,  <span class="number">0.6937</span>]])</span><br></pre></td></tr></table></figure>

<h3 id="计算图和自动微分"><a href="#计算图和自动微分" class="headerlink" title="计算图和自动微分"></a>计算图和自动微分</h3><p>计算图的概念对于深度学习的编程是非常重要的，因为它不需要你自己编写反向传播梯度。一个计算图是如何把你的数据和输出结合起来的简易说明书。因为计算图包含了参数和相应的操作，使得有足够的信息计算导数。这可能听起来有点模糊。所以让我们看看使用了基本的标志requires_grad后会发生什么。<br>首先，从程序员的角度思考，在torch.Tensor对象中，我们保存了什么？很明显，保存了数据，形状和一些其他的东西。我们把两个tensor相加得到一个输出的tensor。这个输出的tensor知道自己的数据和形状，但是它不知道自己是两个tensor相加得到的。（它可能是从文件里读取的，也可能是其他操作得到的，等等）<br>如果把requires_grad=true，tensor将会记录下来，它自己是如何得到的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Tensor factory methods have a ``requires_grad`` flag</span></span><br><span class="line">x = torch.tensor([<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># With requires_grad=True, you can still do all the operations you previously</span></span><br><span class="line"><span class="comment"># could</span></span><br><span class="line">y = torch.tensor([<span class="number">4.</span>, <span class="number">5.</span>, <span class="number">6</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">z = x + y</span><br><span class="line">print(z)</span><br><span class="line"></span><br><span class="line"><span class="comment"># BUT z knows something extra.</span></span><br><span class="line">print(z.grad_fn)</span><br></pre></td></tr></table></figure>

<p>Out:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([<span class="number">5.</span>, <span class="number">7.</span>, <span class="number">9.</span>], grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line">&lt;AddBackward0 object at <span class="number">0x7f1b28628d68</span>&gt;</span><br></pre></td></tr></table></figure>

<p>所以Tensor就知道自己是如何得来的。不是从文件中来的，也不是乘法或者除法得来的，如果你继续跟踪z.grad_fn,你会发现你是由x和y得来的。<br>但是它是如何帮助我们计算梯度的呢？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Lets sum up all the entries in z</span></span><br><span class="line">s = z.sum()</span><br><span class="line">print(s)</span><br><span class="line">print(s.grad_fn)</span><br></pre></td></tr></table></figure>

<p>Out:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor(<span class="number">21.</span>, grad_fn=&lt;SumBackward0&gt;)</span><br><span class="line">&lt;SumBackward0 object at <span class="number">0x7f1b28628ac8</span>&gt;</span><br></pre></td></tr></table></figure>

<p>那么现在，这个总和对x的导数是什么呢？在数学上，我们想获得：<br>$$<br>\cfrac{\partial s}{\partial x}<br>$$<br>我们知道s是tensor z 的和，z是x+y得到的。那么<br>$$<br>s = \overbrace{x_0 + y_0}^\text{$z_0$} + \overbrace{x_1 + y_1}^\text{$z_1$} + \overbrace{x_2 + y_2}^\text{$z_2$}<br>$$<br>所以有了足够的信息得到我们想要的导数就是1.<br>当然，它掩盖了怎样计算导数，关键点在于它带着足够的信息，让我们可以计算导数，在现实中，开发者编码了如何计算sum和加法的梯度和反向传播算法。再深入的讨论就超出了本教程的范围。<br>让我们使用pytorch计算梯度和验证我们是正确的。（注意，如果你多次运行这段代码，梯度会增加，这是因为pytorch累加了梯度到.grad属性中，这样对于很多模型很方便）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># calling .backward() on any variable will run backprop, starting from it.</span></span><br><span class="line">s.backward()</span><br><span class="line">print(x.grad)</span><br></pre></td></tr></table></figure>

<p>Out:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>])</span><br></pre></td></tr></table></figure>

<p>理解下面的代码段对于成为一个合格的深度学习程序员是重要的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">y = torch.randn(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line"><span class="comment"># By default, user created Tensors have ``requires_grad=False``</span></span><br><span class="line">print(x.requires_grad, y.requires_grad)</span><br><span class="line">z = x + y</span><br><span class="line"><span class="comment"># So you can't backprop through z</span></span><br><span class="line">print(z.grad_fn)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ``.requires_grad_( ... )`` changes an existing Tensor's ``requires_grad``</span></span><br><span class="line"><span class="comment"># flag in-place. The input flag defaults to ``True`` if not given.</span></span><br><span class="line">x = x.requires_grad_()</span><br><span class="line">y = y.requires_grad_()</span><br><span class="line"><span class="comment"># z contains enough information to compute gradients, as we saw above</span></span><br><span class="line">z = x + y</span><br><span class="line">print(z.grad_fn)</span><br><span class="line"><span class="comment"># If any input to an operation has ``requires_grad=True``, so will the output</span></span><br><span class="line">print(z.requires_grad)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Now z has the computation history that relates itself to x and y</span></span><br><span class="line"><span class="comment"># Can we just take its values, and **detach** it from its history?</span></span><br><span class="line">new_z = z.detach()</span><br><span class="line"></span><br><span class="line"><span class="comment"># ... does new_z have information to backprop to x and y?</span></span><br><span class="line"><span class="comment"># NO!</span></span><br><span class="line">print(new_z.grad_fn)</span><br><span class="line"><span class="comment"># And how could it? ``z.detach()`` returns a tensor that shares the same storage</span></span><br><span class="line"><span class="comment"># as ``z``, but with the computation history forgotten. It doesn't know anything</span></span><br><span class="line"><span class="comment"># about how it was computed.</span></span><br><span class="line"><span class="comment"># In essence, we have broken the Tensor away from its past history</span></span><br></pre></td></tr></table></figure>

<p>Out:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="literal">False</span> <span class="literal">False</span></span><br><span class="line"><span class="literal">None</span></span><br><span class="line">&lt;AddBackward0 object at <span class="number">0x7f1b28692f60</span>&gt;</span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="literal">None</span></span><br></pre></td></tr></table></figure>

<p>你可以通过with torch.no_grad() 包裹代码段，使得.requires_grad=true的代码停止跟踪历史</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">print(x.requires_grad)</span><br><span class="line">print((x ** <span class="number">2</span>).requires_grad)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    print((x ** <span class="number">2</span>).requires_grad)</span><br></pre></td></tr></table></figure>

<p>Out:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="literal">False</span></span><br></pre></td></tr></table></figure>

<h2 id="使用pytorch进行深度学习"><a href="#使用pytorch进行深度学习" class="headerlink" title="使用pytorch进行深度学习"></a>使用pytorch进行深度学习</h2><h3 id="深度学习的基本组成：仿射映射，非线性和目标"><a href="#深度学习的基本组成：仿射映射，非线性和目标" class="headerlink" title="深度学习的基本组成：仿射映射，非线性和目标"></a>深度学习的基本组成：仿射映射，非线性和目标</h3><p>深度学习是线性和非线性组合起来的聪明方法。非线性的引入造就了很多强大的模型，在这个章节，我们将会操作这些核心的组件，构造目标函数，观察模型是如何训练的。</p>
<h3 id="仿射映射"><a href="#仿射映射" class="headerlink" title="仿射映射"></a>仿射映射</h3><p>深度学习的主力之一就是仿射映射，如下：<br>$$<br>f(x)=Ax+b<br>$$<br>A是矩阵，x和b是向量。A和b是需要学习的参数，b经常叫做偏置项。<br>Pytorch和其他大部分深度学习框架和传统的代数学有一点不同。它映射输入的是行而不是列。下面输出的第i行是输入的第i行在A下的映射加上偏置项。看下面的例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Author: Robert Guthrie</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">in</span> = nn.Linear(<span class="number">5</span>, <span class="number">3</span>)  <span class="comment"># maps from R^5 to R^3, parameters A, b</span></span><br><span class="line"><span class="comment"># data is 2x5.  A maps from 5 to 3... can we map "data" under A?</span></span><br><span class="line">data = torch.randn(<span class="number">2</span>, <span class="number">5</span>)</span><br><span class="line">print(lin(data))  <span class="comment"># yes</span></span><br></pre></td></tr></table></figure>

<p>Out:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ <span class="number">0.1755</span>, <span class="number">-0.3268</span>, <span class="number">-0.5069</span>],</span><br><span class="line">        [<span class="number">-0.6602</span>,  <span class="number">0.2260</span>,  <span class="number">0.1089</span>]], grad_fn=&lt;AddmmBackward&gt;)</span><br></pre></td></tr></table></figure>

<h3 id="非线性"><a href="#非线性" class="headerlink" title="非线性"></a>非线性</h3><p>首先注意下面的事实将会解释为什么我们需要非线性放在第一位。假设我们有两个仿射映射<br>$f(x)=Ax+b$和$g(x)=Cx+d$,那么$f(g(x))$是什么？<br>$$<br>f(g(x)) = A(Cx + d) + b = ACx + (Ad + b)<br>$$<br>AC是矩阵，$Ad+b$是向量，所以我们可以看到，把映射组合起来得到新的映射。<br>从这里我们可以知道，即使我们让神经网络成为仿射成分的长链，然而并不会给模型比仿射映射更强的功能。<br>如果我们在两个仿射映射层之间中引入非线性，情况就不会像上面那样，我们可以建造更强大的模型。<br>这里有一些核心的非线性函数，$\tanh(x), \sigma(x), \text{ReLU}(x)$很常用。你可能会想，为什么是这些函数？我可以想到更多其他的非线性函数。原因就是这些函数的梯度容易计算。对于学习来说，梯度的计算是非常重要的。比如：<br>$$<br>\frac{d\sigma}{dx} = \sigma(x)(1 - \sigma(x))<br>$$<br>一个提示：可能你在之前的AI课程上了解到$\sigma(x)$是一个默认的非线性函数。但是人们不常用它，因为它的梯度随着参数绝对值的增长消失的特别快，梯度很小意味着很难学习。所以大部分人使用tanh或者ReLU.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># In pytorch, most non-linearities are in torch.functional (we have it imported as F)</span></span><br><span class="line"><span class="comment"># Note that non-linearites typically don't have parameters like affine maps do.</span></span><br><span class="line"><span class="comment"># That is, they don't have weights that are updated during training.</span></span><br><span class="line">data = torch.randn(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">print(data)</span><br><span class="line">print(F.relu(data))</span><br></pre></td></tr></table></figure>

<p>Out:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">-0.5404</span>, <span class="number">-2.2102</span>],</span><br><span class="line">        [ <span class="number">2.1130</span>, <span class="number">-0.0040</span>]])</span><br><span class="line">tensor([[<span class="number">0.0000</span>, <span class="number">0.0000</span>],</span><br><span class="line">        [<span class="number">2.1130</span>, <span class="number">0.0000</span>]])</span><br></pre></td></tr></table></figure>

<h3 id="Softmax-和概率"><a href="#Softmax-和概率" class="headerlink" title="Softmax 和概率"></a>Softmax 和概率</h3><p>Softmax也是一个非线性函数，但是它经常用在网络的最后一个操作，这是因为它输入一个实数向量，然后返回概率分布。它的定义如下，x是实数的向量，然后Softmax的第i个部分是<br>$$<br>\frac{\exp(x_i)}{\sum_j \exp(x_j)}<br>$$<br>现在应该明白它的输出是一个概率分布，每一个部分都是非负数，并且所有部分之和为1.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Softmax is also in torch.nn.functional</span></span><br><span class="line">data = torch.randn(<span class="number">5</span>)</span><br><span class="line">print(data)</span><br><span class="line">print(F.softmax(data, dim=<span class="number">0</span>))</span><br><span class="line">print(F.softmax(data, dim=<span class="number">0</span>).sum())  <span class="comment"># Sums to 1 because it is a distribution!</span></span><br><span class="line">print(F.log_softmax(data, dim=<span class="number">0</span>))  <span class="comment"># theres also log_softmax</span></span><br></pre></td></tr></table></figure>

<p>Out:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([ <span class="number">1.3800</span>, <span class="number">-1.3505</span>,  <span class="number">0.3455</span>,  <span class="number">0.5046</span>,  <span class="number">1.8213</span>])</span><br><span class="line">tensor([<span class="number">0.2948</span>, <span class="number">0.0192</span>, <span class="number">0.1048</span>, <span class="number">0.1228</span>, <span class="number">0.4584</span>])</span><br><span class="line">tensor(<span class="number">1.</span>)</span><br><span class="line">tensor([<span class="number">-1.2214</span>, <span class="number">-3.9519</span>, <span class="number">-2.2560</span>, <span class="number">-2.0969</span>, <span class="number">-0.7801</span>])</span><br></pre></td></tr></table></figure>

<h3 id="目标函数"><a href="#目标函数" class="headerlink" title="目标函数"></a>目标函数</h3><p>目标函数就是你的网络被训练达到最小的函数，也被叫做损失函数。你选择的训练数据通过网络计算，最终计算出输出的损失。网络中的参数通过损失函数的导数进行更新。直观来讲，当模型非常确信自己的结果，但是结果错误时，损失会很大，当模型确信自己的结果并且结果正确时，损失会很小。<br>最小化损失函数的思想，就是模型希望在未知的测试，开发，生产数据上具有较小的损失。<br>一个例子就是负对数似然函数损失，这个是在多分类中常用的损失函数，对于监督多分类模型，这意味着训练这个网络最小化正确输出的负对数概率。或者说最大化正确输出的正对数概率。</p>
<h3 id="优化和训练"><a href="#优化和训练" class="headerlink" title="优化和训练"></a>优化和训练</h3><p>那么我们可以为一个实例计算损失函数了，但是怎么计算呢？在前面我们知道了如何为tensor计算梯度，现在损失函数也是一个tensor,我们可以为我们所有使用过的的参数计算梯度，然后我们可以执行标准的梯度更新。假设$\theta$是我们的参数，$L(\theta)$是损失函数，$\eta$是一个非负的学习速率：<br>$$<br>\theta^{(t+1)} = \theta^{(t)} - \eta \nabla_\theta L(\theta)<br>$$<br>许多算法和研究热门，在尝试做更多的事情而不是仅仅更新梯度。有许多人在训练时尝试改变学习速率。你不必担心这些算法在做什么，除非你真的对此感兴趣。Torch提供了很多优化算法在torch.optim包里，它们都是透明的。使用最简单的梯度更新和更复杂的梯度更新是一样的。尝试不同的更新算法和不同参数的更新算法在提高模型的的表现是重要的。常常使用优化算法例如Adam 或者 RMSProp代替SGD会显著提升性能。</p>
<h3 id="使用pytorch创建网络组件"><a href="#使用pytorch创建网络组件" class="headerlink" title="使用pytorch创建网络组件"></a>使用pytorch创建网络组件</h3><p>在我们把注意力转移到NLP之前，我们先做一个有注解的例子，只使用仿射映射和非线性函数。我们也会看到使用负对数似然函数如何计算损失函数，如何通过反向传播更新参数。<br>所有的网络组件应该继承nn.Module,并且覆盖forward()函数。从nn.Module的继承为组件提供了功能。比如，它记录了自己可以训练的参数。你可以使用.to(device)在cpu和gpu之间交换.device 可以是cpu设备torch.device(“cpu”)，或者CUDA设备torch.device(“cuda:0”).<br>让我们写一个注解的例子，<br>让我们编写一个带注释的网络示例，该网络采用稀疏的词袋表示，并在两个标签上输出概率分布：“English”和“Spanish”。这个模型只是逻辑回归。</p>
<h4 id="例子：逻辑回归词袋分类器"><a href="#例子：逻辑回归词袋分类器" class="headerlink" title="例子：逻辑回归词袋分类器"></a>例子：逻辑回归词袋分类器</h4><p>本分类器使用稀疏的BOW(bag of word)表示方法,输出每一个标签的对数概率。我们为词库中的每个单词指定一个索引。比如：我们词库中只有“hello”和“world”两个单词，用索引0和1表示。“hello hello hello hello”的BOW的向量为[4,0],”hello world world hello”的向量为[2,3].总之也就是对于一个句子，其向量是[count(“hello”),count(“world”)]<br>假设BOW的向量是x,那么我们网络的输出是<br>$$<br>\log \text{Softmax}(Ax + b)<br>$$<br>这样，我们把输入通过一个仿射映射，然后做log Softmax.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line">data = [(<span class="string">"me gusta comer en la cafeteria"</span>.split(), <span class="string">"SPANISH"</span>),</span><br><span class="line">        (<span class="string">"Give it to me"</span>.split(), <span class="string">"ENGLISH"</span>),</span><br><span class="line">        (<span class="string">"No creo que sea una buena idea"</span>.split(), <span class="string">"SPANISH"</span>),</span><br><span class="line">        (<span class="string">"No it is not a good idea to get lost at sea"</span>.split(), <span class="string">"ENGLISH"</span>)]</span><br><span class="line"></span><br><span class="line">test_data = [(<span class="string">"Yo creo que si"</span>.split(), <span class="string">"SPANISH"</span>),</span><br><span class="line">             (<span class="string">"it is lost on me"</span>.split(), <span class="string">"ENGLISH"</span>)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># word_to_ix maps each word in the vocab to a unique integer, which will be its</span></span><br><span class="line"><span class="comment"># index into the Bag of words vector</span></span><br><span class="line">word_to_ix = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> sent, _ <span class="keyword">in</span> data + test_data:</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> sent:</span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> word_to_ix:</span><br><span class="line">            word_to_ix[word] = len(word_to_ix)</span><br><span class="line">print(word_to_ix)</span><br><span class="line"></span><br><span class="line">VOCAB_SIZE = len(word_to_ix)</span><br><span class="line">NUM_LABELS = <span class="number">2</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BoWClassifier</span><span class="params">(nn.Module)</span>:</span>  <span class="comment"># inheriting from nn.Module!</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_labels, vocab_size)</span>:</span></span><br><span class="line">        <span class="comment"># calls the init function of nn.Module.  Dont get confused by syntax,</span></span><br><span class="line">        <span class="comment"># just always do it in an nn.Module</span></span><br><span class="line">        super(BoWClassifier, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Define the parameters that you will need.  In this case, we need A and b,</span></span><br><span class="line">        <span class="comment"># the parameters of the affine mapping.</span></span><br><span class="line">        <span class="comment"># Torch defines nn.Linear(), which provides the affine map.</span></span><br><span class="line">        <span class="comment"># Make sure you understand why the input dimension is vocab_size</span></span><br><span class="line">        <span class="comment"># and the output is num_labels!</span></span><br><span class="line">        self.linear = nn.Linear(vocab_size, num_labels)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># NOTE! The non-linearity log softmax does not have parameters! So we don't need</span></span><br><span class="line">        <span class="comment"># to worry about that here</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, bow_vec)</span>:</span></span><br><span class="line">        <span class="comment"># Pass the input through the linear layer,</span></span><br><span class="line">        <span class="comment"># then pass that through log_softmax.</span></span><br><span class="line">        <span class="comment"># Many non-linearities and other functions are in torch.nn.functional</span></span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(self.linear(bow_vec), dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_bow_vector</span><span class="params">(sentence, word_to_ix)</span>:</span></span><br><span class="line">    vec = torch.zeros(len(word_to_ix))</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> sentence:</span><br><span class="line">        vec[word_to_ix[word]] += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> vec.view(<span class="number">1</span>, <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_target</span><span class="params">(label, label_to_ix)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> torch.LongTensor([label_to_ix[label]])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = BoWClassifier(NUM_LABELS, VOCAB_SIZE)</span><br><span class="line"></span><br><span class="line"><span class="comment"># the model knows its parameters.  The first output below is A, the second is b.</span></span><br><span class="line"><span class="comment"># Whenever you assign a component to a class variable in the __init__ function</span></span><br><span class="line"><span class="comment"># of a module, which was done with the line</span></span><br><span class="line"><span class="comment"># self.linear = nn.Linear(...)</span></span><br><span class="line"><span class="comment"># Then through some Python magic from the PyTorch devs, your module</span></span><br><span class="line"><span class="comment"># (in this case, BoWClassifier) will store knowledge of the nn.Linear's parameters</span></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</span><br><span class="line">    print(param)</span><br><span class="line"></span><br><span class="line"><span class="comment"># To run the model, pass in a BoW vector</span></span><br><span class="line"><span class="comment"># Here we don't need to train, so the code is wrapped in torch.no_grad()</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    sample = data[<span class="number">0</span>]</span><br><span class="line">    bow_vector = make_bow_vector(sample[<span class="number">0</span>], word_to_ix)</span><br><span class="line">    log_probs = model(bow_vector)</span><br><span class="line">    print(log_probs)</span><br></pre></td></tr></table></figure>

<p>Out:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">'me'</span>: <span class="number">0</span>, <span class="string">'gusta'</span>: <span class="number">1</span>, <span class="string">'comer'</span>: <span class="number">2</span>, <span class="string">'en'</span>: <span class="number">3</span>, <span class="string">'la'</span>: <span class="number">4</span>, <span class="string">'cafeteria'</span>: <span class="number">5</span>, <span class="string">'Give'</span>: <span class="number">6</span>, <span class="string">'it'</span>: <span class="number">7</span>, <span class="string">'to'</span>: <span class="number">8</span>, <span class="string">'No'</span>: <span class="number">9</span>, <span class="string">'creo'</span>: <span class="number">10</span>, <span class="string">'que'</span>: <span class="number">11</span>, <span class="string">'sea'</span>: <span class="number">12</span>, <span class="string">'una'</span>: <span class="number">13</span>, <span class="string">'buena'</span>: <span class="number">14</span>, <span class="string">'idea'</span>: <span class="number">15</span>, <span class="string">'is'</span>: <span class="number">16</span>, <span class="string">'not'</span>: <span class="number">17</span>, <span class="string">'a'</span>: <span class="number">18</span>, <span class="string">'good'</span>: <span class="number">19</span>, <span class="string">'get'</span>: <span class="number">20</span>, <span class="string">'lost'</span>: <span class="number">21</span>, <span class="string">'at'</span>: <span class="number">22</span>, <span class="string">'Yo'</span>: <span class="number">23</span>, <span class="string">'si'</span>: <span class="number">24</span>, <span class="string">'on'</span>: <span class="number">25</span>&#125;</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([[ <span class="number">0.1194</span>,  <span class="number">0.0609</span>, <span class="number">-0.1268</span>,  <span class="number">0.1274</span>,  <span class="number">0.1191</span>,  <span class="number">0.1739</span>, <span class="number">-0.1099</span>, <span class="number">-0.0323</span>,</span><br><span class="line">         <span class="number">-0.0038</span>,  <span class="number">0.0286</span>, <span class="number">-0.1488</span>, <span class="number">-0.1392</span>,  <span class="number">0.1067</span>, <span class="number">-0.0460</span>,  <span class="number">0.0958</span>,  <span class="number">0.0112</span>,</span><br><span class="line">          <span class="number">0.0644</span>,  <span class="number">0.0431</span>,  <span class="number">0.0713</span>,  <span class="number">0.0972</span>, <span class="number">-0.1816</span>,  <span class="number">0.0987</span>, <span class="number">-0.1379</span>, <span class="number">-0.1480</span>,</span><br><span class="line">          <span class="number">0.0119</span>, <span class="number">-0.0334</span>],</span><br><span class="line">        [ <span class="number">0.1152</span>, <span class="number">-0.1136</span>, <span class="number">-0.1743</span>,  <span class="number">0.1427</span>, <span class="number">-0.0291</span>,  <span class="number">0.1103</span>,  <span class="number">0.0630</span>, <span class="number">-0.1471</span>,</span><br><span class="line">          <span class="number">0.0394</span>,  <span class="number">0.0471</span>, <span class="number">-0.1313</span>, <span class="number">-0.0931</span>,  <span class="number">0.0669</span>,  <span class="number">0.0351</span>, <span class="number">-0.0834</span>, <span class="number">-0.0594</span>,</span><br><span class="line">          <span class="number">0.1796</span>, <span class="number">-0.0363</span>,  <span class="number">0.1106</span>,  <span class="number">0.0849</span>, <span class="number">-0.1268</span>, <span class="number">-0.1668</span>,  <span class="number">0.1882</span>,  <span class="number">0.0102</span>,</span><br><span class="line">          <span class="number">0.1344</span>,  <span class="number">0.0406</span>]], requires_grad=<span class="literal">True</span>)</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([<span class="number">0.0631</span>, <span class="number">0.1465</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">tensor([[<span class="number">-0.5378</span>, <span class="number">-0.8771</span>]])</span><br></pre></td></tr></table></figure>

<p>上面的对数概率哪一个是和“ENGLISH”对应，哪一个和“SPANISH”对应？我们没有定义它，但是如果我们需要训练就需要定义。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">label_to_ix = &#123;<span class="string">"SPANISH"</span>: <span class="number">0</span>, <span class="string">"ENGLISH"</span>: <span class="number">1</span>&#125;</span><br></pre></td></tr></table></figure>

<p>所以让我们训练吧，我们把实例传进去，然后获取到对数概率，计算损失函数，计算损失函数的梯度，使用梯度步长更新参数。在nn包里提供了损失函数<br>nn.NLLLoss() 是我们需要的负对数似然函数。在torch.optim 中也定义了优化函数。在这里，我们仅仅使用SGD进行优化。<br>注意：NLLLoss的输入是对数概率的向量和目标标签。它并不会为我们计算对数概率。这就是为什么我们网络的最后一层是log Softmax。<br>nn.CrossEntropyLoss()和nn.NLLLoss是一样的，只是前者会为你做log Softmax.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Run on test data before we train, just to see a before-and-after</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> instance, label <span class="keyword">in</span> test_data:</span><br><span class="line">        bow_vec = make_bow_vector(instance, word_to_ix)</span><br><span class="line">        log_probs = model(bow_vec)</span><br><span class="line">        print(log_probs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print the matrix column corresponding to "creo"</span></span><br><span class="line">print(next(model.parameters())[:, word_to_ix[<span class="string">"creo"</span>]])</span><br><span class="line"></span><br><span class="line">loss_function = nn.NLLLoss()</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Usually you want to pass over the training data several times.</span></span><br><span class="line"><span class="comment"># 100 is much bigger than on a real data set, but real datasets have more than</span></span><br><span class="line"><span class="comment"># two instances.  Usually, somewhere between 5 and 30 epochs is reasonable.</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">    <span class="keyword">for</span> instance, label <span class="keyword">in</span> data:</span><br><span class="line">        <span class="comment"># Step 1. Remember that PyTorch accumulates gradients.</span></span><br><span class="line">        <span class="comment"># We need to clear them out before each instance</span></span><br><span class="line">        model.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 2. Make our BOW vector and also we must wrap the target in a</span></span><br><span class="line">        <span class="comment"># Tensor as an integer. For example, if the target is SPANISH, then</span></span><br><span class="line">        <span class="comment"># we wrap the integer 0. The loss function then knows that the 0th</span></span><br><span class="line">        <span class="comment"># element of the log probabilities is the log probability</span></span><br><span class="line">        <span class="comment"># corresponding to SPANISH</span></span><br><span class="line">        bow_vec = make_bow_vector(instance, word_to_ix)</span><br><span class="line">        target = make_target(label, label_to_ix)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 3. Run our forward pass.</span></span><br><span class="line">        log_probs = model(bow_vec)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 4. Compute the loss, gradients, and update the parameters by</span></span><br><span class="line">        <span class="comment"># calling optimizer.step()</span></span><br><span class="line">        loss = loss_function(log_probs, target)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> instance, label <span class="keyword">in</span> test_data:</span><br><span class="line">        bow_vec = make_bow_vector(instance, word_to_ix)</span><br><span class="line">        log_probs = model(bow_vec)</span><br><span class="line">        print(log_probs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Index corresponding to Spanish goes up, English goes down!</span></span><br><span class="line">print(next(model.parameters())[:, word_to_ix[<span class="string">"creo"</span>]])</span><br></pre></td></tr></table></figure>

<p>Out:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">-0.9297</span>, <span class="number">-0.5020</span>]])</span><br><span class="line">tensor([[<span class="number">-0.6388</span>, <span class="number">-0.7506</span>]])</span><br><span class="line">tensor([<span class="number">-0.1488</span>, <span class="number">-0.1313</span>], grad_fn=&lt;SelectBackward&gt;)</span><br><span class="line">tensor([[<span class="number">-0.2093</span>, <span class="number">-1.6669</span>]])</span><br><span class="line">tensor([[<span class="number">-2.5330</span>, <span class="number">-0.0828</span>]])</span><br><span class="line">tensor([ <span class="number">0.2803</span>, <span class="number">-0.5605</span>], grad_fn=&lt;SelectBackward&gt;)</span><br></pre></td></tr></table></figure>

<p>我们得到了答案，你可以看到在第一个例子中，Spanish的对数概率更高，第二个例子中，English的对数概率更高。<br>在，您将了解如何创建PyTorch组件，通过它传递一些数据并执行梯度更新。我们准备深入挖掘NLP的深层内容。</p>
<h2 id="词嵌入：词汇语义编码"><a href="#词嵌入：词汇语义编码" class="headerlink" title="词嵌入：词汇语义编码"></a>词嵌入：词汇语义编码</h2><p>词嵌入是一个实数的稠密向量，在你的词库中，一个词是一个向量。在NLP,大多数情况下你的特征是一个词，但是在计算机中如何表示一个词呢？<br>你可以存储ascii字符来表示，但是这只是告诉了这个词是什么，但是不能包含它是什么意思。甚至，你能从这些表示中得到什么信息。我们经常需要从神经网络得到稠密的输出，神经网络的输入时|V|维的，V是我们的词汇表。但是经常输出低维，那么我们怎么从高维空间得到低维空间呢？<br>那么我们使用One-hot 编码代替ascii 编码怎么样？我们可以把w表示为：<br>$$<br>\overbrace{\left[ 0, 0, \dots, 1, \dots, 0, 0 \right]}^\text{|V| elements}<br>$$<br>其中1的位置是唯一的，其他位置都是0.每个单词的1的位置都是不同的。<br>这样的话，会有一个很大的缺点，除了这个向量很大之外，每个向量都是独立的，没有联系的。我们真正想要的是单词之间一些相似的概念。为什么？让我们看个例子。<br>假设我们在建造一个语言模型，假设我们在训练数据中已经看到了下面的句子：<br>    - The mathematician ran to the store.<br>    - The physicist ran to the store.<br>    - The mathematician solved the open problem.<br>现在假设我们看到了一个之前没有见过的新句子：<br>    - The physicist solved the open problem.<br>我们的语言模型可能在这句话上做得很好，但如果我们可以使用以下两个事实就不会好得多：<br>    - 我们可以看到mathematician和physicist 在句子中具有相同的角色，他们之间有一些语义关系。<br>    - 正如我们正在看的physicist,mathematician在没有看到的句子中具有相同的角色。<br>然后我们可以推断，physicist很适合没有看到的句子。这就是我们所说的相似概念：我们所说的语义相似性，不是指具有相似的正交表示，而是通过连接我们看到的和未看到的点，对抗语言数据稀疏性的技术。<br>这节课程的例子依赖于一个基本的语言假设：如果词语出现在相似的上下文中，那么他们在语义上相互联系。这个叫做分布式假设 （distributional hypothesis）.</p>
<h3 id="获取稠密词嵌入"><a href="#获取稠密词嵌入" class="headerlink" title="获取稠密词嵌入"></a>获取稠密词嵌入</h3><p>我们怎么解决这个问题？我们如何编码词语的语义相似度？可能我们会想起一些语义的属性。比如，我们看到数学家和物理学家可以跑步，所以我们可以给这些单词的“可以跑步”这个属性给予一个高分。想出其他的属性，然后想象你会给这些常用的单词的这些属性打多少分。<br>如果每个属性是一个维度，那么我们可以给每个单词像这样的向量表示：<br>$$<br>q_\text{mathematician} = \left[ \overbrace{2.3}^\text{can run},<br>\overbrace{9.4}^\text{likes coffee}, \overbrace{-5.5}^\text{majored in Physics}, \dots \right]<br>$$<br>$$<br>q_\text{physicist} = \left[ \overbrace{2.5}^\text{can run},<br>\overbrace{9.1}^\text{likes coffee}, \overbrace{6.4}^\text{majored in Physics}, \dots \right]<br>$$<br>那么我们就可以得到两个词的相似程度了<br>$$<br>\text{Similarity}(\text{physicist}, \text{mathematician}) = q_\text{physicist} \cdot q_\text{mathematician}<br>$$<br>更常用的是把它归一化：<br>$$<br>\text{Similarity}(\text{physicist}, \text{mathematician}) = \frac{q_\text{physicist} \cdot q_\text{mathematician}}<br>{| q_\text{physicist} | | q_\text{mathematician} |} = \cos (\phi)<br>$$<br>其中$\phi$是两个向量的角度，这样的话，如果两个向量很相近，那么相似度接近1，如果两个向量很不相似，那么相似度接近-1。<br>你可以回想起一开始我们讲的one-hot表示的方法，它可以作为我们这个表示的一种特例：每个单词之间的相似度都是0.每个单词都具有自己的唯一的语义属性。我们现在这个新向量就是稠密向量，里面的实体都不是0.<br>但是，这个向量有个巨大的痛点：你可以想起成千上万个不同的可能相关的语义属性，然后你怎么设置这些不同属性的值呢？深度学习的中心就是神经网络学习特征的表示，而不是要求程序员自己设计他们。所以为什么不让词嵌入成为模型的参数，然后训练的时候更新他们呢？这就是我们将要去做的。我们将会有一些网络可以学习的潜在语义属性。注意一下，词嵌入可能是不是可以解释的。在上面的手稿中，数学家和物理学家都喜欢咖啡，如果我们让神经网络去学习词嵌入，然后看到数学家和物理学家在一些维度有比较大的值，但是我们不知道这个维度意味着什么。他们在潜在的语义维度具有相似性，但是对于我们是不可解释的。<br>总之，词嵌入是对词语的语义表示，有效地编码手头上任务相关的语义信息。你也可以嵌入其他的东西：词性标签，解析树，其他东西。特征嵌入的思想是这个领域的核心。</p>
<h3 id="Pytorch中的词嵌入"><a href="#Pytorch中的词嵌入" class="headerlink" title="Pytorch中的词嵌入"></a>Pytorch中的词嵌入</h3><p>在我们开始例子和练习之前，关于如何在Pytorch和深度学习编程中使用嵌入进行一些快速说明。和one-hot中定义唯一的索引很像，当我们使用词嵌入时也需要为每一个词定义一个索引。这些将会是查找表的key,这样，嵌入会保存在|V|xD的矩阵中，D是嵌入的维度，所以索引为i的词，它的嵌入保存在矩阵的第i行。在我的代码中，单词到索引的映射保存在 word_to_idx中。<br>torch.nn.embedding 模块允许你使用嵌入，它需要两个参数，一个是词汇大小，一个是嵌入的维度。<br>为了可以索引表中数据，你一定要用torch.LongTensor.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Author: Robert Guthrie</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">word_to_ix = &#123;<span class="string">"hello"</span>: <span class="number">0</span>, <span class="string">"world"</span>: <span class="number">1</span>&#125;</span><br><span class="line">embeds = nn.Embedding(<span class="number">2</span>, <span class="number">5</span>)  <span class="comment"># 2 words in vocab, 5 dimensional embeddings</span></span><br><span class="line">lookup_tensor = torch.tensor([word_to_ix[<span class="string">"hello"</span>]], dtype=torch.long)</span><br><span class="line">hello_embed = embeds(lookup_tensor)</span><br><span class="line">print(hello_embed)</span><br></pre></td></tr></table></figure>

<p>Out:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ <span class="number">0.6614</span>,  <span class="number">0.2669</span>,  <span class="number">0.0617</span>,  <span class="number">0.6213</span>, <span class="number">-0.4519</span>]],</span><br><span class="line">       grad_fn=&lt;EmbeddingBackward&gt;)</span><br></pre></td></tr></table></figure>

<h3 id="一个例子：N-gram-语言模型"><a href="#一个例子：N-gram-语言模型" class="headerlink" title="一个例子：N-gram 语言模型"></a>一个例子：N-gram 语言模型</h3><p>回想一下N-gram语言模型，给定一个包含词语w的句子，我们想要计算<br>$$<br>P(w_i | w_{i-1}, w_{i-2}, \dots, w_{i-n+1} )<br>$$<br>$w_i$是这个句子的第i个词语。<br>在这个例子中，我们将要计算在训练例子上的损失函数，然后使用反向传播更新参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line">CONTEXT_SIZE = <span class="number">2</span></span><br><span class="line">EMBEDDING_DIM = <span class="number">10</span></span><br><span class="line"><span class="comment"># We will use Shakespeare Sonnet 2</span></span><br><span class="line">test_sentence = <span class="string">"""When forty winters shall besiege thy brow,</span></span><br><span class="line"><span class="string">And dig deep trenches in thy beauty's field,</span></span><br><span class="line"><span class="string">Thy youth's proud livery so gazed on now,</span></span><br><span class="line"><span class="string">Will be a totter'd weed of small worth held:</span></span><br><span class="line"><span class="string">Then being asked, where all thy beauty lies,</span></span><br><span class="line"><span class="string">Where all the treasure of thy lusty days;</span></span><br><span class="line"><span class="string">To say, within thine own deep sunken eyes,</span></span><br><span class="line"><span class="string">Were an all-eating shame, and thriftless praise.</span></span><br><span class="line"><span class="string">How much more praise deserv'd thy beauty's use,</span></span><br><span class="line"><span class="string">If thou couldst answer 'This fair child of mine</span></span><br><span class="line"><span class="string">Shall sum my count, and make my old excuse,'</span></span><br><span class="line"><span class="string">Proving his beauty by succession thine!</span></span><br><span class="line"><span class="string">This were to be new made when thou art old,</span></span><br><span class="line"><span class="string">And see thy blood warm when thou feel'st it cold."""</span>.split()</span><br><span class="line"><span class="comment"># we should tokenize the input, but we will ignore that for now</span></span><br><span class="line"><span class="comment"># build a list of tuples.  Each tuple is ([ word_i-2, word_i-1 ], target word)</span></span><br><span class="line">trigrams = [([test_sentence[i], test_sentence[i + <span class="number">1</span>]], test_sentence[i + <span class="number">2</span>])</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(len(test_sentence) - <span class="number">2</span>)]</span><br><span class="line"><span class="comment"># print the first 3, just so you can see what they look like</span></span><br><span class="line">print(trigrams[:<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">vocab = set(test_sentence)</span><br><span class="line">word_to_ix = &#123;word: i <span class="keyword">for</span> i, word <span class="keyword">in</span> enumerate(vocab)&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NGramLanguageModeler</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embedding_dim, context_size)</span>:</span></span><br><span class="line">        super(NGramLanguageModeler, self).__init__()</span><br><span class="line">        self.embeddings = nn.Embedding(vocab_size, embedding_dim)</span><br><span class="line">        self.linear1 = nn.Linear(context_size * embedding_dim, <span class="number">128</span>)</span><br><span class="line">        self.linear2 = nn.Linear(<span class="number">128</span>, vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs)</span>:</span></span><br><span class="line">        embeds = self.embeddings(inputs).view((<span class="number">1</span>, <span class="number">-1</span>))</span><br><span class="line">        out = F.relu(self.linear1(embeds))</span><br><span class="line">        out = self.linear2(out)</span><br><span class="line">        log_probs = F.log_softmax(out, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> log_probs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">losses = []</span><br><span class="line">loss_function = nn.NLLLoss()</span><br><span class="line">model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> context, target <span class="keyword">in</span> trigrams:</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 1. Prepare the inputs to be passed to the model (i.e, turn the words</span></span><br><span class="line">        <span class="comment"># into integer indices and wrap them in tensors)</span></span><br><span class="line">        context_idxs = torch.tensor([word_to_ix[w] <span class="keyword">for</span> w <span class="keyword">in</span> context], dtype=torch.long)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 2. Recall that torch *accumulates* gradients. Before passing in a</span></span><br><span class="line">        <span class="comment"># new instance, you need to zero out the gradients from the old</span></span><br><span class="line">        <span class="comment"># instance</span></span><br><span class="line">        model.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 3. Run the forward pass, getting log probabilities over next</span></span><br><span class="line">        <span class="comment"># words</span></span><br><span class="line">        log_probs = model(context_idxs)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 4. Compute your loss function. (Again, Torch wants the target</span></span><br><span class="line">        <span class="comment"># word wrapped in a tensor)</span></span><br><span class="line">        loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 5. Do the backward pass and update the gradient</span></span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Get the Python number from a 1-element Tensor by calling tensor.item()</span></span><br><span class="line">        total_loss += loss.item()</span><br><span class="line">    losses.append(total_loss)</span><br><span class="line">print(losses)  <span class="comment"># The loss decreased every iteration over the training data!</span></span><br></pre></td></tr></table></figure>

<p>Out:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[([<span class="string">'When'</span>, <span class="string">'forty'</span>], <span class="string">'winters'</span>), ([<span class="string">'forty'</span>, <span class="string">'winters'</span>], <span class="string">'shall'</span>), ([<span class="string">'winters'</span>, <span class="string">'shall'</span>], <span class="string">'besiege'</span>)]</span><br><span class="line">[<span class="number">517.3222274780273</span>, <span class="number">514.6745493412018</span>, <span class="number">512.0429074764252</span>, <span class="number">509.4270143508911</span>, <span class="number">506.8274371623993</span>, <span class="number">504.2419238090515</span>, <span class="number">501.66898941993713</span>, <span class="number">499.10938024520874</span>, <span class="number">496.5624313354492</span>, <span class="number">494.0269808769226</span>]</span><br></pre></td></tr></table></figure>

<h3 id="练习：计算词嵌入：连续词袋（Continuous-Bag-of-Words）"><a href="#练习：计算词嵌入：连续词袋（Continuous-Bag-of-Words）" class="headerlink" title="练习：计算词嵌入：连续词袋（Continuous Bag-of-Words）"></a>练习：计算词嵌入：连续词袋（Continuous Bag-of-Words）</h3><p>连续词袋（CBOW）在深度学习NLP中经常使用。这个模型试图根据一些单词的上下文来预测单词。这个模型在语言模型中很特别，因为CBOW不是序列的，也不必是概率性的。CBOW用来快速训练词嵌入，这些嵌入用来初始化更加复杂模型的嵌入。通常，这些涉及到预训练嵌入，这个几乎总是可以提高百分之几。<br>下面就是CBOW模型，假设目标词是$w_i$,在每边有一个N的上下文窗口，$w_{i-1}, \dots, w_{i-N}$和$w_{i+1}, \dots, w_{i+N}$把上下文单词简写为C，CBOW尝试最小化：<br>$$<br>-\log p(w_i | C) = -\log \text{Softmax}(A(\sum_{w \in C} q_w) + b)<br>$$<br>其中$q_w$是单词$w$的嵌入。<br>使用pytorch实现这个模型，补全下面的类，这里有一些建议：<br>    - 思考你有哪些参数需要定义<br>    - 确定你知道每个操作需要的数据形状，如果需要改变形状，可以使用.view()函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">CONTEXT_SIZE = <span class="number">2</span>  <span class="comment"># 2 words to the left, 2 to the right</span></span><br><span class="line">raw_text = <span class="string">"""We are about to study the idea of a computational process.</span></span><br><span class="line"><span class="string">Computational processes are abstract beings that inhabit computers.</span></span><br><span class="line"><span class="string">As they evolve, processes manipulate other abstract things called data.</span></span><br><span class="line"><span class="string">The evolution of a process is directed by a pattern of rules</span></span><br><span class="line"><span class="string">called a program. People create programs to direct processes. In effect,</span></span><br><span class="line"><span class="string">we conjure the spirits of the computer with our spells."""</span>.split()</span><br><span class="line"></span><br><span class="line"><span class="comment"># By deriving a set from `raw_text`, we deduplicate the array</span></span><br><span class="line">vocab = set(raw_text)</span><br><span class="line">vocab_size = len(vocab)</span><br><span class="line"></span><br><span class="line">word_to_ix = &#123;word: i <span class="keyword">for</span> i, word <span class="keyword">in</span> enumerate(vocab)&#125;</span><br><span class="line">data = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>, len(raw_text) - <span class="number">2</span>):</span><br><span class="line">    context = [raw_text[i - <span class="number">2</span>], raw_text[i - <span class="number">1</span>],</span><br><span class="line">               raw_text[i + <span class="number">1</span>], raw_text[i + <span class="number">2</span>]]</span><br><span class="line">    target = raw_text[i]</span><br><span class="line">    data.append((context, target))</span><br><span class="line">print(data[:<span class="number">5</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CBOW</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># create your model and train.  here are some functions to help you make</span></span><br><span class="line"><span class="comment"># the data ready for use by your module</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_context_vector</span><span class="params">(context, word_to_ix)</span>:</span></span><br><span class="line">    idxs = [word_to_ix[w] <span class="keyword">for</span> w <span class="keyword">in</span> context]</span><br><span class="line">    <span class="keyword">return</span> torch.tensor(idxs, dtype=torch.long)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">make_context_vector(data[<span class="number">0</span>][<span class="number">0</span>], word_to_ix)  <span class="comment"># example</span></span><br></pre></td></tr></table></figure>

<p>Out:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[([<span class="string">'We'</span>, <span class="string">'are'</span>, <span class="string">'to'</span>, <span class="string">'study'</span>], <span class="string">'about'</span>), ([<span class="string">'are'</span>, <span class="string">'about'</span>, <span class="string">'study'</span>, <span class="string">'the'</span>], <span class="string">'to'</span>), ([<span class="string">'about'</span>, <span class="string">'to'</span>, <span class="string">'the'</span>, <span class="string">'idea'</span>], <span class="string">'study'</span>), ([<span class="string">'to'</span>, <span class="string">'study'</span>, <span class="string">'idea'</span>, <span class="string">'of'</span>], <span class="string">'the'</span>), ([<span class="string">'study'</span>, <span class="string">'the'</span>, <span class="string">'of'</span>, <span class="string">'a'</span>], <span class="string">'idea'</span>)]</span><br></pre></td></tr></table></figure>

<h2 id="序列模型和长短记忆网络"><a href="#序列模型和长短记忆网络" class="headerlink" title="序列模型和长短记忆网络"></a>序列模型和长短记忆网络</h2><p>这时候，我们看到了不同的前馈神经网络，但是网络中没有状态保持。所以他们的行为可能不会像我们想的那样。序列模型是NLP的核心。他们是与你输入之间存在某种依赖关系。经典的例子就词性标注的隐马尔可夫模型。另外一个例子就是条件随机场。<br>一个循环神经网络是可以保持一些状态的网络。比如，它的输出可以作为下个输入的一部分。所以信息可以随着序列在网络中传播。以LSTM为例，对于序列中的每一个元素，有一个对应的隐藏状态$h_i$,可以包含序列中之前点的任意信息。我们在语言模型中会用隐藏状态预测词语，序列标注，和大量其他的事情。</p>
<h3 id="Pytorch-的LSTM"><a href="#Pytorch-的LSTM" class="headerlink" title="Pytorch 的LSTM"></a>Pytorch 的LSTM</h3><p>在开始示例之前，需要注意一些事件。Pytorch的LSTM期望它的输入时的形状是3D tensor.这些tensor的轴的语义是重要的。序列中的第一个轴是它本身，第二个轴是迷你批次中的索引实例，第三个索引是输入的元素。之前我们没有讨论过迷你批次，所以我们忽略它，假设我们总是遇到第二个轴是一维的。如果我们想在序列模型上运行“The cow jumpd”,我们的输入应该像这样：<br>$$<br>\begin{split}\begin{bmatrix}<br>\overbrace{q_\text{The}}^\text{row vector} \\<br>q_\text{cow} \\<br>q_\text{jumped}<br>\end{bmatrix}\end{split}<br>$$<br>除了记住这是一个附加的大小为1的第二维。<br>此外，您可以一次查看一个序列，在这种情况下，第一个轴也将具有大小1。<br>让我们看一个简单的例子。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># Author: Robert Guthrie</span><br><span class="line"></span><br><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line">import torch.optim as optim</span><br><span class="line"></span><br><span class="line">torch.manual_seed(1)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">lstm = nn.LSTM(<span class="number">3</span>, <span class="number">3</span>)  <span class="comment"># Input dim is 3, output dim is 3</span></span><br><span class="line">inputs = [torch.randn(<span class="number">1</span>, <span class="number">3</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">5</span>)]  <span class="comment"># make a sequence of length 5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># initialize the hidden state.</span></span><br><span class="line">hidden = (torch.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>),</span><br><span class="line">          torch.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> inputs:</span><br><span class="line">    <span class="comment"># Step through the sequence one element at a time.</span></span><br><span class="line">    <span class="comment"># after each step, hidden contains the hidden state.</span></span><br><span class="line">    out, hidden = lstm(i.view(<span class="number">1</span>, <span class="number">1</span>, <span class="number">-1</span>), hidden)</span><br><span class="line"></span><br><span class="line"><span class="comment"># alternatively, we can do the entire sequence all at once.</span></span><br><span class="line"><span class="comment"># the first value returned by LSTM is all of the hidden states throughout</span></span><br><span class="line"><span class="comment"># the sequence. the second is just the most recent hidden state</span></span><br><span class="line"><span class="comment"># (compare the last slice of "out" with "hidden" below, they are the same)</span></span><br><span class="line"><span class="comment"># The reason for this is that:</span></span><br><span class="line"><span class="comment"># "out" will give you access to all hidden states in the sequence</span></span><br><span class="line"><span class="comment"># "hidden" will allow you to continue the sequence and backpropagate,</span></span><br><span class="line"><span class="comment"># by passing it as an argument  to the lstm at a later time</span></span><br><span class="line"><span class="comment"># Add the extra 2nd dimension</span></span><br><span class="line">inputs = torch.cat(inputs).view(len(inputs), <span class="number">1</span>, <span class="number">-1</span>)</span><br><span class="line">hidden = (torch.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>), torch.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>))  <span class="comment"># clean out hidden state</span></span><br><span class="line">out, hidden = lstm(inputs, hidden)</span><br><span class="line">print(out)</span><br><span class="line">print(hidden)</span><br></pre></td></tr></table></figure>

<p>Out:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[<span class="number">-0.0187</span>,  <span class="number">0.1713</span>, <span class="number">-0.2944</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">-0.3521</span>,  <span class="number">0.1026</span>, <span class="number">-0.2971</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">-0.3191</span>,  <span class="number">0.0781</span>, <span class="number">-0.1957</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">-0.1634</span>,  <span class="number">0.0941</span>, <span class="number">-0.1637</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">-0.3368</span>,  <span class="number">0.0959</span>, <span class="number">-0.0538</span>]]], grad_fn=&lt;StackBackward&gt;)</span><br><span class="line">(tensor([[[<span class="number">-0.3368</span>,  <span class="number">0.0959</span>, <span class="number">-0.0538</span>]]], grad_fn=&lt;StackBackward&gt;), tensor([[[<span class="number">-0.9825</span>,  <span class="number">0.4715</span>, <span class="number">-0.0633</span>]]], grad_fn=&lt;StackBackward&gt;))</span><br></pre></td></tr></table></figure>

<h3 id="例子：LSTM进行词性标注"><a href="#例子：LSTM进行词性标注" class="headerlink" title="例子：LSTM进行词性标注"></a>例子：LSTM进行词性标注</h3><p>在这个例子中，我们将使用LSTM进行序列标注。我们不使用类似维比特算法和前向后向算法的算法。作为一个练习，在看到下面的算法后，你可以考虑在这之中如何使用维比特算法。<br>这个模型是这样的，我们的输入时$w_1,w_2,…w_M$,其中$w_i \in V$,V是我们的词汇，T是我们的标注，$y_i$是我们对单词$w_i$的标注。我们用$\hat{y_i}$表示我们对$w_i$预测的标签。<br>这是一个结构化的预测模型，我们的输出是一个序列$\hat{y_1},\dots,\hat{y_M}$，其中$\hat{y_i} \in T$<br>为了做这个预测，在序列上使用LSTM,假设在时间i时，隐藏状态是$h_i$.假设每个标签是不同的索引（像我们做词嵌入时，word_to_idx那样）。然后我们对$\hat{y_i}$的预测规则是<br>$$<br>\hat{y}_i = \text{argmax}_j \  (\log \text{Softmax}(Ah_i + b))_j<br>$$<br>其实就是对隐藏状态做仿射映射，然后做log softmax 。预测的结果就是向量里面的最大的值。这就意味着A的目标空间维度是|T|。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prepare_sequence</span><span class="params">(seq, to_ix)</span>:</span></span><br><span class="line">    idxs = [to_ix[w] <span class="keyword">for</span> w <span class="keyword">in</span> seq]</span><br><span class="line">    <span class="keyword">return</span> torch.tensor(idxs, dtype=torch.long)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">training_data = [</span><br><span class="line">    (<span class="string">"The dog ate the apple"</span>.split(), [<span class="string">"DET"</span>, <span class="string">"NN"</span>, <span class="string">"V"</span>, <span class="string">"DET"</span>, <span class="string">"NN"</span>]),</span><br><span class="line">    (<span class="string">"Everybody read that book"</span>.split(), [<span class="string">"NN"</span>, <span class="string">"V"</span>, <span class="string">"DET"</span>, <span class="string">"NN"</span>])</span><br><span class="line">]</span><br><span class="line">word_to_ix = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> sent, tags <span class="keyword">in</span> training_data:</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> sent:</span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> word_to_ix:</span><br><span class="line">            word_to_ix[word] = len(word_to_ix)</span><br><span class="line">print(word_to_ix)</span><br><span class="line">tag_to_ix = &#123;<span class="string">"DET"</span>: <span class="number">0</span>, <span class="string">"NN"</span>: <span class="number">1</span>, <span class="string">"V"</span>: <span class="number">2</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># These will usually be more like 32 or 64 dimensional.</span></span><br><span class="line"><span class="comment"># We will keep them small, so we can see how the weights change as we train.</span></span><br><span class="line">EMBEDDING_DIM = <span class="number">6</span></span><br><span class="line">HIDDEN_DIM = <span class="number">6</span></span><br></pre></td></tr></table></figure>

<p>Out:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">'The'</span>: <span class="number">0</span>, <span class="string">'dog'</span>: <span class="number">1</span>, <span class="string">'ate'</span>: <span class="number">2</span>, <span class="string">'the'</span>: <span class="number">3</span>, <span class="string">'apple'</span>: <span class="number">4</span>, <span class="string">'Everybody'</span>: <span class="number">5</span>, <span class="string">'read'</span>: <span class="number">6</span>, <span class="string">'that'</span>: <span class="number">7</span>, <span class="string">'book'</span>: <span class="number">8</span>&#125;</span><br></pre></td></tr></table></figure>

<p>Create the model:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LSTMTagger</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, embedding_dim, hidden_dim, vocab_size, tagset_size)</span>:</span></span><br><span class="line">        super(LSTMTagger, self).__init__()</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line"></span><br><span class="line">        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># The LSTM takes word embeddings as inputs, and outputs hidden states</span></span><br><span class="line">        <span class="comment"># with dimensionality hidden_dim.</span></span><br><span class="line">        self.lstm = nn.LSTM(embedding_dim, hidden_dim)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># The linear layer that maps from hidden state space to tag space</span></span><br><span class="line">        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, sentence)</span>:</span></span><br><span class="line">        embeds = self.word_embeddings(sentence)</span><br><span class="line">        lstm_out, _ = self.lstm(embeds.view(len(sentence), <span class="number">1</span>, <span class="number">-1</span>))</span><br><span class="line">        tag_space = self.hidden2tag(lstm_out.view(len(sentence), <span class="number">-1</span>))</span><br><span class="line">        tag_scores = F.log_softmax(tag_space, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> tag_scores</span><br></pre></td></tr></table></figure>

<p>Train the model:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))</span><br><span class="line">loss_function = nn.NLLLoss()</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># See what the scores are before training</span></span><br><span class="line"><span class="comment"># Note that element i,j of the output is the score for tag j for word i.</span></span><br><span class="line"><span class="comment"># Here we don't need to train, so the code is wrapped in torch.no_grad()</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    inputs = prepare_sequence(training_data[<span class="number">0</span>][<span class="number">0</span>], word_to_ix)</span><br><span class="line">    tag_scores = model(inputs)</span><br><span class="line">    print(tag_scores)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">300</span>):  <span class="comment"># again, normally you would NOT do 300 epochs, it is toy data</span></span><br><span class="line">    <span class="keyword">for</span> sentence, tags <span class="keyword">in</span> training_data:</span><br><span class="line">        <span class="comment"># Step 1. Remember that Pytorch accumulates gradients.</span></span><br><span class="line">        <span class="comment"># We need to clear them out before each instance</span></span><br><span class="line">        model.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 2. Get our inputs ready for the network, that is, turn them into</span></span><br><span class="line">        <span class="comment"># Tensors of word indices.</span></span><br><span class="line">        sentence_in = prepare_sequence(sentence, word_to_ix)</span><br><span class="line">        targets = prepare_sequence(tags, tag_to_ix)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 3. Run our forward pass.</span></span><br><span class="line">        tag_scores = model(sentence_in)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 4. Compute the loss, gradients, and update the parameters by</span></span><br><span class="line">        <span class="comment">#  calling optimizer.step()</span></span><br><span class="line">        loss = loss_function(tag_scores, targets)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line"><span class="comment"># See what the scores are after training</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    inputs = prepare_sequence(training_data[<span class="number">0</span>][<span class="number">0</span>], word_to_ix)</span><br><span class="line">    tag_scores = model(inputs)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># The sentence is "the dog ate the apple".  i,j corresponds to score for tag j</span></span><br><span class="line">    <span class="comment"># for word i. The predicted tag is the maximum scoring tag.</span></span><br><span class="line">    <span class="comment"># Here, we can see the predicted sequence below is 0 1 2 0 1</span></span><br><span class="line">    <span class="comment"># since 0 is index of the maximum value of row 1,</span></span><br><span class="line">    <span class="comment"># 1 is the index of maximum value of row 2, etc.</span></span><br><span class="line">    <span class="comment"># Which is DET NOUN VERB DET NOUN, the correct sequence!</span></span><br><span class="line">    print(tag_scores)</span><br></pre></td></tr></table></figure>

<p>OUT:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">-1.1389</span>, <span class="number">-1.2024</span>, <span class="number">-0.9693</span>],</span><br><span class="line">        [<span class="number">-1.1065</span>, <span class="number">-1.2200</span>, <span class="number">-0.9834</span>],</span><br><span class="line">        [<span class="number">-1.1286</span>, <span class="number">-1.2093</span>, <span class="number">-0.9726</span>],</span><br><span class="line">        [<span class="number">-1.1190</span>, <span class="number">-1.1960</span>, <span class="number">-0.9916</span>],</span><br><span class="line">        [<span class="number">-1.0137</span>, <span class="number">-1.2642</span>, <span class="number">-1.0366</span>]])</span><br><span class="line">tensor([[<span class="number">-0.0462</span>, <span class="number">-4.0106</span>, <span class="number">-3.6096</span>],</span><br><span class="line">        [<span class="number">-4.8205</span>, <span class="number">-0.0286</span>, <span class="number">-3.9045</span>],</span><br><span class="line">        [<span class="number">-3.7876</span>, <span class="number">-4.1355</span>, <span class="number">-0.0394</span>],</span><br><span class="line">        [<span class="number">-0.0185</span>, <span class="number">-4.7874</span>, <span class="number">-4.6013</span>],</span><br><span class="line">        [<span class="number">-5.7881</span>, <span class="number">-0.0186</span>, <span class="number">-4.1778</span>]])</span><br></pre></td></tr></table></figure>

<h3 id="练习：使用字符特征扩展LSTM词性标注"><a href="#练习：使用字符特征扩展LSTM词性标注" class="headerlink" title="练习：使用字符特征扩展LSTM词性标注"></a>练习：使用字符特征扩展LSTM词性标注</h3><p>在上面的例子中，每个单词都有嵌入作为我们模型的输入，让我们使用单词派生的方法扩展词嵌入。我们预计这个会有很大的帮助。由于单词基本的信息，像词缀，在词性标记中有很大的作用。比如在英语中，对于词缀-ly经常被标记为副词。<br>为了做这些，让$c_w$表示单词w的字符级表示。让$x_w$表示之前的词嵌入。然后我们的输入是$x_w$和$c_w$的连接。所以如果$x_w$的维度是5，$c_w$的维度是3，那么LSTM的应该接受的输入维度是8。<br>为了得到字符级表示，使用单词的字符作LSTM,让$c_w$成为LSTM的最终隐藏状态。有一些提示：<br>    -在新的模型中应该有两个LSTM,原始的那个输出POS标记的分数，新的输出每个单词的字符级表示。<br>    -为了在字符上做序列模型，你应该对字符做嵌入。字符的嵌入将会输入到字符LSTM中。</p>
<h2 id="进阶：动态决定和BI-LSTM-CRF"><a href="#进阶：动态决定和BI-LSTM-CRF" class="headerlink" title="进阶：动态决定和BI-LSTM CRF"></a>进阶：动态决定和BI-LSTM CRF</h2><h3 id="动态和静态深度学习工具包"><a href="#动态和静态深度学习工具包" class="headerlink" title="动态和静态深度学习工具包"></a>动态和静态深度学习工具包</h3><p>Pytorch 是一个动态深度学习包。另一个动态深度学习包的例子是Dynet(我提到这个是因为Dynet和Pytorch比较相似，如果你看到了Dynet写的例子，会帮助你使用pytorch实现它)。相对应的是静态工具包，包含Theano, Keras, TensorFlow等等。他们之间最核心的不同是：<br>    - 在静态工具包中，你定义了计算图一次，然后编译它，然后实例流式传给它。<br>    - 在动态工具包中，你会为每个实例定义计算图，它不会被编译，它会即时执行的。<br>没有很多经验的话，很难领会到差异，一个例子是我们想建立一个深层成分分析器，假设我们的模型大概包含下面的步骤：<br>    - 我们从下向上建立树<br>    - 标记根节点（序列的单词）<br>    - 然后，使用神经网络和单词的嵌入发现成分的组合，无论何时你组成一个成分时，使用一些技术获得成分的嵌入。这样，我们的网络完全依赖输入序列。在这个句子“The green cat scratched the wall”，在这个模型的某个地方，我们想组合这个范围$(i,j,r) = (1, 3, \text{NP})$(意思是一个NP成分从第1个单词到第3个单词，也就是“The green cat”)<br>然而，另一个句子可能是“Somewhere, the big fat cat scratched the wall”。在这个句子中，我们在某个地方想要组成这个成分$(2, 4, NP)$。这个成分将会依赖这个实例。如果我们在静态工具包里编译这个计算图一次，这将会非常困难或者根本不可能。在动态工具包里，这就仅仅是一个预定义的计算图。它可以是为每一个实例生成新计算图。所以就没有这个问题了。<br>动态工具包有一些优点，比如更容易debug,更类似于宿主语言（意思是，Pytorch和Dynet比Keras和Theano 更像Python）。</p>
<h3 id="BI-LSTM-条件随机场讨论"><a href="#BI-LSTM-条件随机场讨论" class="headerlink" title="BI-LSTM 条件随机场讨论"></a>BI-LSTM 条件随机场讨论</h3><p>在这个部分，我们将会看到一个完整，复杂的BI-LSTM 条件随机场的例子。之前的LSTM标记只是满足了词性标注，但是一个像CRF这样的序列模型对于NER表现很好。假设我们已经熟悉CRF,虽然明着听着可怕，但是所有模型都是CRF,但是LSTM提供了一些功能。这是一个先进的模型，比我们这个教程的其他模型都要复杂，如果你想跳过这个也没有关系。如果你准备好了，看看你是否可以：<br>    - 在步骤i中为标记k写出维特比变量的循环。<br>    - 修改上面的循环计算前向变量<br>    - 修改上面的循环在log空间计算前向变量<br>如果你可以做上面的三件事，那么你应该可以理解下面的代码。回想CRF计算了条件概率。假设y是标签序列，x是输入序列。然后我们计算<br>$$<br>P(y|x) = \frac{\exp{(\text{Score}(x, y)})}{\sum_{y’} \exp{(\text{Score}(x, y’)})}<br>$$<br>其中的sorce由定义一些对数势$\log \psi_i(x,y)$来定义：<br>$$<br>\text{Score}(x,y) = \sum_i \log \psi_i(x,y)<br>$$<br>为了让分隔函数易于管理，势必须只看本地的特征。<br>在BI-LSTM CRF 中，我们定义了两种势，发射和转移，在索引i出单词的势来自于在时间点i时，Bi-LSTM的隐藏状态。这个转移分数存储在|T|x|T|的矩阵$\textbf{P}$中,T是标签集。在我的实现中，$\textbf{P}<em>{j,k}$是从标签k到标签j的转移分数。所以：<br>$$<br>\text{Score}(x,y) = \sum_i \log \psi_\text{EMIT}(y_i \rightarrow x_i) + \log \psi_\text{TRANS}(y</em>{i-1} \rightarrow y_i)  \\<br>= \sum_i h_i[y_i] + \textbf{P}<em>{y_i, y</em>{i-1}}<br>$$<br>在第二个式子中，我们认为标签被用非负的索引标记。<br>如果上面讨论太简略，你可以看一下Michael Collins写的关于CRF的<a href="http://www.cs.columbia.edu/~mcollins/crf.pdf" target="_blank" rel="noopener">文章</a></p>
<h3 id="实现笔记"><a href="#实现笔记" class="headerlink" title="实现笔记"></a>实现笔记</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Author: Robert Guthrie</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.autograd <span class="keyword">as</span> autograd</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<p>帮助函数可以让代码更易读</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">argmax</span><span class="params">(vec)</span>:</span></span><br><span class="line">    <span class="comment"># return the argmax as a python int</span></span><br><span class="line">    _, idx = torch.max(vec, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> idx.item()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prepare_sequence</span><span class="params">(seq, to_ix)</span>:</span></span><br><span class="line">    idxs = [to_ix[w] <span class="keyword">for</span> w <span class="keyword">in</span> seq]</span><br><span class="line">    <span class="keyword">return</span> torch.tensor(idxs, dtype=torch.long)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute log sum exp in a numerically stable way for the forward algorithm</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">log_sum_exp</span><span class="params">(vec)</span>:</span></span><br><span class="line">    max_score = vec[<span class="number">0</span>, argmax(vec)]</span><br><span class="line">    max_score_broadcast = max_score.view(<span class="number">1</span>, <span class="number">-1</span>).expand(<span class="number">1</span>, vec.size()[<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">return</span> max_score + \</span><br><span class="line">        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))</span><br></pre></td></tr></table></figure>

<p>创建模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BiLSTM_CRF</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim)</span>:</span></span><br><span class="line">        super(BiLSTM_CRF, self).__init__()</span><br><span class="line">        self.embedding_dim = embedding_dim</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.vocab_size = vocab_size</span><br><span class="line">        self.tag_to_ix = tag_to_ix</span><br><span class="line">        self.tagset_size = len(tag_to_ix)</span><br><span class="line"></span><br><span class="line">        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)</span><br><span class="line">        self.lstm = nn.LSTM(embedding_dim, hidden_dim // <span class="number">2</span>,</span><br><span class="line">                            num_layers=<span class="number">1</span>, bidirectional=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Maps the output of the LSTM into tag space.</span></span><br><span class="line">        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Matrix of transition parameters.  Entry i,j is the score of</span></span><br><span class="line">        <span class="comment"># transitioning *to* i *from* j.</span></span><br><span class="line">        self.transitions = nn.Parameter(</span><br><span class="line">            torch.randn(self.tagset_size, self.tagset_size))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># These two statements enforce the constraint that we never transfer</span></span><br><span class="line">        <span class="comment"># to the start tag and we never transfer from the stop tag</span></span><br><span class="line">        self.transitions.data[tag_to_ix[START_TAG], :] = <span class="number">-10000</span></span><br><span class="line">        self.transitions.data[:, tag_to_ix[STOP_TAG]] = <span class="number">-10000</span></span><br><span class="line"></span><br><span class="line">        self.hidden = self.init_hidden()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_hidden</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> (torch.randn(<span class="number">2</span>, <span class="number">1</span>, self.hidden_dim // <span class="number">2</span>),</span><br><span class="line">                torch.randn(<span class="number">2</span>, <span class="number">1</span>, self.hidden_dim // <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_forward_alg</span><span class="params">(self, feats)</span>:</span></span><br><span class="line">        <span class="comment"># Do the forward algorithm to compute the partition function</span></span><br><span class="line">        init_alphas = torch.full((<span class="number">1</span>, self.tagset_size), <span class="number">-10000.</span>)</span><br><span class="line">        <span class="comment"># START_TAG has all of the score.</span></span><br><span class="line">        init_alphas[<span class="number">0</span>][self.tag_to_ix[START_TAG]] = <span class="number">0.</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Wrap in a variable so that we will get automatic backprop</span></span><br><span class="line">        forward_var = init_alphas</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Iterate through the sentence</span></span><br><span class="line">        <span class="keyword">for</span> feat <span class="keyword">in</span> feats:</span><br><span class="line">            alphas_t = []  <span class="comment"># The forward tensors at this timestep</span></span><br><span class="line">            <span class="keyword">for</span> next_tag <span class="keyword">in</span> range(self.tagset_size):</span><br><span class="line">                <span class="comment"># broadcast the emission score: it is the same regardless of</span></span><br><span class="line">                <span class="comment"># the previous tag</span></span><br><span class="line">                emit_score = feat[next_tag].view(</span><br><span class="line">                    <span class="number">1</span>, <span class="number">-1</span>).expand(<span class="number">1</span>, self.tagset_size)</span><br><span class="line">                <span class="comment"># the ith entry of trans_score is the score of transitioning to</span></span><br><span class="line">                <span class="comment"># next_tag from i</span></span><br><span class="line">                trans_score = self.transitions[next_tag].view(<span class="number">1</span>, <span class="number">-1</span>)</span><br><span class="line">                <span class="comment"># The ith entry of next_tag_var is the value for the</span></span><br><span class="line">                <span class="comment"># edge (i -&gt; next_tag) before we do log-sum-exp</span></span><br><span class="line">                next_tag_var = forward_var + trans_score + emit_score</span><br><span class="line">                <span class="comment"># The forward variable for this tag is log-sum-exp of all the</span></span><br><span class="line">                <span class="comment"># scores.</span></span><br><span class="line">                alphas_t.append(log_sum_exp(next_tag_var).view(<span class="number">1</span>))</span><br><span class="line">            forward_var = torch.cat(alphas_t).view(<span class="number">1</span>, <span class="number">-1</span>)</span><br><span class="line">        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]</span><br><span class="line">        alpha = log_sum_exp(terminal_var)</span><br><span class="line">        <span class="keyword">return</span> alpha</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_get_lstm_features</span><span class="params">(self, sentence)</span>:</span></span><br><span class="line">        self.hidden = self.init_hidden()</span><br><span class="line">        embeds = self.word_embeds(sentence).view(len(sentence), <span class="number">1</span>, <span class="number">-1</span>)</span><br><span class="line">        lstm_out, self.hidden = self.lstm(embeds, self.hidden)</span><br><span class="line">        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)</span><br><span class="line">        lstm_feats = self.hidden2tag(lstm_out)</span><br><span class="line">        <span class="keyword">return</span> lstm_feats</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_score_sentence</span><span class="params">(self, feats, tags)</span>:</span></span><br><span class="line">        <span class="comment"># Gives the score of a provided tag sequence</span></span><br><span class="line">        score = torch.zeros(<span class="number">1</span>)</span><br><span class="line">        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long), tags])</span><br><span class="line">        <span class="keyword">for</span> i, feat <span class="keyword">in</span> enumerate(feats):</span><br><span class="line">            score = score + \</span><br><span class="line">                self.transitions[tags[i + <span class="number">1</span>], tags[i]] + feat[tags[i + <span class="number">1</span>]]</span><br><span class="line">        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[<span class="number">-1</span>]]</span><br><span class="line">        <span class="keyword">return</span> score</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_viterbi_decode</span><span class="params">(self, feats)</span>:</span></span><br><span class="line">        backpointers = []</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Initialize the viterbi variables in log space</span></span><br><span class="line">        init_vvars = torch.full((<span class="number">1</span>, self.tagset_size), <span class="number">-10000.</span>)</span><br><span class="line">        init_vvars[<span class="number">0</span>][self.tag_to_ix[START_TAG]] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># forward_var at step i holds the viterbi variables for step i-1</span></span><br><span class="line">        forward_var = init_vvars</span><br><span class="line">        <span class="keyword">for</span> feat <span class="keyword">in</span> feats:</span><br><span class="line">            bptrs_t = []  <span class="comment"># holds the backpointers for this step</span></span><br><span class="line">            viterbivars_t = []  <span class="comment"># holds the viterbi variables for this step</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> next_tag <span class="keyword">in</span> range(self.tagset_size):</span><br><span class="line">                <span class="comment"># next_tag_var[i] holds the viterbi variable for tag i at the</span></span><br><span class="line">                <span class="comment"># previous step, plus the score of transitioning</span></span><br><span class="line">                <span class="comment"># from tag i to next_tag.</span></span><br><span class="line">                <span class="comment"># We don't include the emission scores here because the max</span></span><br><span class="line">                <span class="comment"># does not depend on them (we add them in below)</span></span><br><span class="line">                next_tag_var = forward_var + self.transitions[next_tag]</span><br><span class="line">                best_tag_id = argmax(next_tag_var)</span><br><span class="line">                bptrs_t.append(best_tag_id)</span><br><span class="line">                viterbivars_t.append(next_tag_var[<span class="number">0</span>][best_tag_id].view(<span class="number">1</span>))</span><br><span class="line">            <span class="comment"># Now add in the emission scores, and assign forward_var to the set</span></span><br><span class="line">            <span class="comment"># of viterbi variables we just computed</span></span><br><span class="line">            forward_var = (torch.cat(viterbivars_t) + feat).view(<span class="number">1</span>, <span class="number">-1</span>)</span><br><span class="line">            backpointers.append(bptrs_t)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Transition to STOP_TAG</span></span><br><span class="line">        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]</span><br><span class="line">        best_tag_id = argmax(terminal_var)</span><br><span class="line">        path_score = terminal_var[<span class="number">0</span>][best_tag_id]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Follow the back pointers to decode the best path.</span></span><br><span class="line">        best_path = [best_tag_id]</span><br><span class="line">        <span class="keyword">for</span> bptrs_t <span class="keyword">in</span> reversed(backpointers):</span><br><span class="line">            best_tag_id = bptrs_t[best_tag_id]</span><br><span class="line">            best_path.append(best_tag_id)</span><br><span class="line">        <span class="comment"># Pop off the start tag (we dont want to return that to the caller)</span></span><br><span class="line">        start = best_path.pop()</span><br><span class="line">        <span class="keyword">assert</span> start == self.tag_to_ix[START_TAG]  <span class="comment"># Sanity check</span></span><br><span class="line">        best_path.reverse()</span><br><span class="line">        <span class="keyword">return</span> path_score, best_path</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">neg_log_likelihood</span><span class="params">(self, sentence, tags)</span>:</span></span><br><span class="line">        feats = self._get_lstm_features(sentence)</span><br><span class="line">        forward_score = self._forward_alg(feats)</span><br><span class="line">        gold_score = self._score_sentence(feats, tags)</span><br><span class="line">        <span class="keyword">return</span> forward_score - gold_score</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, sentence)</span>:</span>  <span class="comment"># dont confuse this with _forward_alg above.</span></span><br><span class="line">        <span class="comment"># Get the emission scores from the BiLSTM</span></span><br><span class="line">        lstm_feats = self._get_lstm_features(sentence)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Find the best path, given the features.</span></span><br><span class="line">        score, tag_seq = self._viterbi_decode(lstm_feats)</span><br><span class="line">        <span class="keyword">return</span> score, tag_seq</span><br></pre></td></tr></table></figure>

<p>进行训练：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">START_TAG = <span class="string">"&lt;START&gt;"</span></span><br><span class="line">STOP_TAG = <span class="string">"&lt;STOP&gt;"</span></span><br><span class="line">EMBEDDING_DIM = <span class="number">5</span></span><br><span class="line">HIDDEN_DIM = <span class="number">4</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Make up some training data</span></span><br><span class="line">training_data = [(</span><br><span class="line">    <span class="string">"the wall street journal reported today that apple corporation made money"</span>.split(),</span><br><span class="line">    <span class="string">"B I I I O O O B I O O"</span>.split()</span><br><span class="line">), (</span><br><span class="line">    <span class="string">"georgia tech is a university in georgia"</span>.split(),</span><br><span class="line">    <span class="string">"B I O O O O B"</span>.split()</span><br><span class="line">)]</span><br><span class="line"></span><br><span class="line">word_to_ix = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> sentence, tags <span class="keyword">in</span> training_data:</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> sentence:</span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> word_to_ix:</span><br><span class="line">            word_to_ix[word] = len(word_to_ix)</span><br><span class="line"></span><br><span class="line">tag_to_ix = &#123;<span class="string">"B"</span>: <span class="number">0</span>, <span class="string">"I"</span>: <span class="number">1</span>, <span class="string">"O"</span>: <span class="number">2</span>, START_TAG: <span class="number">3</span>, STOP_TAG: <span class="number">4</span>&#125;</span><br><span class="line"></span><br><span class="line">model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM)</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>, weight_decay=<span class="number">1e-4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check predictions before training</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    precheck_sent = prepare_sequence(training_data[<span class="number">0</span>][<span class="number">0</span>], word_to_ix)</span><br><span class="line">    precheck_tags = torch.tensor([tag_to_ix[t] <span class="keyword">for</span> t <span class="keyword">in</span> training_data[<span class="number">0</span>][<span class="number">1</span>]], dtype=torch.long)</span><br><span class="line">    print(model(precheck_sent))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Make sure prepare_sequence from earlier in the LSTM section is loaded</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(</span><br><span class="line">        <span class="number">300</span>):  <span class="comment"># again, normally you would NOT do 300 epochs, it is toy data</span></span><br><span class="line">    <span class="keyword">for</span> sentence, tags <span class="keyword">in</span> training_data:</span><br><span class="line">        <span class="comment"># Step 1. Remember that Pytorch accumulates gradients.</span></span><br><span class="line">        <span class="comment"># We need to clear them out before each instance</span></span><br><span class="line">        model.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 2. Get our inputs ready for the network, that is,</span></span><br><span class="line">        <span class="comment"># turn them into Tensors of word indices.</span></span><br><span class="line">        sentence_in = prepare_sequence(sentence, word_to_ix)</span><br><span class="line">        targets = torch.tensor([tag_to_ix[t] <span class="keyword">for</span> t <span class="keyword">in</span> tags], dtype=torch.long)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 3. Run our forward pass.</span></span><br><span class="line">        loss = model.neg_log_likelihood(sentence_in, targets)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 4. Compute the loss, gradients, and update the parameters by</span></span><br><span class="line">        <span class="comment"># calling optimizer.step()</span></span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check predictions after training</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    precheck_sent = prepare_sequence(training_data[<span class="number">0</span>][<span class="number">0</span>], word_to_ix)</span><br><span class="line">    print(model(precheck_sent))</span><br><span class="line"><span class="comment"># We got it!</span></span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(tensor(<span class="number">2.6907</span>), [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>])</span><br><span class="line">(tensor(<span class="number">20.4906</span>), [<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>])</span><br></pre></td></tr></table></figure>

<h3 id="练习：辨别标记的新损失函数"><a href="#练习：辨别标记的新损失函数" class="headerlink" title="练习：辨别标记的新损失函数"></a>练习：辨别标记的新损失函数</h3><p>在做解码的时候，我们不必创建一个计算图，因为我们不会从维比特路径分反向传播。因为我们无论如何都有它，尝试训练这个标记器，其中损失函数是维比特路径得分和金标准路径之间的差异，应该清楚的是，当预测是序列是正确的序列时，这个函数是非负的和0，这个基本上是结构感知器。<br>这个改动应该是比较短的，因为维比特算法和分数序列已经实现了。这是依赖训练实例的计算图的例子，虽然我没有使用静态工具包实现这个，但我能想象的到，那是可能的但是不会那么简单。<br>找一些实际的数据做一下对比吧！</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Pytorch/" rel="tag"><i class="fa fa-tag"></i> Pytorch</a>
          
            <a href="/tags/nlp/" rel="tag"><i class="fa fa-tag"></i> nlp</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/05/17/统计自然语言处理-句法分析/" rel="next" title="统计自然语言处理-句法分析">
                <i class="fa fa-chevron-left"></i> 统计自然语言处理-句法分析
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/08/03/使用Pytorch实现word2vec-skip-gram/" rel="prev" title="使用Pytorch实现word2vec(skip-gram)">
                使用Pytorch实现word2vec(skip-gram) <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
    </div>
  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.png" alt="Arvin_Zhang">
            
              <p class="site-author-name" itemprop="name">Arvin_Zhang</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">23</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">6</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">20</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/zhangweifeng919" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:zhangweifeng919@gmail.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#前言"><span class="nav-number">1.</span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#pytorch-介绍"><span class="nav-number">2.</span> <span class="nav-text">pytorch 介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#介绍torch-的tensor库"><span class="nav-number">2.1.</span> <span class="nav-text">介绍torch 的tensor库</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#创建Tensors"><span class="nav-number">2.2.</span> <span class="nav-text">创建Tensors</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#操作tensor"><span class="nav-number">2.3.</span> <span class="nav-text">操作tensor</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#重排tensor形状"><span class="nav-number">2.4.</span> <span class="nav-text">重排tensor形状</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#计算图和自动微分"><span class="nav-number">2.5.</span> <span class="nav-text">计算图和自动微分</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#使用pytorch进行深度学习"><span class="nav-number">3.</span> <span class="nav-text">使用pytorch进行深度学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#深度学习的基本组成：仿射映射，非线性和目标"><span class="nav-number">3.1.</span> <span class="nav-text">深度学习的基本组成：仿射映射，非线性和目标</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#仿射映射"><span class="nav-number">3.2.</span> <span class="nav-text">仿射映射</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#非线性"><span class="nav-number">3.3.</span> <span class="nav-text">非线性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Softmax-和概率"><span class="nav-number">3.4.</span> <span class="nav-text">Softmax 和概率</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#目标函数"><span class="nav-number">3.5.</span> <span class="nav-text">目标函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#优化和训练"><span class="nav-number">3.6.</span> <span class="nav-text">优化和训练</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#使用pytorch创建网络组件"><span class="nav-number">3.7.</span> <span class="nav-text">使用pytorch创建网络组件</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#例子：逻辑回归词袋分类器"><span class="nav-number">3.7.1.</span> <span class="nav-text">例子：逻辑回归词袋分类器</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#词嵌入：词汇语义编码"><span class="nav-number">4.</span> <span class="nav-text">词嵌入：词汇语义编码</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#获取稠密词嵌入"><span class="nav-number">4.1.</span> <span class="nav-text">获取稠密词嵌入</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Pytorch中的词嵌入"><span class="nav-number">4.2.</span> <span class="nav-text">Pytorch中的词嵌入</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#一个例子：N-gram-语言模型"><span class="nav-number">4.3.</span> <span class="nav-text">一个例子：N-gram 语言模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#练习：计算词嵌入：连续词袋（Continuous-Bag-of-Words）"><span class="nav-number">4.4.</span> <span class="nav-text">练习：计算词嵌入：连续词袋（Continuous Bag-of-Words）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#序列模型和长短记忆网络"><span class="nav-number">5.</span> <span class="nav-text">序列模型和长短记忆网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Pytorch-的LSTM"><span class="nav-number">5.1.</span> <span class="nav-text">Pytorch 的LSTM</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#例子：LSTM进行词性标注"><span class="nav-number">5.2.</span> <span class="nav-text">例子：LSTM进行词性标注</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#练习：使用字符特征扩展LSTM词性标注"><span class="nav-number">5.3.</span> <span class="nav-text">练习：使用字符特征扩展LSTM词性标注</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#进阶：动态决定和BI-LSTM-CRF"><span class="nav-number">6.</span> <span class="nav-text">进阶：动态决定和BI-LSTM CRF</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#动态和静态深度学习工具包"><span class="nav-number">6.1.</span> <span class="nav-text">动态和静态深度学习工具包</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#BI-LSTM-条件随机场讨论"><span class="nav-number">6.2.</span> <span class="nav-text">BI-LSTM 条件随机场讨论</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#实现笔记"><span class="nav-number">6.3.</span> <span class="nav-text">实现笔记</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#练习：辨别标记的新损失函数"><span class="nav-number">6.4.</span> <span class="nav-text">练习：辨别标记的新损失函数</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2019 &mdash; <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Arvin_Zhang</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Site words total count&#58;</span>
    
    <span title="Site words total count"></span>
  
</div>











<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
    console.log("已经推送");
})();
</script>

        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  










  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item=>{
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: false,
        appId: 'kjMy4Wir9L7O2wDtjHexeyl8-gzGzoHsz',
        appKey: 'zruMkYmFF2lxyMAreQ8AdUWr',
        placeholder: '欢迎提问，一起讨论\(^o^)/~',
        avatar:'mm',
        guest_info:guest,
        pageSize:'10' || 10,
    });
  </script>



  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("kjMy4Wir9L7O2wDtjHexeyl8-gzGzoHsz", "zruMkYmFF2lxyMAreQ8AdUWr");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

</body>
</html>
