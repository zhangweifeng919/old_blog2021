---
title: 统计自然语言处理-预备知识
date: 2019-04-06 17:13:37
tags: 
    - nlp
    - 统计自然语言处理
categories:
    - 读书笔记
description: 介绍统计自然语言处理中的预备知识，包含概率论，信息论，和支持向量机SVM
---

## 概率论基本概念

### 最大似然估计

略

### 条件概率

略

### 贝叶斯法则

$$P(B|A)=\cfrac{P(A|B)P(B)}{P(A)}$$s
因为通常只关心给定事件A情况下事件B的概率，P(A)的值通常是确定的，故有
$$arg\max_{B}\cfrac{P(A|B)P(B)}{P(A)}=arg \max_{B}P(A|B)P(B)$$


### 随机变量

略

### 二项分布

略

### 联合概率分布和条件概率分布

略

### 贝叶斯决策理论（Bayesian decision theory)

贝叶斯决策理论是统计方法处理模式分类的基本理论之一。假设研究的分类问题有c个类别，各个类别状态用$w_i$表示；对应于各个类别$w_i$出现的先验概率为$P(w_i)$;在特征空间已经观察到某一向量X,$x=[x_1,x_2,...,x_d]$,T是d维特征空间上的一点，且条件概率密度函数$p(x|w_i)$是已知的，那么利用贝叶斯公式可以得到后验概率$P(w_i|x)$ 如下

$$ P(w_i|x)=\cfrac{ p(\mathbf{x}|w_i)P(w_i)} {\sum_{j=1}^c p(\mathbf{x}|w_j)P(w_j) }$$
基于最小错误率的贝叶斯决策规则为：
$$ 如果P(w_i|\mathbf{x})= \max_{j=1,2,3,...,c} P(w_j|\mathbf{x}),那么，\mathbf{x} \in w_i$$

### 期望和方差

略

## 信息论基本概念

### 熵

1948年6月和10月，由贝尔实验室出版的《贝尔系统技术》杂志连载了香农博士的文章《通信的数学原理》，该文奠定了信息论的基础。熵（entropy) 是信息论的基本概念。
如果X是一个离散型随机变量，取值空间为$\mathbf{R}$,其概率分布$p(x)=P(X=x),x \in \mathbf{R}$。那么，X的熵H(X)定义为：
$$
H(X)=-\sum_{x \in \mathbf{R}} p(x)\log_2{p(x)}
$$
其中，约定$0\log0=0$。H(x)可以写成H(p)。该公式定义的熵的单位为二进制（比特）。通常将$\log_2 p(x)$ 简写为$\log p(x)$  
熵又称为自信息（self-information),可以视为描述一个随机变量不确定的数量。它表示信源X每发一个符号（不论发什么符号）所提供的平均信息量【姜丹，2001】。一个随机变量的熵越大，它的不确定性越大，那么正确估计其值的可能性越小。越不确定的随机变量越需要大的信息量用以确定其值。

### 联合熵和条件熵

如果X，Y是一对离散型随机变量$X,Y~p(x,y)$，X，Y的联合熵（joint entropy）H(x,y)定义为：
$$H(X,Y)=-\sum_{x \in X} \sum_{y \in Y} p(x,y)\log p(x,y)$$
联合熵实际上就是描述一对随机变量平均所需要的信息量

给定随机变量X的情况下，随机变量Y的条件熵（conditional entropy) 定义为：

\begin{align}
H(Y|X) & = \sum_{x \in X} p(x)H(Y|X=x) \\\\
& = \sum_{x \in X}p(x)\left[-\sum_{y \in Y} p(y|x) \log p(y|x)\right] \\\\
& = -\sum_{x \in X} \sum_{y \in Y} p(x,y) \log p(y|x)
\end{align}

将联合熵中的联合概率$\log p(x,y)$ 展开，可得：

\begin{align}
H(X,Y) & = - \sum_{x \in X} \sum_{y \in Y} p(x,y) \log \left[p(x)p(y|x) \right]  \\\\
        & = -\sum_{x \in X} \sum_{y \in Y} p(x,y) \left[ \log p(x) + \log p(y|x) \right]  \\\\
        & = -\sum_{x \in X} \sum_{y \in Y} p(x,y) \log p(x) -\sum_{x \in X} \sum_{y \in Y} p(x,y) \log p(y|x)  \\\\
        & = -\sum_{x \in X} p(x) \log p(x) -\sum_{x \in X} \sum_{y \in Y} p(x,y) \log p(y|x) \\\\
        & = H(X) + H(Y|X)
\end{align}
我们称为熵的连锁规则（chain rule of entropy)。推广的到一般，有：
$$ H(X_1,X_2,...,X_n)=H(X_1) + H(X_2 | X_1) + ...H(X_n |X_1,...,X_{n-1})$$
一般地，对于长度为n的信息，每一个字符或字的熵为：
$$H_{rate}= \cfrac{1} {n} H(X_{1n} = - \cfrac{1}{n} \sum_{x_{1n}} p(x_{1n}) \log p(x_{1n})$$
这个数值称为熵率（entropy rate）。其中$X_{1n}$表示随机变量序列$(X_1,X_2,...,X_n)$。
如果假定一种语言由一系列符号组成的随机过程，$L=(X_i)$，例如，某报纸的一批语料，那么我们可以定义这种语言L的熵作为其随机过程的熵率，即：
$$ H_{rate}(L)= \lim_{n \to \infty} \cfrac{1}{n}H(X_1,X_2,...,X_n)$$
之所以把语言L看作语言样本的极限，因为理论上样本可以无限长。

### 互信息

根据熵的连锁规则，有
$$H(X,Y)=H(X)+ H(Y|X) =H(Y) + H(X|Y)$$
因此：
$$H(X)-H(X|Y)=H(Y)-H(Y|X)$$
这个差叫做X和Y的互信息（mutual information，MI），记作I（X；Y）。
I（X；Y）反映的是在知道了Y的值以后X的不确定性的减少量。可以理解为Y的值透露了多少关于X的信息量。
如果将定义中的H（X）和H（X|Y）展开，可得
\begin{align}
I(X;Y) &= H(X)-H(X|Y) \\\\
&= H(X)+H(Y)-H(X,Y)   \\\\
&=\sum_x p(x) \log \cfrac{1}{p(x)} + \sum_y p(y) \log \cfrac{1}{p(y)} + \sum_{x,y} p(x,y)logp(x,y) \\\\
&=\sum_{x,y} \log \cfrac{p(x,y)}{p(x)p(y)}
\end{align}
由于H（X|X）=0，因此
$$H(X)=H(X)-H(X|X)=I(X;X)$$
一方面说明了为什么熵又称为自信息，另一方面说明了两个完全相互依赖的变量之间的互信息并不是一个常量，而是取决它们的熵。
实际上，互信息体现了两变量之间的依赖程度：如果I（X:Y）>> 0，表明X和Y是高度相关的，如果I(X:Y)=0,表明X和Y是相互独立的。
如果I（X:Y）<<0,表明Y的出现，增大了X的不确定性，常常是不利的，平均互信息量是非负的。
同样，我们可以推导出条件互信息和互信息的连锁规则：

$$I(X;Y |Z)=I((X;Y) |Z)=H(X|Z)-H(X|Y,Z) $$
\begin{align}
I(X_{1n};Y) &= I(X_1,Y)+ ...+I(I_n;Y | X_1,...,X_{n-1}) \\\\
            &=\sum_{i=1}^n I(X_i;Y | X_1,...,X_{i-1})
\end{align}
互信息在词汇聚类，汉语自动分词，词义消岐问题的研究中具有重要用途。

### 相对熵

相对熵（relative entropy） 又称 Kullback-Leibler 差异（Kullback-Leibler divergence）,简称KL距离，是衡量相同事件空间里两个概率分布相对差距的测度。两个概率分布p(x)和q(x)的相对熵定义为
$$
D(p \parallel q)= \sum_{x \in X} p(x) \log \cfrac{p(x)}{q(x)} 
$$
该定义中约定 $0 \log (0/q)= 0, p\log(p/0) = \infty$。 表示成期望值为：
$$ D((p \parallel q) = E_p\left( \log \cfrac{p(X)}{q(X)} \right)$$
显然，当两个随机分布完全相同时，即p=q,相对熵为0.当随机分布差别增加时，相对熵期望值也增大。
互信息实际上是衡量一个联合分布于独立性差距多大的测度：
$$ I(X;Y) =D(p(x,y) \parallel p(x)p(y))$$
同样，可以推导出条件相对熵和相对熵的连锁规则
$$
D\left(p(y|x) \parallel q(y|x) \right) = \sum_x p(x) \sum_y p(y|x) \log \cfrac{p(y|x)}{q(y|x)} \\\\
D\left(p(x,y) \parallel q(x,y) \right) =D \left( p(x) \parallel q(x) \right) + D \left(p(y|x) \parallel q(y|x) \right)
$$
<!-- 交叉熵-->
### 交叉熵

交叉熵是用来衡量估计模型与真实概率分布之间差异情况的
如果一个随机变量$X \sim p(x)$，q(x)为用于近似p(x)的概率分布，那么，随机变量X和模型q之间的交叉熵(cross entropy) 定义为：

\begin{align}
H(X,q) &= H(X) +D(p \parallel q) \\\\
 &= - \sum_x p(x) \log q(x) \\\\
 &= E_p \left( \log \cfrac{1}{q(x)} \right)
\end{align}

由此，可以定义语言$L=(X_i) \sim p(x)$ 与其模型q的交叉熵为：
$$ H(L,q)= - \lim_{n \to \infty} \cfrac{1}{n} \sum_{x_1^n} p(x_1^n) \log q(x_1^n)$$
其中，$x_1^n =x_1,x_2,...,x_n$为L的语句，$p(x_1^n)$为L中$x_1^n$的概率，$q(x_1^n)$ 为模型q对$x_1^n$的概率估计。
至此，仍然无法计算这个语言的交叉熵，因为我们并不知道真实概率$p(x_1^n)$，不过可以假设这个语言是”理想“的，即n趋于无限大，其全部”单词“的概率和为1，也即，根据信息论定理：假设语言L是稳态（stationary) 遍历的（ergodic) 随机过程，L与其模型q的交叉熵计算公式就变为：
$$ H(L,q)= - \lim_{n \to \infty} \cfrac{1}{n} \log q(x_1^n)$$
由此，可以根据模型q和一个含有大量数据的L的样本来计算交叉熵。
在设计模型q时，目的是使得交叉熵最小，从而使得模型最接近真是的概率分布$p(x)$。一般，在n足够大时，我们近似地采用如下计算方法：
$$H(L,q) \approx - \cfrac{1}{n} log q(x_1^n)$$
交叉熵与模型在测试语料中分配给每个单词的平均概率所表达的含义正好相反，模型的交叉熵越小，模型的表现越好。
<!-- 困惑度--->
### 困惑度

在设计语言模型时，我们通常用困惑度（perplexity）来代替交叉熵衡量语言模型的好坏。给定语言L的样本$l_1^n= l_1,l_2,...l_n$的困惑度$PP_q$定义为：
$$ PP_q = 2^{H(L,q)} \approx 2^{- \cfrac{1}{n} log q(l_1^n)}= \left[ q(l_1^n) \right]^{- \cfrac{1}{n}}$$
同样，语言模型设计的任务就是寻找困惑度最小的模型，使其最接近真实语言的情况。

### 噪声信道模型

信息熵可以定量的估计信源每发送一个符号所提供的平均信息量，但对于通信系统来说，最根本的问题还在于如何定量的估算从信道输出中获取多少信息量。
香农为了模型化信道通信问题，在熵的概念基础上提出了噪声信道模型（noisy channel model）。目标是优化噪声信道中信号传输的吞吐量和准确率。其基本假设是一个信道的输出以一定的概率依赖输入。
在自然语言处理中不需要编码，一种自然语言的句子可以视为已经编码的符号序列，但是需要进行解码，使得观察到的输出序列更接近于输入。
模拟信道模型中，在自然语言处理中，很多问题可以归结为在给定输出O（可能含有误传信息）的情况下，如何从所有可能的输入I中求解最有可能的那个，即求出使$p(I|O)$最大的I作为输入$\hat{I}$。根据贝叶斯公式，有
\begin{align}
\hat{I} & = \arg\max_I p(I|O)= \arg\max_I \cfrac{p(I)p(O|I)}{p(O)} \\\\
& = \arg\max_I p(I)p(O|Is)
\end{align}
式中有两个概率分布需要考虑，一个是p(I),称为语言模型（language model），是指输入语言中”词“序列的概率分布；另一个是p(O|I),称为信道概率（channel probability）。
举例而言，如果想要把一个法语句子f翻译成英语e,那么相应的翻译信道模型就是假设法语句子f作为信道模型的输出，它原本是一个英语句子e,但是通过噪声通道传输时被改变成了法语句子f。那么现在要做的就是如何根据概率p(e)和p(f|e)的计算求出最接近原始英文句子e的解$\hat{e}$。
噪声信道模型在自然语言处理中有着广泛的用途，除了机器翻译以外，还用于词性标注，语音识别，文字识别等很多问题的研究。

## 支持向量机（support vector machine,SVM）

支持向量机是在高维特征空间使用的线性函数假设空间的学习系统，在分类方面具有良好的性能。在模式识别，知识发现等理论研究，计算机视觉与图像识别，生物信息学以及自然语言处理等相关技术研究中得到了广泛应用。在自然语言处理中，SVM广泛用于短语识别，词义消岐，文本自动分类和信息过滤等方面。

详细介绍 略，请参考其他网上资源。

## 参考资料

《统计自然语言处理》（第二版）  宗成庆 著 清华大学出版社 [统计自然语言处理.mobi](/books/mobi/统计自然语言处理（第2版）.mobi)
