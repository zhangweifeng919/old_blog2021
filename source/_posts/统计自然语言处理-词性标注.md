---
title: 统计自然语言处理-词性标注
date: 2019-05-05 14:35:06
tags:
- 统计自然语言处理
- nlp
categories:
- 读书笔记
description: 词性（part-of-speech）是词汇基本的语法属性，通常也称为词类。词性标注就是在给定句子中判定每个词的语法范畴，确定其词性并加以标注的过程。词性标注是自然语言处理中一项非常重要的基础性工作。
---

## 概述
词性（part-of-speech）是词汇基本的语法属性，通常也称为词类。词性标注就是在给定句子中判定每个词的语法范畴，确定其词性并加以标注的过程。词性标注是自然语言处理中一项非常重要的基础性工作。

汉语词性标注同样面临许多棘手的问题，其主要难点可以归纳为如下三个方面［刘开瑛，2000］：

（1）汉语是一种缺乏词形态变化的语言，词的类别不能像印欧语那样，直接从词的形态变化上来判别。

（2）常用词兼类现象严重。《现代汉语八百词》（吕叔湘主编．北京：商务印书馆，1996）收取的常用词中，兼类词所占的比例高达22.5％，而且越是常用的词，不同的用法越多。而根据张虎等人（2004）对北京大学计算语言学研究所在网上公布的200万汉字语料进行的统计，兼类词占到11％，但兼类词的词次却占到了47％。所以，尽管兼类现象仅占汉语词汇很小的一部分，但由于兼类使用的程度高，兼类现象纷繁，覆盖面广，涉及汉语中大部分词类，因而造成在汉语文本中词类歧义排除的任务量大，而且面广，复杂多样。

（3）研究者主观原因造成的困难。语言学界在词性划分的目的、标准等问题上还存在分歧。与汉语分词规范类似，到目前为止，还没有一个统一的被广泛认可汉语词类划分标准，词类划分的粒度和标记符号都不统一。例如，在LDC标注语料中，将汉语词性一级标注集划分为33类［Xia,2000］；北京大学计算语言学研究所开发的语料库加工规范中有26个基本词类代码，74个扩充代码，标记集中共有106个代码［俞士汶等，2003a］；而山西大学提出的汉语词类标记集共有25类，包括17个大类和前缀、后缀、语素等其他类型［刘开瑛，2000］等，不一而足。词类划分标准和标记符号集的差异，以及分词规范的含混性，给中文信息处理带来了极大的困难。一方面，各研究单位各持己见，重复进行大量的低水平劳动；另一方面，大量的标注语料得不到充分利用和共享，从而造成了极大的人力、物力和资源的浪费。

总之，汉语词性标注与分词一样，是中文信息处理面临的重要的基础性问题，而且两者有着密切的关系。本节我们将更多地关注词性兼类歧义的消除方法。

## 基于统计模型的词性标注方法
1983年I.Marshall建立的LOB语料库词性标注系统CLAWS（Constituent-Likelihood Automatic Word-tagging System）是基于统计模型（n元语法与一阶马尔可夫转移矩阵）的词性标注方法的典型代表［Marshall，1983］，该系统通过对n元语法概率的统计优化，实现了133个词类标记的合理标注。其中，利用100万词的布朗标准英语语料（Brown Standard Corpus of English）测试，CLAWS系统早期版本（CLAWS2）的标注正确率已经超过了96％，CLAWS4系统完成了对1亿词汇规模的大不列颠国家语料库（British National Corpus, BNC）的标注工作［Leech et al.,1994］。
实现基于HMM的词性标注方法时，模型的参数估计是其中的关键问题。根据前面的介绍，我们可以随机地初始化HMM的所有参数，但是，这将使词性标注问题过于缺乏限制。因此，通常利用词典信息约束模型的参数。假设输出符号表由单词构成（即词序列为HMM的观察序列），如果某个对应的“词汇-词性标记”对没有被包含在词典中，那么，就令该词的生成概率（符号发生概率）为0，否则，该词的生成概率为其可能被标记的所有词性个数的倒数，即
$$
b_{j . l}=\frac{b_{j . l}^{\ast} C\left(w^{l}\right)}{\sum_{w^{m}} b_{j . m}^{*} C\left(w^{m}\right)}
$$ 
其中，$b_{j.l}$为词l由词性标记j生成的概率，C（$w^l$）为词$w^l$出现的次数，分母为在词典中所有词汇范围的求和，而
$$
b_{j . l}^{\ast}=\left\\{\begin{array}{l}{0} \qquad \text{如果} t^j \text{不是词} w^l \text{所允许的词性}\\\\
{\frac{1}{T\left(w^{l}\right)}} \qquad \text{其他情况}\end{array}\right.
$$
该式中T（$w^j$）为词$w^j$允许标记的词性个数。
这种方法是1985年由F.Jelinek提出的，我们不妨称它为Jelinek方法。这种方法等同于用最大似然估计来估算概率P（wk|ti）以初始化HMM，并假设每个词与其每个可能的词性标记出现的概率相等［Manning and Schütze,1999］。
另外，还有一种方法是采用将词汇划分成若干等价类的策略，以类为单位进行参数估计，从而避免了为每个单词单独调整参数，大大减少了参数的总个数［Kupiec,1992］。在这种方法中，首先把所有具有相同的可能词性的词汇划分为一组，不妨称其为元词（metawords），记作$u_L$。其中，下标L是一个从1到T的整数子集，T为标注集中不同标注符号的个数。
$$
u_L= \\{w^l | j \in L \leftrightarrow t^j \text{是} w^l \text{所允许的词性}\\} \qquad \forall L \subseteq \{ 1,2,...T \}  
$$
例如，如果NN＝$t^5$并且JJ＝$t^8$，那么，u{5,8}将包含词典中所有词性标记只能为NN和JJ的单词。
经过上述方法将词汇分组以后，对元词$u_L$的处理方法与Jelinek方法一样：
$$
b_{j . l}=\frac{b_{j, L}^{\ast} C\left(u_{L}\right)}{\sum_{u_{L^{\prime}}} b_{j, L^{\prime}}^{\ast} C\left(u_{L^{\prime}}\right)}
$$
其中，C（$u_{L′}$）是元词组$u_{L′}$中词汇出现的次数，分母是在所有元词$u_{L′}$上的求和，并且，
$$
b_{j . L}^{\ast}=\left\\{\begin{array}{l}{0} \qquad j \notin L \\\\
 {\frac{1}{|L|}}   \qquad  \text{否则}\end{array}\right.
$$
其中，|L|是集合L中元素的个数。
J.Kupiec提出的这种方法的优点是不需要为每一个单词调整参数，通过引入等价类以后，参数的数量大大地减少了，从而使参数估计更可靠。但是，如果有足够多的训练数据能够用Jelinek方法准确地逐词估计参数时，Kupiec方法的优势反而会变成劣势。Merialdo（1994）的实验表明，无监督方法对每个词分别估计参数时会引入一些错误，所以，上述Kupiec方法并不适合高频词的词性标注，因此，J.Kupiec在划分等价类时不包含100个出现频率最高的词汇，而是把这100个高频词分别作为一类。

一旦初始化完成以后，HMM参数就可以利用前面介绍的前向后向算法进行训练。

此外，在HMM模型实现中，还有另外一个问题需要注意，就是模型参数对训练语料的适应性。也就是说，由于不同领域语料的概率有所差异，HMM的参数也应随着语料的变化而变化。这个问题涉及两个方面，一个是对原有的训练语料增加新的语料以后，模型的参数需要重新调整；另一个是在经典HMM理论框架下，利用标注过的语料对模型初始化以后，已标注的语料就难以再发挥作用。而这两方面问题Baum-Welch方法都不能解决。为此，王挺等（1997）对原训练方法做了如下修改：
给定两个训练语料C1和C2（不妨假设C1为原有的训练语料，C2为新增加的训练语料），N为状态个数，即不同词性的个数。模型μ＝（A，B，π）的参数估计如下：
![基于统计的词性标注7-43](/images/读书笔记/统计自然语言处理/基于统计的词性标注7-43.png)
给定训练语料C1，首先用Baum-Welch方法从该语料中训练得到模型μ＝（A，B，π），但是在保存该模型时，并不直接保存π，A和B的值，而是在μ中保存所有的C1的期望变量：$start_state^{（C1）}$（i），$transition^{（C1）}$（i, j），$transition_from^{（C1）}$（i），$observation^{（C1）}$（j, k）和$state^{（C1）}$（j）（对于所有的i, j,k）。由这些变量，可以很容易地利用下面的公式计算出π，A和B的值：
![7-46](/images/读书笔记/统计自然语言处理/7-46.png)
在此基础上，假如有新的语料C2引入，我们希望建立一个既能反映C1又能反映C2的模型，那么，使用μ作为初始模型，利用前向后向变量，通过Baum-Welch方法可以得到C2的期望值变量：$start_state^{（C2）}$（i），$transition^{（C2）}$（i, j），$transition_from^{（C2）}$（i），$observation^{（C2）}$（j, k）和$state^{（C2）}$（j）（对于所有的i, j,k）。然后，将μ中保存的C1的期望值变量与C2的期望值变量相加，得到了反映C1和C2的期望值变量的值，将这些值保存下来就得到了新的模型μ*。显然，μ*的π，A和B的值也可以方便地由式（7-46）～式（7-48）计算得到。这样，根据式（7-43）～式（7-45）计算得到的模型μ*既反映了C1的信息，又反映了C2的信息。而且，由于保存了期望值变量，就不需要再为后续的训练而保存用过的训练语料了。因此，模型和训练模型的语料能够分离开来，具有良好的灵活性。

根据上述修改，标注语料的利用问题也可以得到解决。我们注意到，上面定义的期望值变量保存的是用Baum-Welch方法计算的期望值，如果给定的训练数据是标注过的语料，那么，这些期望值变量的值就是已标注语料的相应的频率统计值。不妨假设C2是被手工标注过的语料，我们能够利用下面的方法通过频率统计得到它的期望值：
![7-49](/images/读书笔记/统计自然语言处理/7-49.png)
类似地，可以将μ中保存的C1的期望值变量与式（7-49）～式（7-53）计算的C2的期望值变量相加，从而得到新的模型μ*。显然，模型μ*既反映了C1的信息，也反映了C2的信息。因此，利用该方法标注过的语料所包含的信息可以被很好地结合进了模型，并保持了模型的概率学意义。王挺等（1997）的实验表明，上述改进方法能够在新的语料（不管语料是否经过标注）引入时，方便地修改模型的参数，使之能够同时反映新的语料和原有训练语料的信息，提高模型的准确性。

另外，魏欧等人从词性概率矩阵与词汇概率矩阵的结构和数值变化等方面，对目前常用的基于统计模型的汉语词性标注方法中，训练语料规模与标注正确率之间所存在的非线性关系做了分析，并利用未标注的语料进行训练，获取概率参数，实现了一个非监督训练的标注模型［魏欧等，2000］。
朱莉等（2003）对四种常用的数据平滑方法（Good-Turing估计、线性插值平滑方法、Katz平滑方法和交叉校验参数平滑方法）在基于三元的HMM词性标注中的效果，进行了较为详细的比较研究。其实验表明，在语料有限的情况下，选择不同的数据平滑方法对实验效果影响较大。相对而言，线性插值参数平滑方法和Katz回退参数平滑方法的效果较好；Good-Turing估计只适合对低频参数进行平滑，不合适对高频参数进行平滑；交叉校验参数平滑方法高度依赖于训练语料的规模和语料划分，并且没有提供对未出现参数进行估计的方法，其平滑效果最不理想。

## 基于规则的词性标注方法

基于规则的词性标注方法是人们提出较早的一种词性标注方法，其基本思想是按兼类词搭配关系和上下文语境建造词类消歧规则。早期的词类标注规则一般由人工构造，如美国布朗大学开发的TAGGIT词类标注系统。刘开瑛（2000）曾按兼类词搭配关系构造了词类识别规则库，针对动名词兼类现象，归纳出了9条词性鉴别规则，包括：并列鉴别、同境鉴别、区别词鉴别和唯名形容词鉴别规则等，并结合词类同现概率实现了汉语词性标注系统。

然而，随着标注语料库规模的逐步增大，可利用资源越来越多，以人工提取规则的方式显然是不现实的，于是，人们提出了基于机器学习的规则自动提取方法。

E.Brill提出了通过机器学习方法从大规模语料中自动获取规则的思想［Brill,1992,1995］，从而为实现基于规则的词性标注系统提供了极大的便利。E.Brill提出的基于转换的错误驱动的（transformation-based and error-driven）学习方法可以由图7-6来描述。
![基于转换规则的错误驱动的机器学习方法](/images/读书笔记/统计自然语言处理/基于转换规则的错误驱动的机器学习方法.png)
从图7-6可以看出，基于转换规则的错误驱动的学习方法的基本思想是，首先运用初始状态标注器（initial state annotator）标识未标注的文本，由此产生已标注的文本。文本一旦被标注以后，将其与正确的标注文本（参考答案）进行比较，在E.Brill的实验中，正确的标注文本是用手工标注的语料。由于初始标注器标注的文本一般会含有错误，学习器通过将这些标注文本与正确的标注文本相比较，可以学习到一些转换规则，从而形成一个排序的转换规则集，使其能够修正已标注的文本，使标注结果更接近参考答案。这样，在所有学习到的可能的转换规则中，搜索那些使已标注文本中的错误数减少最多的规则加入到规则集，并将该规则用于调整已标注的文本，然后对已标注的语料重新打分（统计错误数）。不断重复该过程，直到没有新的转换规则能够使已标注的语料错误数减少。最终的转换规则集就是学习到转换规则结果［Brill,1995］。
基于转换规则的错误驱动的学习方法用于词性标注，解决了传统的规则方法中由手工构造规则的不足，而且与统计方法相比，系统标注速度有很大的优势。但是，该方法存在一个很大的问题就是学习时间过长。为此，周明等人（1998）提出了相应的改进方法，在改进算法的每次迭代过程中，只调整受到影响的小部分转换规则，而不需要搜索所有的转换规则。周明等人通过研究发现，每当一条获取的规则对训练语料实施标注后，语料中只有少数的词性会被改变，而只有在词性发生改变的地方，才影响与该位置相关的规则的得分，可见在E.Brill算法获取规则的过程中，大量时间花在数量占绝大多数的分值不需要修改的变换上。如果在用新的规则标注的过程中，当上下文环境满足，规则成立，而导致某一位置的词性发生改变时，准确判断哪些规则受到影响，然后相应修改这些规则的得分，而不必理会那些未受影响的其他所有规则，这样大大地节省了处理时间，提高了学习算法的速度。
另外，李晓黎等人（2000）尝试了利用数据采掘方法获取汉语词性标注规则的方法。该方法不但根据上下文中的词性和词，而且根据二者的组合来判断某个词的词性。在统计语料规模较大的情况下，给定最小支持度和最小可信度后，首先采掘大于最小支持度的常用模式集，然后生成关联规则。若此规则的可信度大于最小可信度，则得到词性标注规则。只要最小可信度定义得足够合理，获得的规则就可以用于处理词性的兼类问题。以这些获取的规则作为统计方法的补充，从而可以较好实现汉语词性标注。这种方法对于训练语料有较大的依赖性，尤其在语料库规则不够大的情况下。而且，在规则集中如何利用归纳学习方法进行归纳，以提高规则匹配的效率也值得进一步探讨。

## 统计方法与规则方法相结合的词性标注方法

理性主义方法与经验主义方法相结合的处理策略一直是自然语言处理领域的专家们不断研究和探索的问题，对于词性标注问题也不例外。
周强（1995）给出了一种规则方法与统计方法相结合的词性标注算法，其基本思想是，对汉语句子的初始标注结果（每个词带有所有可能的词类标记），首先经过规则排歧，排除那些最常见的、语言现象比较明显的歧义现象，然后通过统计排歧，处理那些剩余的多类词并进行未登录词的词性推断，最后再进行人工校对，得到正确的标注结果。这样做有两个好处：一方面利用标注语料对统计模型进行参数训练，可以得到统计排歧所需要的不同参数；另一方面，通过将机器自动标注的结果（规则排歧的或统计排歧的）与人工校对结果进行比较，可以发现自动处理的错误所在，从中总结出大量有用的信息以补充和调整规则库的内容。
但是，张民等（1998）指出，在文献［周强，1995］提出的方法中，规则的作用域是非受限的，而且并没有考虑统计的可信度，这使规则与统计的作用域不明确。因此，张民等人（1998）通过研究统计的可信度，引入置信区间的方法，构造了一种基于置信区间的评价函数，实现了统计和规则并举。

在基于HMM的汉语分词与词性标注一体化方法中，对于i状态的词w的出现次数可以通过前向后向算法计算出来。如果采用三元语法模型，前向后向算法可以用下面的式子描述：
$$
F\left(t_{i-1}, t_{i}\right)=\sum_{t_{i-2}}\left[F\left(t_{i-2}, t_{i-1}\right) \times P\left(t_{i} | t_{i-1}, t_{i-2}\right) \times P\left(w_{i-1} | t_{i-1}\right)\right] \qquad (7-54)
$$
$$
B\left(t_{i-1}, t_{i}\right)=\sum_{t_{i+1}}\left[B\left(t_{i}, t_{i+1}\right) \times P\left(t_{i+1} | t_{i}, t_{i-1}\right) \times P\left(w_{i+1} | t_{i+1}\right)\right] \qquad (7-55)
$$
$$
\phi(w)_{i}=\arg \max_t \sum_{t_{i-1}} [ F\left(t_{i-1}, t_{i}\right) \times B\left(t_{i-1}, t_{i}\right) \times P\left(w_{i} | t_{i}\right) ] \qquad (7-56)
$$
其中，t为词类标记，F值和B值分别通过递推公式（7-54）和公式（7-55）对整个HMM遍历求得。
现在假设兼类词w的候选词性为$T_1，T_2，T_3$，其对应概率的真实值分别为$p_1，p_2，p_3$，前向后向算法计算出的概率值分别为$\hat{p}_1, \hat{p}_2, \hat{p}_3$利用式（7-56）计算出当词w的词性为$T_i$（i＝1,2,3）时的出现次数为$\phi（w）T_i$。那么，
![7-56-1](/images/读书笔记/统计自然语言处理/7-56-1.png)
为了简单起见，i＝1,2,3时，可将$\phi（w）T_i$分别记作$n_1，n_2，n_3$（令n1>n2>n3）。
若$p_1$与$p_2$相差很大时，选择$T_1$导致错误的可能性就很小；若$p_1$与$p_2$相差不大时，选择$T_1$导致错误的可能性就较大。在决定是否选择$T_1$时，简单的阈值法肯定是不可取的，而以p1/p2是否大于阈值作为是否选择T1的判定条件比直接判断T1的阈值更加合理。但这种判定条件仍然存在下面的问题：假设p1/p2＝n1/n2＝3，一种情况可能是n1≈300，n2≈100；另一种情况有可能是n1≈3，n2≈1。显然在前一种情况下选择T1比在后一种情况下选择T1更加可靠。由此，评价算法必须能够反映出这种差别。为了保证在一定的正确率下作出选择，需要研究选择的统计可信度。也就是说，根据n1，n2计算出的p1，p2只是p1，p2的近似值，我们必须估计出这种近似的误差，对p1/p2进行修正，然后再对修正后的p1/p2进行判别。

由于ln（p1/p2）比p1/p2更快地逼近正态分布［Dagan and Itai,1994］，因此，可应用单边区间估计方法计算ln（p1/p2）的置信区间。假设希望的错误率（desired error probability）（显著性水平）为α（0＜α＜1），则可信度为1-α，服从正态分布的随机变量X的置信区间为$Z_{1-α} \sqrt{v a x X}$，其中，$Z_{1-α}$为置信系数（confidence coefficient） 〔14〕，可从统计表中直接查到，vaxX为随机变量X的标准差，在这里取
$$
\operatorname{vax} X=\operatorname{vax}\left[\ln \frac{\hat{p}_1}{\hat{p}_2}\right] \approx \frac{1}{n_1}+\frac{1}{n_2}
$$
因此，置信区间为$Z_{1-\alpha} \sqrt{\frac{1}{n_1}+\frac{1}{n_2}}$。
基于文献［Dagan and Itai,1994］的思路，张民等人（1998）给出最终的评价函数：
$$
\ln \frac{n_{1}}{n_{2}} \geqslant \theta+Z_{1-\alpha} \sqrt{\frac{1}{n_{1}}+\frac{1}{n_{2}}} \qquad (7-57)
$$
上述详细推演过程请参阅文献［Dagan and Itai,1994］
式（7-57）中的θ为经验值，可通过训练得到，张民等人（1998）的实验中取θ＝0.4。可以看出式（7-57）是一个动态阈值函数。假设
β＝$\ln \frac{n_{1}}{n_{2}}$，阈值$L_β＝θ＋Z_{1－α}\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}$。式（7-57）共有3组参数：α，θ和$n_1$，$n_2$。其中，θ是β的最低静态阈值，独立于训练语料的规模；α反映了统计的显著性，当α变小时可信度1－α变大，$Z_{1－α}$变大，阈值$L_β$就要变大，这与直观理解一致。$L_β$和$n_1，n_2$是倒数关系，在某种程度上反映了语料的稀疏程度。当n1，n2变大时Lβ变小。因此，动态阈值函数式（7-57）是在静态阈值θ的基础上由可信度（1－α）和训练语料规模（n1，n2）共同决定的。
![7-57举例](/images/读书笔记/统计自然语言处理/7-57举例.png)

## 词性标注中的生词处理方法

在任何一个自然语言处理系统中，生词的出现都是不可避免的，在词性标注中也不例外。在基于规则的词法标注方法中，生词处理通常是与词形分析和兼类词歧义消解一起进行的，而在基于统计模型的词性标注方法中，生词的词性标注问题通常是通过合理处理词汇的发射概率来解决的。
![7-58](/images/读书笔记/统计自然语言处理/7-58.png)
文献［赵铁军等，2001］在处理生词标注问题时，把生词的词汇发射概率赋值为1，即令$P（x_j|t_j）＝1$。这种处理方法的优点是易于实现，处理效率高，但毕竟由于缺乏统计的先验知识基础，因此，标注的正确率受到一定的影响。为此，张孝飞等人（2003）提出了另外一种估算生词$x_j$的词汇发射概率的方法：假设将词汇序列W加入训练集，由于仅加入一个词汇序列，而训练集足够大，因此，可以认为对其他词的发射概率和整个模型的词性转移概率的影响忽略不计。根据HMM的假设，$x_j$的词性$t_j$由$w_{j－1}$的词性$t_{j－1}$决定，那么，
$$
P\left(t_{j} | x_{j}\right) \approx \sum_{k=1}^{M} P\left(t_{k} | w_{j-1}\right) P\left(t_{j} | t_{k}\right) \qquad (7-60)
$$
其中，M为词性种类的数目。根据Bayes公式，词汇的发射概率为
$$
P\left(x_{j} | t_{j}\right)=\frac{P\left(x_{j}\right)}{P\left(t_{j}\right)} \times P\left(t_{j} | x_{j}\right) \qquad (7-61)
$$
将式（7-60）代入式（7-61），得
$$
P\left(x_{j} | t_{j}\right) \approx \frac{P\left(x_{j}\right)}{P\left(t_{j}\right)} \times \sum_{k=1}^{M} P\left(t_{k} | w_{j-1}\right) P\left(t_{j} | t_{k}\right) \qquad (7-62)
$$
对式（7-62）中的各概率值采用最大似然估计，得
$$
\begin{aligned} P\left(x_{j} | t_{j}\right) & \approx \frac{C\left(x_{j}\right)}{C\left(t_{j}\right)} \sum_{k=1}^{M} P\left(t_{k} | w_{j-1}\right) P\left(t_{j} | t_{k}\right) \\\\ &=\frac{1}{C\left(t_{j}\right)} \sum_{k=1}^{M}\left[\frac{C\left(w_{j-1} t_{k}\right)}{C\left(w_{j-1}\right)} \times \frac{C\left(t_{k} t_{j}\right)}{C\left(t_{k}\right)}\right] \end{aligned}
$$
其中，C（$t_j$）、C（$t_k$）和C（$w_{j-1}$）分别为词性$t_j、t_k$和词$w_{j-1}$在训练语料中的出现次数；C（$t_kt_j$）、C（$w_{j-1}t_k$）分别为词性串$t_kt_j$和串$w_{j-1}t_k$的同现次数。

张孝飞等（2003）利用式（7-63）作为生词词汇发射概率的估算模型，用《人民日报》语料进行了汉语词性标注实验。开放测试的结果表明，该方法与强令P（xj|tj）＝1的生词处理方法相比，其词性标注的正确率平均高出近1％。

另外，朱靖波等（1999）曾提出了一种基于NA假设（nonambiguity-ambiguity assumption, NAA）的词性标注方法。该方法基于NAA从无标注语料中抽取词性三元组数据，训练词性统计模型所需要的参数，对稀疏数据进行平滑处理，对词典中未登录词（生词）的词性进行猜测，根据生词的上下文评估各种词性的概率，最终选取最大概率词性作为生词的词性。


